{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8d9dfd-638a-44d8-b89e-521361ee79f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp core.guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517bcd5d-dd1c-40fa-aa3e-7456740b82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from typing import Dict, Any, List, Optional, Callable, Union\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "class GuardrailType(Enum):\n",
    "    INPUT_VALIDATION = \"input_validation\"\n",
    "    OUTPUT_FILTERING = \"output_filtering\" \n",
    "    TOOL_RESTRICTION = \"tool_restriction\"\n",
    "    CONTENT_SAFETY = \"content_safety\"\n",
    "    RATE_LIMITING = \"rate_limiting\"\n",
    "\n",
    "\n",
    "class GuardrailAction(Enum):\n",
    "    ALLOW = \"allow\"\n",
    "    BLOCK = \"block\"\n",
    "    MODIFY = \"modify\"\n",
    "    WARN = \"warn\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GuardrailResult:\n",
    "    \"\"\"Result of guardrail evaluation\"\"\"\n",
    "    action: GuardrailAction\n",
    "    passed: bool\n",
    "    message: Optional[str] = None\n",
    "    modified_content: Optional[str] = None\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "class Guardrail:\n",
    "    \"\"\"Base class for all guardrails\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, guardrail_type: GuardrailType):\n",
    "        self.name = name\n",
    "        self.type = guardrail_type\n",
    "        self.enabled = True\n",
    "    \n",
    "    def evaluate(self, content: str, context: Optional[Dict[str, Any]] = None) -> GuardrailResult:\n",
    "        \"\"\"Evaluate content against this guardrail\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def enable(self):\n",
    "        \"\"\"Enable this guardrail\"\"\"\n",
    "        self.enabled = True\n",
    "    \n",
    "    def disable(self):\n",
    "        \"\"\"Disable this guardrail\"\"\"\n",
    "        self.enabled = False\n",
    "\n",
    "\n",
    "class ContentSafetyGuardrail(Guardrail):\n",
    "    \"\"\"Guardrail for content safety\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"content_safety\", GuardrailType.CONTENT_SAFETY)\n",
    "        self.blocked_patterns = [\n",
    "            r'\\b(password|secret|api[_-]?key|token)\\s*[:=]\\s*\\S+',\n",
    "            r'\\b\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\\b',  # Credit card\n",
    "            r'\\b\\d{3}-\\d{2}-\\d{4}\\b',  # SSN\n",
    "            r'rm\\s+-rf\\s+/',  # Dangerous rm commands\n",
    "            r'sudo\\s+rm',  # Sudo rm commands\n",
    "        ]\n",
    "        self.warning_patterns = [\n",
    "            r'\\bsudo\\b',\n",
    "            r'\\bchmod\\s+777\\b',\n",
    "            r'\\bdd\\s+if=',\n",
    "        ]\n",
    "    \n",
    "    def evaluate(self, content: str, context: Optional[Dict[str, Any]] = None) -> GuardrailResult:\n",
    "        if not self.enabled:\n",
    "            return GuardrailResult(GuardrailAction.ALLOW, True)\n",
    "        \n",
    "        # Check for blocked patterns\n",
    "        for pattern in self.blocked_patterns:\n",
    "            if re.search(pattern, content, re.IGNORECASE):\n",
    "                return GuardrailResult(\n",
    "                    GuardrailAction.BLOCK,\n",
    "                    False,\n",
    "                    f\"Content blocked: Contains sensitive information or dangerous command\"\n",
    "                )\n",
    "        \n",
    "        # Check for warning patterns\n",
    "        for pattern in self.warning_patterns:\n",
    "            if re.search(pattern, content, re.IGNORECASE):\n",
    "                return GuardrailResult(\n",
    "                    GuardrailAction.WARN,\n",
    "                    True,\n",
    "                    f\"Warning: Content contains potentially dangerous command\"\n",
    "                )\n",
    "        \n",
    "        return GuardrailResult(GuardrailAction.ALLOW, True)\n",
    "\n",
    "\n",
    "class ToolRestrictionGuardrail(Guardrail):\n",
    "    \"\"\"Guardrail for restricting tool usage\"\"\"\n",
    "    \n",
    "    def __init__(self, allowed_tools: Optional[List[str]] = None, blocked_tools: Optional[List[str]] = None):\n",
    "        super().__init__(\"tool_restriction\", GuardrailType.TOOL_RESTRICTION)\n",
    "        self.allowed_tools = allowed_tools\n",
    "        self.blocked_tools = blocked_tools or []\n",
    "    \n",
    "    def evaluate(self, content: str, context: Optional[Dict[str, Any]] = None) -> GuardrailResult:\n",
    "        if not self.enabled:\n",
    "            return GuardrailResult(GuardrailAction.ALLOW, True)\n",
    "        \n",
    "        # Extract tool calls from context\n",
    "        tool_calls = context.get(\"tool_calls\", []) if context else []\n",
    "        \n",
    "        for tool_call in tool_calls:\n",
    "            tool_name = tool_call.get(\"function\", {}).get(\"name\")\n",
    "            \n",
    "            if tool_name in self.blocked_tools:\n",
    "                return GuardrailResult(\n",
    "                    GuardrailAction.BLOCK,\n",
    "                    False,\n",
    "                    f\"Tool '{tool_name}' is blocked by policy\"\n",
    "                )\n",
    "            \n",
    "            if self.allowed_tools and tool_name not in self.allowed_tools:\n",
    "                return GuardrailResult(\n",
    "                    GuardrailAction.BLOCK,\n",
    "                    False,\n",
    "                    f\"Tool '{tool_name}' is not in allowed tools list\"\n",
    "                )\n",
    "        \n",
    "        return GuardrailResult(GuardrailAction.ALLOW, True)\n",
    "\n",
    "\n",
    "class InputValidationGuardrail(Guardrail):\n",
    "    \"\"\"Guardrail for input validation\"\"\"\n",
    "    \n",
    "    def __init__(self, max_length: int = 10000, min_length: int = 1):\n",
    "        super().__init__(\"input_validation\", GuardrailType.INPUT_VALIDATION)\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "        self.blocked_keywords = [\"hack\", \"exploit\", \"bypass\", \"jailbreak\"]\n",
    "    \n",
    "    def evaluate(self, content: str, context: Optional[Dict[str, Any]] = None) -> GuardrailResult:\n",
    "        if not self.enabled:\n",
    "            return GuardrailResult(GuardrailAction.ALLOW, True)\n",
    "        \n",
    "        # Length validation\n",
    "        if len(content) > self.max_length:\n",
    "            return GuardrailResult(\n",
    "                GuardrailAction.BLOCK,\n",
    "                False,\n",
    "                f\"Input too long: {len(content)} > {self.max_length} characters\"\n",
    "            )\n",
    "        \n",
    "        if len(content) < self.min_length:\n",
    "            return GuardrailResult(\n",
    "                GuardrailAction.BLOCK,\n",
    "                False,\n",
    "                f\"Input too short: {len(content)} < {self.min_length} characters\"\n",
    "            )\n",
    "        \n",
    "        # Keyword validation\n",
    "        content_lower = content.lower()\n",
    "        for keyword in self.blocked_keywords:\n",
    "            if keyword in content_lower:\n",
    "                return GuardrailResult(\n",
    "                    GuardrailAction.WARN,\n",
    "                    True,\n",
    "                    f\"Warning: Input contains potentially problematic keyword: {keyword}\"\n",
    "                )\n",
    "        \n",
    "        return GuardrailResult(GuardrailAction.ALLOW, True)\n",
    "\n",
    "\n",
    "class OutputFilteringGuardrail(Guardrail):\n",
    "    \"\"\"Guardrail for filtering output content\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"output_filtering\", GuardrailType.OUTPUT_FILTERING)\n",
    "        self.sensitive_patterns = [\n",
    "            r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',  # Email\n",
    "            r'\\b\\d{3}-\\d{3}-\\d{4}\\b',  # Phone number\n",
    "            r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b',  # IP address\n",
    "        ]\n",
    "    \n",
    "    def evaluate(self, content: str, context: Optional[Dict[str, Any]] = None) -> GuardrailResult:\n",
    "        if not self.enabled:\n",
    "            return GuardrailResult(GuardrailAction.ALLOW, True)\n",
    "        \n",
    "        modified_content = content\n",
    "        modifications_made = False\n",
    "        \n",
    "        for pattern in self.sensitive_patterns:\n",
    "            matches = re.findall(pattern, content)\n",
    "            if matches:\n",
    "                for match in matches:\n",
    "                    # Replace with redacted version\n",
    "                    redacted = \"[REDACTED]\"\n",
    "                    modified_content = modified_content.replace(match, redacted)\n",
    "                    modifications_made = True\n",
    "        \n",
    "        if modifications_made:\n",
    "            return GuardrailResult(\n",
    "                GuardrailAction.MODIFY,\n",
    "                True,\n",
    "                \"Output modified: Sensitive information redacted\",\n",
    "                modified_content\n",
    "            )\n",
    "        \n",
    "        return GuardrailResult(GuardrailAction.ALLOW, True)\n",
    "\n",
    "\n",
    "class RateLimitingGuardrail(Guardrail):\n",
    "    \"\"\"Guardrail for rate limiting\"\"\"\n",
    "    \n",
    "    def __init__(self, max_requests_per_minute: int = 60):\n",
    "        super().__init__(\"rate_limiting\", GuardrailType.RATE_LIMITING)\n",
    "        self.max_requests_per_minute = max_requests_per_minute\n",
    "        self.request_history: List[float] = []\n",
    "    \n",
    "    def evaluate(self, content: str, context: Optional[Dict[str, Any]] = None) -> GuardrailResult:\n",
    "        if not self.enabled:\n",
    "            return GuardrailResult(GuardrailAction.ALLOW, True)\n",
    "        \n",
    "        import time\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Clean old requests (older than 1 minute)\n",
    "        self.request_history = [t for t in self.request_history if current_time - t < 60]\n",
    "        \n",
    "        # Check rate limit\n",
    "        if len(self.request_history) >= self.max_requests_per_minute:\n",
    "            return GuardrailResult(\n",
    "                GuardrailAction.BLOCK,\n",
    "                False,\n",
    "                f\"Rate limit exceeded: {len(self.request_history)} requests in last minute\"\n",
    "            )\n",
    "        \n",
    "        # Add current request\n",
    "        self.request_history.append(current_time)\n",
    "        \n",
    "        return GuardrailResult(GuardrailAction.ALLOW, True)\n",
    "\n",
    "\n",
    "class GuardrailManager:\n",
    "    \"\"\"Manages multiple guardrails\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.guardrails: List[Guardrail] = []\n",
    "        self.enforcement_mode = \"strict\"  # strict, permissive, logging_only\n",
    "    \n",
    "    def add_guardrail(self, guardrail: Guardrail):\n",
    "        \"\"\"Add a guardrail\"\"\"\n",
    "        self.guardrails.append(guardrail)\n",
    "    \n",
    "    def remove_guardrail(self, name: str):\n",
    "        \"\"\"Remove a guardrail by name\"\"\"\n",
    "        self.guardrails = [g for g in self.guardrails if g.name != name]\n",
    "    \n",
    "    def evaluate_input(self, content: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate input against all guardrails\"\"\"\n",
    "        results = []\n",
    "        overall_passed = True\n",
    "        final_action = GuardrailAction.ALLOW\n",
    "        \n",
    "        for guardrail in self.guardrails:\n",
    "            if guardrail.type in [GuardrailType.INPUT_VALIDATION, GuardrailType.CONTENT_SAFETY]:\n",
    "                result = guardrail.evaluate(content, context)\n",
    "                results.append({\n",
    "                    \"guardrail\": guardrail.name,\n",
    "                    \"result\": result\n",
    "                })\n",
    "                \n",
    "                if not result.passed:\n",
    "                    overall_passed = False\n",
    "                \n",
    "                if result.action == GuardrailAction.BLOCK:\n",
    "                    final_action = GuardrailAction.BLOCK\n",
    "        \n",
    "        return {\n",
    "            \"passed\": overall_passed,\n",
    "            \"action\": final_action,\n",
    "            \"results\": results,\n",
    "            \"enforcement_mode\": self.enforcement_mode\n",
    "        }\n",
    "    \n",
    "    def evaluate_output(self, content: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate output against all guardrails\"\"\"\n",
    "        results = []\n",
    "        modified_content = content\n",
    "        \n",
    "        for guardrail in self.guardrails:\n",
    "            if guardrail.type == GuardrailType.OUTPUT_FILTERING:\n",
    "                result = guardrail.evaluate(modified_content, context)\n",
    "                results.append({\n",
    "                    \"guardrail\": guardrail.name,\n",
    "                    \"result\": result\n",
    "                })\n",
    "                \n",
    "                if result.modified_content:\n",
    "                    modified_content = result.modified_content\n",
    "        \n",
    "        return {\n",
    "            \"original_content\": content,\n",
    "            \"modified_content\": modified_content,\n",
    "            \"results\": results\n",
    "        }\n",
    "    \n",
    "    def set_enforcement_mode(self, mode: str):\n",
    "        \"\"\"Set enforcement mode: strict, permissive, logging_only\"\"\"\n",
    "        self.enforcement_mode = mode\n",
    "\n",
    "\n",
    "def create_default_guardrails() -> GuardrailManager:\n",
    "    \"\"\"Create a default set of guardrails\"\"\"\n",
    "    manager = GuardrailManager()\n",
    "    \n",
    "    # Add default guardrails\n",
    "    manager.add_guardrail(ContentSafetyGuardrail())\n",
    "    manager.add_guardrail(InputValidationGuardrail())\n",
    "    manager.add_guardrail(OutputFilteringGuardrail())\n",
    "    manager.add_guardrail(RateLimitingGuardrail())\n",
    "    \n",
    "    # Add tool restrictions for sensitive tools\n",
    "    dangerous_tools = [\"execute_bash\", \"fs_write\"]\n",
    "    manager.add_guardrail(ToolRestrictionGuardrail(blocked_tools=dangerous_tools))\n",
    "    \n",
    "    return manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494d68d-a666-46c9-9021-6fcd02f3063d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
