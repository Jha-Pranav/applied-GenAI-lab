{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04126151-3813-4ca5-9fda-753c68482c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp backend.tools.fs_read_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c52edd-e0f3-42d7-ad4f-363dc1f99710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from agentic.tools.base import Tool\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c33336-2f83-40d3-9c1e-38b7213ceaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class FsReadTool(Tool):\n",
    "    def get_tool_schema(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return the OpenAI-compatible schema for fs_read.\"\"\"\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"fs_read\",\n",
    "                \"description\": \"File system operations: read files with intelligent chunking, list directories with depth, find files, and grep across files.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"operations\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"mode\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"enum\": [\"Line\", \"Directory\", \"Find\", \"Grep\"],\n",
    "                                        \"description\": \"Operation mode: Line (read file), Directory (list with depth), Find (find files by name), Grep (search across files)\"\n",
    "                                    },\n",
    "                                    \"path\": {\"type\": \"string\", \"description\": \"File or directory path\"},\n",
    "                                    \"pattern\": {\"type\": \"string\", \"description\": \"Search pattern for Grep mode\"},\n",
    "                                    \"name_pattern\": {\"type\": \"string\", \"description\": \"File name pattern for Find mode\"},\n",
    "                                    \"file_pattern\": {\"type\": \"string\", \"description\": \"File pattern for Grep mode (e.g., '*.py')\"},\n",
    "                                    \"depth\": {\"type\": \"integer\", \"description\": \"Directory depth for Directory mode\"},\n",
    "                                    \"max_depth\": {\"type\": \"integer\", \"description\": \"Maximum depth for Find mode\"},\n",
    "                                    \"context_lines\": {\"type\": \"integer\", \"description\": \"Context lines around matches\"},\n",
    "                                    \"start_line\": {\"type\": \"integer\", \"description\": \"Start line for Line mode\"},\n",
    "                                    \"end_line\": {\"type\": \"integer\", \"description\": \"End line for Line mode (-1 for end)\"},\n",
    "                                    \"show_hidden\": {\"type\": \"boolean\", \"description\": \"Show hidden files/directories\"},\n",
    "                                    \"case_sensitive\": {\"type\": \"boolean\", \"description\": \"Case sensitive search\"},\n",
    "                                    \"recursive\": {\"type\": \"boolean\", \"description\": \"Recursive search for Grep mode\"},\n",
    "                                    \"file_type\": {\"type\": \"string\", \"enum\": [\"file\", \"dir\", \"all\"], \"description\": \"File type filter for Find mode\"},\n",
    "                                    \"smart_chunk\": {\"type\": \"boolean\", \"description\": \"Use intelligent chunking for large files\"}\n",
    "                                },\n",
    "                                \"required\": [\"mode\", \"path\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"operations\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def execute(self, operations: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Execute fs_read with intelligent chunking and advanced features.\"\"\"\n",
    "        results = []\n",
    "        valid_modes = {\"directory\", \"line\", \"find\", \"grep\"}\n",
    "        \n",
    "        for op in operations:\n",
    "            mode = op.get(\"mode\").lower()\n",
    "            path = op.get(\"path\")\n",
    "\n",
    "            if mode not in valid_modes:\n",
    "                results.append({\n",
    "                    \"path\": path,\n",
    "                    \"error\": f\"Invalid mode '{mode}'. Valid modes: {', '.join(valid_modes)}\"\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                if mode == \"directory\":\n",
    "                    depth = op.get(\"depth\", 1)\n",
    "                    show_hidden = op.get(\"show_hidden\", False)\n",
    "                    items = self._list_directory_tree(path, depth, show_hidden)\n",
    "                    results.append({\"path\": path, \"items\": items, \"depth\": depth})\n",
    "                \n",
    "                elif mode == \"line\":\n",
    "                    start_line = op.get(\"start_line\", 1)\n",
    "                    end_line = op.get(\"end_line\", -1)\n",
    "                    smart_chunk = op.get(\"smart_chunk\", True)\n",
    "                    content = self._read_file_intelligent(path, start_line, end_line, smart_chunk)\n",
    "                    results.append({\"path\": path, \"content\": content, \"start_line\": start_line, \"end_line\": end_line})\n",
    "                \n",
    "                elif mode == \"find\":\n",
    "                    name_pattern = op.get(\"name_pattern\", \"*\")\n",
    "                    file_type = op.get(\"file_type\", \"all\")\n",
    "                    max_depth = op.get(\"max_depth\", 10)\n",
    "                    found_items = self._find_files(path, name_pattern, file_type, max_depth)\n",
    "                    results.append({\"path\": path, \"name_pattern\": name_pattern, \"found\": found_items})\n",
    "                \n",
    "                elif mode == \"grep\":\n",
    "                    pattern = op.get(\"pattern\", \"\")\n",
    "                    file_pattern = op.get(\"file_pattern\", \"*\")\n",
    "                    recursive = op.get(\"recursive\", True)\n",
    "                    context_lines = op.get(\"context_lines\", 2)\n",
    "                    matches = self._grep_files(path, pattern, file_pattern, recursive, context_lines)\n",
    "                    results.append({\"path\": path, \"pattern\": pattern, \"matches\": matches})\n",
    "            \n",
    "            except Exception as e:\n",
    "                results.append({\"path\": path, \"error\": str(e)})\n",
    "        \n",
    "        return {\"results\": results}\n",
    "\n",
    "    def _get_file_info(self, path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive file information for intelligent reading decisions.\"\"\"\n",
    "        path_obj = Path(path)\n",
    "        if not path_obj.exists():\n",
    "            return {\"error\": \"File not found\"}\n",
    "        \n",
    "        stat = path_obj.stat()\n",
    "        file_size = stat.st_size\n",
    "        \n",
    "        file_info = {\n",
    "            \"size\": file_size,\n",
    "            \"lines\": 0,\n",
    "            \"is_binary\": False,\n",
    "            \"encoding\": \"utf-8\",\n",
    "            \"file_type\": path_obj.suffix.lower(),\n",
    "            \"is_large\": file_size > 1024 * 1024,  # > 1MB\n",
    "            \"estimated_lines\": file_size // 50 if file_size > 0 else 0\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                chunk = f.read(1024)\n",
    "                if b'\\x00' in chunk:\n",
    "                    file_info[\"is_binary\"] = True\n",
    "                    return file_info\n",
    "        except:\n",
    "            file_info[\"is_binary\"] = True\n",
    "            return file_info\n",
    "        \n",
    "        try:\n",
    "            if file_size < 10 * 1024 * 1024:  # < 10MB\n",
    "                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    file_info[\"lines\"] = sum(1 for _ in f)\n",
    "            else:\n",
    "                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    sample_lines = sum(1 for _, line in zip(range(10000), f))\n",
    "                    file_info[\"lines\"] = int((file_size / (f.tell() or 1)) * sample_lines)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return file_info\n",
    "\n",
    "    def _decide_read_strategy(self, path: str, start_line: int, end_line: int, file_info: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Decide how to read the file based on size, type, and request.\"\"\"\n",
    "        strategy = {\n",
    "            \"method\": \"full\",\n",
    "            \"chunk_size\": 1000,\n",
    "            \"sample_ratio\": 0.1,\n",
    "            \"reason\": \"\"\n",
    "        }\n",
    "        \n",
    "        total_lines = file_info.get(\"lines\", 0)\n",
    "        file_size = file_info.get(\"size\", 0)\n",
    "        \n",
    "        if file_info.get(\"is_binary\"):\n",
    "            strategy.update({\"method\": \"binary_info\", \"reason\": \"Binary file detected\"})\n",
    "        elif file_size > 50 * 1024 * 1024:\n",
    "            strategy.update({\"method\": \"head_tail\", \"chunk_size\": 500, \"reason\": \"Very large file, showing head and tail\"})\n",
    "        elif total_lines > 10000 and (end_line - start_line + 1) > 5000:\n",
    "            strategy.update({\"method\": \"sampled\", \"sample_ratio\": 0.2, \"reason\": \"Large file with many requested lines\"})\n",
    "        elif total_lines > 5000 and (start_line == 1 and end_line == total_lines):\n",
    "            strategy.update({\"method\": \"chunked\", \"chunk_size\": 2000, \"reason\": \"Large file, full read requested\"})\n",
    "        elif (end_line - start_line + 1) > 2000:\n",
    "            strategy.update({\"method\": \"chunked\", \"chunk_size\": 1000, \"reason\": \"Many lines requested\"})\n",
    "        else:\n",
    "            strategy.update({\"method\": \"full\", \"reason\": \"Standard read\"})\n",
    "        \n",
    "        return strategy\n",
    "\n",
    "    def _read_file_intelligent(self, path: str, start_line: int = 1, end_line: int = -1, smart_chunk: bool = True) -> str:\n",
    "        \"\"\"Intelligently read file based on size and content type.\"\"\"\n",
    "        if not smart_chunk:\n",
    "            return self._read_file_lines(path, start_line, end_line)\n",
    "        \n",
    "        file_info = self._get_file_info(path)\n",
    "        if \"error\" in file_info:\n",
    "            return file_info[\"error\"]\n",
    "        \n",
    "        if file_info.get(\"is_binary\"):\n",
    "            return f\"[Binary file: {file_info['size']} bytes, type: {file_info.get('file_type', 'unknown')}]\"\n",
    "        \n",
    "        strategy = self._decide_read_strategy(path, start_line, end_line, file_info)\n",
    "        \n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            total_lines = len(lines)\n",
    "            if end_line == -1:\n",
    "                end_line = total_lines\n",
    "            \n",
    "            start_idx = max(0, start_line - 1)\n",
    "            end_idx = min(total_lines, end_line)\n",
    "            \n",
    "            if strategy[\"method\"] == \"full\":\n",
    "                content = ''.join(lines[start_idx:end_idx])\n",
    "            \n",
    "            elif strategy[\"method\"] == \"chunked\":\n",
    "                chunk_size = strategy[\"chunk_size\"]\n",
    "                chunks = []\n",
    "                current_idx = start_idx\n",
    "                \n",
    "                while current_idx < end_idx:\n",
    "                    chunk_end = min(current_idx + chunk_size, end_idx)\n",
    "                    chunk = ''.join(lines[current_idx:chunk_end])\n",
    "                    chunks.append(f\"--- Lines {current_idx + 1}-{chunk_end} ---\\n{chunk}\")\n",
    "                    current_idx = chunk_end\n",
    "                    \n",
    "                    if len(chunks) >= 5:\n",
    "                        remaining = end_idx - current_idx\n",
    "                        if remaining > 0:\n",
    "                            chunks.append(f\"... [{remaining} more lines omitted] ...\")\n",
    "                        break\n",
    "                \n",
    "                content = '\\n'.join(chunks)\n",
    "            \n",
    "            elif strategy[\"method\"] == \"head_tail\":\n",
    "                chunk_size = strategy[\"chunk_size\"]\n",
    "                head = ''.join(lines[start_idx:start_idx + chunk_size])\n",
    "                tail = ''.join(lines[max(start_idx + chunk_size, end_idx - chunk_size):end_idx])\n",
    "                \n",
    "                omitted = end_idx - start_idx - (2 * chunk_size)\n",
    "                content = f\"--- Head (lines {start_idx + 1}-{start_idx + chunk_size}) ---\\n{head}\\n\"\n",
    "                if omitted > 0:\n",
    "                    content += f\"... [{omitted} lines omitted] ...\\n\"\n",
    "                content += f\"--- Tail (lines {end_idx - chunk_size + 1}-{end_idx}) ---\\n{tail}\"\n",
    "            \n",
    "            elif strategy[\"method\"] == \"sampled\":\n",
    "                sample_ratio = strategy[\"sample_ratio\"]\n",
    "                total_requested = end_idx - start_idx\n",
    "                sample_size = int(total_requested * sample_ratio)\n",
    "                \n",
    "                if sample_size < 100:\n",
    "                    sample_size = min(100, total_requested)\n",
    "                \n",
    "                step = max(1, total_requested // sample_size)\n",
    "                sampled_lines = []\n",
    "                \n",
    "                for i in range(start_idx, end_idx, step):\n",
    "                    if len(sampled_lines) >= sample_size:\n",
    "                        break\n",
    "                    sampled_lines.append(f\"{i + 1:6d}: {lines[i]}\")\n",
    "                \n",
    "                content = f\"--- Sampled {len(sampled_lines)} lines from {total_requested} total ---\\n\"\n",
    "                content += ''.join(sampled_lines)\n",
    "            \n",
    "            metadata = f\"\\n--- File Info: {file_info['size']} bytes, {file_info['lines']} lines, Strategy: {strategy['method']} ({strategy['reason']}) ---\"\n",
    "            return content + metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "    def _read_file_lines(self, path: str, start_line: int, end_line: int) -> str:\n",
    "        \"\"\"Simple line-based file reading.\"\"\"\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        if end_line == -1:\n",
    "            end_line = len(lines)\n",
    "        \n",
    "        start_idx = max(0, start_line - 1)\n",
    "        end_idx = min(len(lines), end_line)\n",
    "        \n",
    "        return ''.join(lines[start_idx:end_idx])\n",
    "\n",
    "    def _list_directory_tree(self, path: str, depth: int, show_hidden: bool) -> List[Dict]:\n",
    "        \"\"\"List directory with depth and metadata.\"\"\"\n",
    "        items = []\n",
    "        path_obj = Path(path)\n",
    "        \n",
    "        if not path_obj.exists() or not path_obj.is_dir():\n",
    "            return items\n",
    "            \n",
    "        def _scan_dir(dir_path: Path, current_depth: int) -> List[Dict]:\n",
    "            dir_items = []\n",
    "            if current_depth > depth:\n",
    "                return dir_items\n",
    "                \n",
    "            try:\n",
    "                for item in sorted(dir_path.iterdir()):\n",
    "                    if not show_hidden and item.name.startswith('.'):\n",
    "                        continue\n",
    "                        \n",
    "                    stat = item.stat()\n",
    "                    item_info = {\n",
    "                        \"name\": item.name,\n",
    "                        \"type\": \"directory\" if item.is_dir() else \"file\",\n",
    "                        \"size\": stat.st_size if item.is_file() else None,\n",
    "                        \"modified\": stat.st_mtime,\n",
    "                        \"permissions\": oct(stat.st_mode)[-3:],\n",
    "                        \"path\": str(item)\n",
    "                    }\n",
    "                    \n",
    "                    if item.is_dir() and current_depth < depth:\n",
    "                        item_info[\"children\"] = _scan_dir(item, current_depth + 1)\n",
    "                    \n",
    "                    dir_items.append(item_info)\n",
    "            except PermissionError:\n",
    "                pass\n",
    "                \n",
    "            return dir_items\n",
    "        \n",
    "        return _scan_dir(path_obj, 0)\n",
    "\n",
    "    def _search_in_file(self, path: str, pattern: str, context_lines: int, case_sensitive: bool) -> List[Dict]:\n",
    "        \"\"\"Search pattern in file with context.\"\"\"\n",
    "        matches = []\n",
    "        \n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                lines = f.readlines()\n",
    "        except:\n",
    "            return matches\n",
    "        \n",
    "        search_pattern = pattern if case_sensitive else pattern.lower()\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            search_line = line if case_sensitive else line.lower()\n",
    "            \n",
    "            if search_pattern in search_line:\n",
    "                start_ctx = max(0, i - context_lines)\n",
    "                end_ctx = min(len(lines), i + context_lines + 1)\n",
    "                \n",
    "                context = []\n",
    "                for j in range(start_ctx, end_ctx):\n",
    "                    context.append({\n",
    "                        \"line_number\": j + 1,\n",
    "                        \"content\": lines[j].rstrip(),\n",
    "                        \"is_match\": j == i\n",
    "                    })\n",
    "                \n",
    "                matches.append({\n",
    "                    \"line_number\": i + 1,\n",
    "                    \"content\": line.rstrip(),\n",
    "                    \"context\": context\n",
    "                })\n",
    "        \n",
    "        return matches\n",
    "\n",
    "    def _find_files(self, path: str, name_pattern: str, file_type: str, max_depth: int) -> List[Dict]:\n",
    "        \"\"\"Find files by name pattern.\"\"\"\n",
    "        found = []\n",
    "        path_obj = Path(path)\n",
    "        \n",
    "        def _search_recursive(dir_path: Path, current_depth: int):\n",
    "            if current_depth > max_depth:\n",
    "                return\n",
    "                \n",
    "            try:\n",
    "                for item in dir_path.iterdir():\n",
    "                    if fnmatch.fnmatch(item.name, name_pattern):\n",
    "                        if (file_type == \"all\" or \n",
    "                            (file_type == \"file\" and item.is_file()) or\n",
    "                            (file_type == \"dir\" and item.is_dir())):\n",
    "                            \n",
    "                            stat = item.stat()\n",
    "                            found.append({\n",
    "                                \"name\": item.name,\n",
    "                                \"path\": str(item),\n",
    "                                \"type\": \"directory\" if item.is_dir() else \"file\",\n",
    "                                \"size\": stat.st_size if item.is_file() else None,\n",
    "                                # \"modified\": stat.st_mtime  \n",
    "                            })\n",
    "                    \n",
    "                    if item.is_dir() and current_depth < max_depth:\n",
    "                        _search_recursive(item, current_depth + 1)\n",
    "            except PermissionError:\n",
    "                pass\n",
    "        \n",
    "        if path_obj.is_dir():\n",
    "            _search_recursive(path_obj, 0)\n",
    "        \n",
    "        return found\n",
    "\n",
    "    def _grep_files(self, path: str, pattern: str, file_pattern: str, recursive: bool, context_lines: int) -> List[Dict]:\n",
    "        \"\"\"Grep pattern across multiple files.\"\"\"\n",
    "        matches = []\n",
    "        path_obj = Path(path)\n",
    "        \n",
    "        def _grep_file(file_path: Path):\n",
    "            try:\n",
    "                file_matches = self._search_in_file(str(file_path), pattern, context_lines, False)\n",
    "                if file_matches:\n",
    "                    matches.append({\n",
    "                        \"file\": str(file_path),\n",
    "                        \"matches\": file_matches\n",
    "                    })\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        def _scan_directory(dir_path: Path):\n",
    "            try:\n",
    "                for item in dir_path.iterdir():\n",
    "                    if item.is_file() and fnmatch.fnmatch(item.name, file_pattern):\n",
    "                        _grep_file(item)\n",
    "                    elif item.is_dir() and recursive:\n",
    "                        _scan_directory(item)\n",
    "            except PermissionError:\n",
    "                pass\n",
    "        \n",
    "        if path_obj.is_file():\n",
    "            _grep_file(path_obj)\n",
    "        elif path_obj.is_dir():\n",
    "            _scan_directory(path_obj)\n",
    "        \n",
    "        return matches\n",
    "\n",
    "    def _generate_tree(self, path: str, max_depth: int, show_hidden: bool) -> str:\n",
    "        \"\"\"Generate tree-like directory structure.\"\"\"\n",
    "        tree_lines = []\n",
    "        path_obj = Path(path)\n",
    "        \n",
    "        def _build_tree(dir_path: Path, prefix: str, current_depth: int):\n",
    "            if current_depth > max_depth:\n",
    "                return\n",
    "                \n",
    "            try:\n",
    "                items = sorted(dir_path.iterdir(), key=lambda x: (x.is_file(), x.name.lower()))\n",
    "                if not show_hidden:\n",
    "                    items = [item for item in items if not item.name.startswith('.')]\n",
    "                \n",
    "                for i, item in enumerate(items):\n",
    "                    is_last = i == len(items) - 1\n",
    "                    current_prefix = \"└── \" if is_last else \"├── \"\n",
    "                    tree_lines.append(f\"{prefix}{current_prefix}{item.name}\")\n",
    "                    \n",
    "                    if item.is_dir() and current_depth < max_depth:\n",
    "                        next_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "                        _build_tree(item, next_prefix, current_depth + 1)\n",
    "            except PermissionError:\n",
    "                tree_lines.append(f\"{prefix}└── [Permission Denied]\")\n",
    "        \n",
    "        tree_lines.append(str(path_obj))\n",
    "        if path_obj.is_dir():\n",
    "            _build_tree(path_obj, \"\", 0)\n",
    "        \n",
    "        return \"\\n\".join(tree_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "324d7e7c-009f-434e-aea8-c777d5379173",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f2a98f-c8ba-4d14-a32b-edbff03d470f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': '.ipynb_checkpoints',\n",
       "  'type': 'directory',\n",
       "  'size': None,\n",
       "  'modified': 1758054614.2661624,\n",
       "  'permissions': '775',\n",
       "  'path': '../.ipynb_checkpoints',\n",
       "  'children': [{'name': 'analyzer-checkpoint.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 33594,\n",
       "    'modified': 1758055336.882689,\n",
       "    'permissions': '664',\n",
       "    'path': '../.ipynb_checkpoints/analyzer-checkpoint.ipynb'},\n",
       "   {'name': 'llm_factory-checkpoint.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 104749,\n",
       "    'modified': 1758050950.6899283,\n",
       "    'permissions': '664',\n",
       "    'path': '../.ipynb_checkpoints/llm_factory-checkpoint.ipynb'},\n",
       "   {'name': 'schemas-checkpoint.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 13262,\n",
       "    'modified': 1758110822.1125474,\n",
       "    'permissions': '664',\n",
       "    'path': '../.ipynb_checkpoints/schemas-checkpoint.ipynb'},\n",
       "   {'name': 'tokens-checkpoint.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 72,\n",
       "    'modified': 1757871031.1988478,\n",
       "    'permissions': '664',\n",
       "    'path': '../.ipynb_checkpoints/tokens-checkpoint.ipynb'}]},\n",
       " {'name': 'analyzer.ipynb',\n",
       "  'type': 'file',\n",
       "  'size': 33594,\n",
       "  'modified': 1758055336.882689,\n",
       "  'permissions': '664',\n",
       "  'path': '../analyzer.ipynb'},\n",
       " {'name': 'llm_factory.ipynb',\n",
       "  'type': 'file',\n",
       "  'size': 104749,\n",
       "  'modified': 1758050950.6899283,\n",
       "  'permissions': '664',\n",
       "  'path': '../llm_factory.ipynb'},\n",
       " {'name': 'schemas.ipynb',\n",
       "  'type': 'file',\n",
       "  'size': 13262,\n",
       "  'modified': 1758110822.1125474,\n",
       "  'permissions': '664',\n",
       "  'path': '../schemas.ipynb'},\n",
       " {'name': 'tokens.ipynb',\n",
       "  'type': 'file',\n",
       "  'size': 5991,\n",
       "  'modified': 1757871156.998925,\n",
       "  'permissions': '664',\n",
       "  'path': '../tokens.ipynb'},\n",
       " {'name': 'tools',\n",
       "  'type': 'directory',\n",
       "  'size': None,\n",
       "  'modified': 1758112609.0213854,\n",
       "  'permissions': '775',\n",
       "  'path': '../tools',\n",
       "  'children': [{'name': '.ipynb_checkpoints',\n",
       "    'type': 'directory',\n",
       "    'size': None,\n",
       "    'modified': 1758055070.3429048,\n",
       "    'permissions': '775',\n",
       "    'path': '../tools/.ipynb_checkpoints',\n",
       "    'children': [{'name': 'base-checkpoint.ipynb',\n",
       "      'type': 'file',\n",
       "      'size': 1797,\n",
       "      'modified': 1757874498.0214076,\n",
       "      'permissions': '664',\n",
       "      'path': '../tools/.ipynb_checkpoints/base-checkpoint.ipynb'},\n",
       "     {'name': 'code_interpreter-checkpoint.ipynb',\n",
       "      'type': 'file',\n",
       "      'size': 5925,\n",
       "      'modified': 1757878985.207363,\n",
       "      'permissions': '664',\n",
       "      'path': '../tools/.ipynb_checkpoints/code_interpreter-checkpoint.ipynb'},\n",
       "     {'name': 'code_quality-checkpoint.ipynb',\n",
       "      'type': 'file',\n",
       "      'size': 5073,\n",
       "      'modified': 1757879002.9615881,\n",
       "      'permissions': '664',\n",
       "      'path': '../tools/.ipynb_checkpoints/code_quality-checkpoint.ipynb'},\n",
       "     {'name': 'debate_agent-checkpoint.ipynb',\n",
       "      'type': 'file',\n",
       "      'size': 5560,\n",
       "      'modified': 1757879217.7752368,\n",
       "      'permissions': '664',\n",
       "      'path': '../tools/.ipynb_checkpoints/debate_agent-checkpoint.ipynb'},\n",
       "     {'name': 'doc_generator-checkpoint.ipynb',\n",
       "      'type': 'file',\n",
       "      'size': 6288,\n",
       "      'modified': 1757879025.6616397,\n",
       "      'permissions': '664',\n",
       "      'path': '../tools/.ipynb_checkpoints/doc_generator-checkpoint.ipynb'},\n",
       "     {'name': 'execute_bash-checkpoint.ipynb',\n",
       "      'type': 'file',\n",
       "      'size': 4469,\n",
       "      'modified': 1757879432.9576962,\n",
       "      'permissions': '664',\n",
       "      'path': '../tools/.ipynb_checkpoints/execute_bash-checkpoint.ipynb'},\n",
       "     {'name': 'fs_read-checkpoint.ipynb',\n",
       "      'type': 'file',\n",
       "      'size': 30349,\n",
       "      'modified': 1758110171.1147003,\n",
       "      'permissions': '664',\n",
       "      'path': '../tools/.ipynb_checkpoints/fs_read-checkpoint.ipynb'},\n",
       "     {'name': 'fs_write-checkpoint.ipynb',\n",
       "      'type': 'file',\n",
       "      'size': 13154,\n",
       "      'modified': 1757878749.2928915,\n",
       "      'permissions': '664',\n",
       "      'path': '../tools/.ipynb_checkpoints/fs_write-checkpoint.ipynb'},\n",
       "     {'name': 'introspect-checkpoint.ipynb',\n",
       "      'type': 'file',\n",
       "      'size': 5433,\n",
       "      'modified': 1757878919.9904726,\n",
       "      'permissions': '664',\n",
       "      'path': '../tools/.ipynb_checkpoints/introspect-checkpoint.ipynb'},\n",
       "     {'name': 'manager-checkpoint.ipynb',\n",
       "      'type': 'file',\n",
       "      'size': 3272,\n",
       "      'modified': 1757880080.9728749,\n",
       "      'permissions': '664',\n",
       "      'path': '../tools/.ipynb_checkpoints/manager-checkpoint.ipynb'},\n",
       "     {'name': 'memory-checkpoint.ipynb',\n",
       "      'type': 'file',\n",
       "      'size': 4747,\n",
       "      'modified': 1757879044.6078825,\n",
       "      'permissions': '664',\n",
       "      'path': '../tools/.ipynb_checkpoints/memory-checkpoint.ipynb'},\n",
       "     {'name': 'task_executor-checkpoint.ipynb',\n",
       "      'type': 'file',\n",
       "      'size': 26181,\n",
       "      'modified': 1758055201.0381222,\n",
       "      'permissions': '664',\n",
       "      'path': '../tools/.ipynb_checkpoints/task_executor-checkpoint.ipynb'},\n",
       "     {'name': 'task_monitor-checkpoint.ipynb',\n",
       "      'type': 'file',\n",
       "      'size': 72,\n",
       "      'modified': 1758054969.8982177,\n",
       "      'permissions': '664',\n",
       "      'path': '../tools/.ipynb_checkpoints/task_monitor-checkpoint.ipynb'},\n",
       "     {'name': 'task_planner-checkpoint.ipynb',\n",
       "      'type': 'file',\n",
       "      'size': 13216,\n",
       "      'modified': 1758055295.118698,\n",
       "      'permissions': '664',\n",
       "      'path': '../tools/.ipynb_checkpoints/task_planner-checkpoint.ipynb'},\n",
       "     {'name': 'todo-checkpoint.ipynb',\n",
       "      'type': 'file',\n",
       "      'size': 6471,\n",
       "      'modified': 1757879077.3866436,\n",
       "      'permissions': '664',\n",
       "      'path': '../tools/.ipynb_checkpoints/todo-checkpoint.ipynb'}]},\n",
       "   {'name': 'base.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 1961,\n",
       "    'modified': 1758053797.4025726,\n",
       "    'permissions': '664',\n",
       "    'path': '../tools/base.ipynb'},\n",
       "   {'name': 'code_interpreter.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 4971,\n",
       "    'modified': 1757879335.5174735,\n",
       "    'permissions': '664',\n",
       "    'path': '../tools/code_interpreter.ipynb'},\n",
       "   {'name': 'code_quality.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 5499,\n",
       "    'modified': 1757879455.5923398,\n",
       "    'permissions': '664',\n",
       "    'path': '../tools/code_quality.ipynb'},\n",
       "   {'name': 'debate_agent.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 4495,\n",
       "    'modified': 1757879455.4293423,\n",
       "    'permissions': '664',\n",
       "    'path': '../tools/debate_agent.ipynb'},\n",
       "   {'name': 'doc_generator.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 5253,\n",
       "    'modified': 1757879455.4213424,\n",
       "    'permissions': '664',\n",
       "    'path': '../tools/doc_generator.ipynb'},\n",
       "   {'name': 'execute_bash.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 4469,\n",
       "    'modified': 1757879432.9576962,\n",
       "    'permissions': '664',\n",
       "    'path': '../tools/execute_bash.ipynb'},\n",
       "   {'name': 'fs_read.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 41637,\n",
       "    'modified': 1758112609.0173852,\n",
       "    'permissions': '664',\n",
       "    'path': '../tools/fs_read.ipynb'},\n",
       "   {'name': 'fs_write.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 13539,\n",
       "    'modified': 1757879575.3207572,\n",
       "    'permissions': '664',\n",
       "    'path': '../tools/fs_write.ipynb'},\n",
       "   {'name': 'introspect.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 5854,\n",
       "    'modified': 1757879575.4067562,\n",
       "    'permissions': '664',\n",
       "    'path': '../tools/introspect.ipynb'},\n",
       "   {'name': 'manager.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 6209,\n",
       "    'modified': 1758054835.971331,\n",
       "    'permissions': '664',\n",
       "    'path': '../tools/manager.ipynb'},\n",
       "   {'name': 'memory.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 5167,\n",
       "    'modified': 1757879575.4967551,\n",
       "    'permissions': '664',\n",
       "    'path': '../tools/memory.ipynb'},\n",
       "   {'name': 'task_executor.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 26358,\n",
       "    'modified': 1758055527.8175037,\n",
       "    'permissions': '664',\n",
       "    'path': '../tools/task_executor.ipynb'},\n",
       "   {'name': 'task_monitor.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 8643,\n",
       "    'modified': 1758055100.7359765,\n",
       "    'permissions': '664',\n",
       "    'path': '../tools/task_monitor.ipynb'},\n",
       "   {'name': 'task_planner.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 13216,\n",
       "    'modified': 1758055404.5901303,\n",
       "    'permissions': '664',\n",
       "    'path': '../tools/task_planner.ipynb'},\n",
       "   {'name': 'todo.ipynb',\n",
       "    'type': 'file',\n",
       "    'size': 5423,\n",
       "    'modified': 1757879575.504755,\n",
       "    'permissions': '664',\n",
       "    'path': '../tools/todo.ipynb'}]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = FsReadTool()\n",
    "fs._list_directory_tree(\"../\",2,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9bac58-1d93-4240-b488-2256b330457f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytest'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtempfile\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytest\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      9\u001b[39m \u001b[38;5;129m@pytest\u001b[39m.fixture\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample_dir_structure\u001b[39m():\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pytest'"
     ]
    }
   ],
   "source": [
    "# | export \n",
    "import os\n",
    "import tempfile\n",
    "import pytest\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_dir_structure():\n",
    "    \"\"\"Create a temporary directory with files for testing.\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        root = Path(tmpdir)\n",
    "        # Create some files\n",
    "        (root / \"file1.txt\").write_text(\"hello world\\nfoo bar\\nbaz qux\\n\")\n",
    "        (root / \"file2.log\").write_text(\"log line 1\\nlog line 2\\n\")\n",
    "        # Create subdirectory\n",
    "        subdir = root / \"subdir\"\n",
    "        subdir.mkdir()\n",
    "        (subdir / \"nested.txt\").write_text(\"nested content\\nanother line\\n\")\n",
    "        yield root\n",
    "\n",
    "\n",
    "def test_directory_mode_lists_files(sample_dir_structure):\n",
    "    tool = FsReadTool()\n",
    "    ops = [{\"mode\": \"Directory\", \"path\": str(sample_dir_structure), \"depth\": 2}]\n",
    "    result = tool.execute(ops)\n",
    "    assert \"results\" in result\n",
    "    items = result[\"results\"][0][\"items\"]\n",
    "    assert any(\"file1.txt\" in str(i) for i in items)\n",
    "    assert any(\"subdir\" in str(i) for i in items)\n",
    "\n",
    "\n",
    "def test_line_mode_reads_partial_content(sample_dir_structure):\n",
    "    tool = FsReadTool()\n",
    "    file_path = sample_dir_structure / \"file1.txt\"\n",
    "    ops = [{\"mode\": \"Line\", \"path\": str(file_path), \"start_line\": 2, \"end_line\": 3}]\n",
    "    result = tool.execute(ops)\n",
    "    content = result[\"results\"][0][\"content\"]\n",
    "    assert \"foo bar\" in content\n",
    "    assert \"hello world\" not in content  # because we started from line 2\n",
    "\n",
    "\n",
    "def test_find_mode_finds_files(sample_dir_structure):\n",
    "    tool = FsReadTool()\n",
    "    ops = [{\"mode\": \"Find\", \"path\": str(sample_dir_structure), \"name_pattern\": \"*.txt\"}]\n",
    "    result = tool.execute(ops)\n",
    "    found = result[\"results\"][0][\"found\"]\n",
    "    assert any(\"file1.txt\" in str(f) for f in found)\n",
    "    assert any(\"nested.txt\" in str(f) for f in found)\n",
    "\n",
    "\n",
    "def test_grep_mode_finds_pattern(sample_dir_structure):\n",
    "    tool = FsReadTool()\n",
    "    ops = [{\"mode\": \"Grep\", \"path\": str(sample_dir_structure), \"pattern\": \"log line\", \"file_pattern\": \"*.log\"}]\n",
    "    result = tool.execute(ops)\n",
    "    matches = result[\"results\"][0][\"matches\"]\n",
    "    assert any(\"log line 1\" in m[\"line\"] for m in matches)\n",
    "    assert all(m[\"file\"].endswith(\".log\") for m in matches)\n",
    "\n",
    "\n",
    "def test_invalid_mode_returns_error(sample_dir_structure):\n",
    "    tool = FsReadTool()\n",
    "    ops = [{\"mode\": \"InvalidMode\", \"path\": str(sample_dir_structure)}]\n",
    "    result = tool.execute(ops)\n",
    "    assert \"error\" in result[\"results\"][0]\n",
    "    assert \"Invalid mode\" in result[\"results\"][0][\"error\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db20bb9-f458-41c1-b7bc-7ed3909d80c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
