{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8ec34ab-ef7b-4f99-9d09-00934f31feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp backend.llms.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d47375d7-1f2c-4efe-b2c6-9b55db3f61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "from typing import Dict, Any, List, Optional, Iterator\n",
    "from openai import OpenAI\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63494c70-07c9-47c6-8874-ce027082137f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'agentic.backend.llms'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# | export\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfigs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_model_config\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresponse_processor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResponseProcessor\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstreaming_handler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StreamingHandler\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'agentic.backend.llms'"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "from agentic.configs.loader import get_model_config\n",
    "from agentic.backend.llms.response_processor import ResponseProcessor\n",
    "from agentic.backend.llms.streaming_handler import StreamingHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac836d-f04d-428e-8f4f-f47be660f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class LLMClient:\n",
    "    \"\"\"Enhanced LLM client\"\"\"\n",
    "    \n",
    "    def __init__(self, model: Optional[str] = None, base_url: Optional[str] = None, \n",
    "                 api_key: Optional[str] = None):\n",
    "        # Load config defaults\n",
    "        model_config = get_model_config()\n",
    "        settings_config = get_settings_config()\n",
    "        \n",
    "        # Set parameters with fallbacks\n",
    "        self.model = model or model_config.get('name', 'qwen3:8b')\n",
    "        self.base_url = base_url or model_config.get('url', 'http://localhost:11434/v1')\n",
    "        self.api_key = api_key or model_config.get('api_key', 'ollama')\n",
    "\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI(base_url=self.base_url, \n",
    "                             api_key=self.api_key,\n",
    "                             timeout=timeout)\n",
    "        \n",
    "        # Initialize processors\n",
    "        self.response_processor = ResponseProcessor()\n",
    "        self.streaming_handler = StreamingHandler()\n",
    "        \n",
    "        # Validate connection\n",
    "        self._validate_connection()\n",
    "    \n",
    "    def _validate_connection(self):\n",
    "        \"\"\"Validate LLM connection\"\"\"\n",
    "        try:\n",
    "            # Test with a simple request\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "                max_tokens=1,\n",
    "                timeout=5\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ LLM connection warning: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def create_completion(self, messages: List[Dict[str, Any]], \n",
    "                         tools: Optional[List[Dict]] = None,\n",
    "                         stream: bool = True, **kwargs) -> Any:\n",
    "        \"\"\"Create chat completion with optional tools\"\"\"\n",
    "        completion_params = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": stream,\n",
    "            **kwargs\n",
    "        }\n",
    "        \n",
    "        if tools:\n",
    "            completion_params[\"tools\"] = tools\n",
    "            completion_params[\"tool_choice\"] = \"auto\"\n",
    "        \n",
    "        try:\n",
    "            return self.client.chat.completions.create(**completion_params)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"LLM completion failed: {e}\")\n",
    "    \n",
    "    def process_response(self, response: Any, console: Optional[Console] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Process non-streaming response\"\"\"\n",
    "        return self.response_processor.process_response(response, console)\n",
    "    \n",
    "    def handle_streaming_response(self, response: Iterator, console: Optional[Console] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Handle streaming response\"\"\"\n",
    "        return self.streaming_handler.handle_streaming_response(response, console)\n",
    "    \n",
    "    def get_model_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get information about the current model\"\"\"\n",
    "        return {\n",
    "            \"model\": self.model,\n",
    "            \"base_url\": self.base_url,\n",
    "            \"api_key_set\": bool(self.api_key),\n",
    "            \"connection_valid\": self._validate_connection()\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
