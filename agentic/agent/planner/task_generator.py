# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../nbs/buddy/backend/agents/planner/task_generator.ipynb.

# %% auto 0
__all__ = ['TaskGenerator']

# %% ../../../nbs/buddy/backend/agents/planner/task_generator.ipynb 1
import json
import re
from typing import Optional
from rich.console import Console
from .models import Task, ProjectBreakdown, ProjectContext


# %% ../../../nbs/buddy/backend/agents/planner/task_generator.ipynb 2
class TaskGenerator:
    """Handles task generation and validation"""
    
    def __init__(self, agent, console: Console):
        self.agent = agent
        self.console = console
    
    def generate_next_task(self, context: ProjectContext, breakdown: ProjectBreakdown, estimated_total: int) -> Optional[Task]:
        """Generate single next task using project breakdown context"""
        
        context_prompt = self._build_context_prompt(context)
        execution_context = self._get_execution_context(context)
        
        completed_count = len(context.execution_history)
        next_task_id = f"T{completed_count + 1:03d}"
        
        prompt = f"""
CRITICAL: You MUST generate EXACTLY ONE task object (not an array). Generate the NEXT logical task in the project sequence.

CURRENT PROGRESS: {completed_count} tasks completed out of estimated {estimated_total}

{execution_context}

PROJECT BREAKDOWN CONTEXT:
Summary: {breakdown.project_summary}
Phases: {', '.join(breakdown.phases)}
Key Deliverables: {', '.join(breakdown.key_deliverables)}
Technical Approach: {breakdown.technical_approach}

ORIGINAL REQUEST: {context.original_request}

{context_prompt}

Based on the project breakdown and what has been completed, generate the NEXT SINGLE TASK. 

TASK PROGRESSION LOGIC:
- If 0-2 tasks completed: Focus on project setup, requirements analysis, structure creation
- If 3-5 tasks completed: Focus on core implementation, main components
- If 6-8 tasks completed: Focus on integration, data handling, advanced features  
- If 9+ tasks completed: Focus on testing, optimization, deployment

Generate a task that logically follows the completed work and moves toward the key deliverables.

RESPOND WITH SINGLE TASK OBJECT:
{{
  "id": "{next_task_id}",
  "name": "Next Task Name",
  "description": "Task description aligned with project phases",
  "dependencies": [],
  "actions": [
    {{
      "step": 1,
      "purpose": "Main action purpose",
      "sub_steps": ["Specific step 1", "Specific step 2"],
      "introspect_after": true,
      "system_prompt": "You are a developer. CONTEXT: {breakdown.project_summary}. Your task is to...",
      "user_prompt": "Based on the project requirements, please...",
      "introspect_prompt": "Validate that the task was completed by checking...",
      "execution_mode": "sequential"
    }}
  ],
  "success_criteria": "Specific success criteria",
  "expected_outputs": ["output1.py"],
  "potential_options": [],
  "needs_debate": false
}}

IMPORTANT: Always generate a valid task object. Only return null if truly no more work is needed (which should be rare for complex projects).
"""
        
        try:
            response = self.agent.run(prompt).get("content", "")
            
            # Save raw response for debugging
            with open("task_generation_raw.txt", 'w') as f:
                f.write(response)
            
            # Parse and validate task
            response = response.strip()
            
            # Only return None if explicitly null AND we have enough tasks
            if response.lower() == "null":
                if context.total_tasks_completed >= 5:
                    return None
                else:
                    self.console.print("⚠️ LLM returned null but project needs more tasks. Regenerating...")
                    return self._force_generate_next_task(context, breakdown, estimated_total)
            
            # Try to extract JSON from response
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                task_data = json.loads(json_str)
            else:
                task_data = json.loads(response)
                
        except json.JSONDecodeError as e:
            self.console.print(f"❌ JSON parsing failed: {e}")
            task_data = self._extract_task_from_text(response, context)
            if not task_data:
                self.console.print("❌ Failed to extract task from response")
                return None
        
        # If LLM returns array, take only the first task
        if isinstance(task_data, list):
            if len(task_data) > 0:
                task_data = task_data[0]
                self.console.print("⚠️ LLM returned multiple tasks, using only the first one")
            else:
                self.console.print("⚠️ LLM returned empty task array")
                return None
        
        # Validate required fields
        if not isinstance(task_data, dict) or 'name' not in task_data:
            self.console.print("⚠️ Invalid task format returned by LLM")
            return None
        
        task = Task(**task_data)
        task.id = f"T{len(context.execution_history) + 1:03d}"
        
        # Check for task repetition
        for prev_task in context.execution_history:
            if task.name.lower().strip() == prev_task.task_name.lower().strip():
                self.console.print(f"⚠️ Task '{task.name}' appears to be a repeat. Skipping.")
                return None
        
        return task
    
    def regenerate_task_with_feedback(self, context: ProjectContext, breakdown: ProjectBreakdown, feedback: str) -> Optional[Task]:
        """Regenerate task incorporating introspection feedback"""
        context_prompt = self._build_context_prompt(context)
        execution_context = self._get_execution_context(context)
        
        prompt = f"""
CRITICAL: The previously generated task was rejected. Generate a BETTER task incorporating the feedback.

FEEDBACK FROM VALIDATION: {feedback}

{execution_context}

PROJECT BREAKDOWN CONTEXT:
Summary: {breakdown.project_summary}
Phases: {', '.join(breakdown.phases)}
Key Deliverables: {', '.join(breakdown.key_deliverables)}

{context_prompt}

Generate ONE improved task that addresses the validation feedback. Ensure it's unique and not a repeat.

RESPOND WITH SINGLE TASK OBJECT OR NULL:
"""
        
        try:
            agent_response = self.agent.run(prompt)
            response = agent_response.get("content", "") if isinstance(agent_response, dict) else str(agent_response)
            
            if response.strip().lower() == "null":
                return None
                
            task_data = json.loads(response.strip())
            
        except json.JSONDecodeError:
            task_data = self._extract_task_from_text(response.strip(), context)
            if not task_data:
                raise ValueError("Failed to parse JSON and extract task fields. Please provide valid JSON format with required fields: id, name, description, actions.")
            
            if isinstance(task_data, list) and len(task_data) > 0:
                task_data = task_data[0]
            elif isinstance(task_data, list):
                return None
                
            if not isinstance(task_data, dict) or 'name' not in task_data:
                return None
                
            task = Task(**task_data)
            task.id = f"T{len(context.execution_history) + 1:03d}"
            
            return task
            
        except Exception as e:
            self.console.print(f"❌ Task regeneration failed: {e}")
            return None
    
    def _force_generate_next_task(self, context: ProjectContext, breakdown: ProjectBreakdown, estimated_total: int) -> Optional[Task]:
        """Force generation of next task when LLM returns null prematurely"""
        
        completed_count = len(context.execution_history)
        next_task_id = f"T{completed_count + 1:03d}"
        
        # Determine what type of task is needed based on progress
        if completed_count < 3:
            task_type = "setup and structure"
            example_tasks = ["Create project structure", "Setup dependencies", "Configure environment"]
        elif completed_count < 6:
            task_type = "core implementation"
            example_tasks = ["Implement main scraper", "Add rate limiting", "Create data models"]
        else:
            task_type = "integration and testing"
            example_tasks = ["Add error handling", "Create tests", "Setup deployment"]
        
        prompt = f"""
CRITICAL: The project needs more tasks to be complete. You MUST generate a task.

CURRENT STATUS: Only {completed_count} tasks completed - this is insufficient for a complete web scraping framework.

REQUIRED TASK TYPE: {task_type}
EXAMPLES: {', '.join(example_tasks)}

PROJECT CONTEXT:
- Original Request: {context.original_request}
- Project Summary: {breakdown.project_summary}
- Completed Tasks: {[t.task_name for t in context.execution_history]}

Generate a {task_type} task that is essential for completing the web scraping framework.

RESPOND WITH TASK OBJECT:
{{
  "id": "{next_task_id}",
  "name": "Essential Task Name",
  "description": "Description of essential task",
  "dependencies": [],
  "actions": [
    {{
      "step": 1,
      "purpose": "Main purpose",
      "sub_steps": ["Step 1", "Step 2"],
      "introspect_after": true,
      "system_prompt": "You are a developer working on a web scraping framework. Complete this essential task.",
      "user_prompt": "Implement the required functionality for the web scraping framework.",
      "introspect_prompt": "Validate that the implementation is correct and functional.",
      "execution_mode": "sequential"
    }}
  ],
  "success_criteria": "Task completion criteria",
  "expected_outputs": ["output.py"],
  "potential_options": [],
  "needs_debate": false
}}
"""
        
        try:
            response = self.agent.run(prompt).get("content", "")
            
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                task_data = json.loads(json_match.group())
            else:
                task_data = json.loads(response.strip())
            
            if isinstance(task_data, dict) and 'name' in task_data:
                task = Task(**task_data)
                task.id = next_task_id
                return task
                
        except Exception as e:
            self.console.print(f"❌ Force task generation failed: {e}")
        
        return None
    
    def _build_context_prompt(self, context: ProjectContext) -> str:
        """Build comprehensive context from execution history"""
        if not context.execution_history:
            return "PREVIOUS CONTEXT: This is the first task."
        
        context_parts = ["EXECUTION HISTORY:"]
        
        for i, task_result in enumerate(context.execution_history, 1):
            status_emoji = "✅" if task_result.status == "success" else "❌"
            context_parts.append(f"{i}. {task_result.task_name} (ID: {task_result.task_id}) - {status_emoji} {task_result.status}")
            context_parts.append(f"   Description: {task_result.task_name}")
            if task_result.artifacts_created:
                if len(task_result.artifacts_created) <= 3:
                    context_parts.append(f"   Artifacts: {', '.join(task_result.artifacts_created)}")
                else:
                    shown = ', '.join(task_result.artifacts_created[:3])
                    remaining = len(task_result.artifacts_created) - 3
                    context_parts.append(f"   Artifacts: {shown} ... (+{remaining} more)")
        
        context_parts.append(f"\nTOTAL ARTIFACTS: {len(context.current_artifacts)}")
        context_parts.append(f"PROJECT STATUS: {context.project_status}")
        context_parts.append(f"PROGRESS: {context.total_tasks_completed}/{len(context.execution_history)} tasks completed")
        
        return "\n".join(context_parts)
    
    def _get_execution_context(self, context: ProjectContext) -> str:
        """Generate minimal context for next task generation"""
        if not context.execution_history:
            return "CONTEXT: This is the first task."
        
        last_task = context.execution_history[-1].task_name
        artifacts_count = len(context.current_artifacts)
        
        return f"CONTEXT: Last completed task: '{last_task}'. Total artifacts created: {artifacts_count}."
    
    def _extract_task_from_text(self, text: str, context: ProjectContext) -> Optional[dict]:
        """Extract task fields from malformed JSON/text as fallback"""
        
        json_match = re.search(r'\{.*\}', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group())
            except:
                pass
        
        task_data = {}
        
        if match := re.search(r'"?id"?\s*:\s*"?([^",\n]+)"?', text, re.IGNORECASE):
            task_data["id"] = match.group(1).strip('"')
        
        if match := re.search(r'"?name"?\s*:\s*"([^"]+)"', text, re.IGNORECASE):
            task_data["name"] = match.group(1)
        
        if match := re.search(r'"?description"?\s*:\s*"([^"]+)"', text, re.IGNORECASE):
            task_data["description"] = match.group(1)
        
        if "name" in task_data:
            task_data.setdefault("id", f"T{len(context.execution_history) + 1:03d}")
            task_data.setdefault("description", task_data["name"])
            task_data.setdefault("dependencies", [])
            task_data.setdefault("success_criteria", f"Complete {task_data['name']}")
            task_data.setdefault("expected_outputs", [])
            task_data.setdefault("potential_options", [])
            task_data.setdefault("needs_debate", False)
            
            task_data.setdefault("actions", [{
                "step": 1,
                "purpose": task_data["name"],
                "sub_steps": [],
                "introspect_after": True,
                "system_prompt": f"You are an AI assistant. Complete the task: {task_data['name']}",
                "user_prompt": task_data["description"],
                "introspect_prompt": f"Validate that {task_data['name']} was completed successfully",
                "execution_mode": "sequential"
            }])
            
            return task_data
        
        return None

