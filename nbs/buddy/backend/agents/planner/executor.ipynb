{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3befa7-7d62-40b0-9577-2c64b5ca7c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp agent.planner.executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f88181b2-65ac-4122-ba8b-ad92cd683acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 11:28:28,425 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 11:28:28,430 - INFO - Initialized LLM client with model: gpt-oss:20b\n",
      "2025-09-30 11:28:28,519 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 11:28:28,520 - INFO - Initialized LLM client with model: gpt-oss:20b\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üìã Decomposing into tasks<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üìã Decomposing into tasks\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 11:28:29,425 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;2;200;100;120m‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ü§î Thinking ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\u001b[0m\n",
      "\u001b[38;2;200;100;120m‚îÇ \u001b[38;2;200;100;120mWe\u001b[0m\u001b[38;2;200;100;120m need\u001b[0m\u001b[38;2;200;100;120m to\u001b[0m\u001b[38;2;200;100;120m produce\u001b[0m\u001b[38;2;200;100;120m a\u001b[0m\u001b[38;2;200;100;120m JSON\u001b[0m\u001b[38;2;200;100;120m array\u001b[0m\u001b[38;2;200;100;120m of\u001b[0m\u001b[38;2;200;100;120m tasks\u001b[0m\u001b[38;2;200;100;120m with\u001b[0m\u001b[38;2;200;100;120m detailed\u001b[0m\u001b[38;2;200;100;120m sub\u001b[0m\u001b[38;2;200;100;120msteps\u001b[0m\u001b[38;2;200;100;120m,\u001b[0m\u001b[38;2;200;100;120m intros\u001b[0m\u001b[38;2;200;100;120mpection\u001b[0m\u001b[38;2;200;100;120m,\u001b[0m\u001b[38;2;200;100;120m dependencies\u001b[0m\u001b[38;2;200;100;120m,\u001b[0m\u001b[38;2;200;100;120m etc\u001b[0m\u001b[38;2;200;100;120m.\u001b[0m\u001b[38;2;200;100;120m First\u001b[0m\u001b[38;2;200;100;120m task\u001b[0m\u001b[38;2;200;100;120m is\u001b[0m\u001b[38;2;200;100;120m Requirement\u001b[0m\u001b[38;2;200;100;120m Analysis\u001b[0m\u001b[38;2;200;100;120m.\u001b[0m\u001b[38;2;200;100;120m Use\u001b[0m\u001b[38;2;200;100;120m code\u001b[0m\u001b[38;2;200;100;120m_inter\u001b[0m\u001b[38;2;200;100;120mpreter\u001b[0m\u001b[38;2;200;100;120m or\u001b[0m\u001b[38;2;200;100;120m intros\u001b[0m\u001b[38;2;200;100;120mpect\u001b[0m\u001b[38;2;200;100;120m.\u001b[0m\u001b[38;2;200;100;120m Also\u001b[0m\u001b[38;2;200;100;120m possibly\u001b[0m\u001b[38;2;200;100;120m debate\u001b[0m\u001b[38;2;200;100;120m_agent\u001b[0m\u001b[38;2;200;100;120m if\u001b[0m\u001b[38;2;200;100;120m frameworks\u001b[0m\u001b[38;2;200;100;120m chosen\u001b[0m\u001b[38;2;200;100;120m.\n",
      "\n",
      "\u001b[0m\u001b[38;2;200;100;120mWe\u001b[0m\u001b[38;2;200;100;120m only\u001b[0m\u001b[38;2;200;100;120m have\u001b[0m\u001b[38;2;200;100;120m one\u001b[0m\u001b[38;2;200;100;120m user\u001b[0m\u001b[38;2;200;100;120m prompt\u001b[0m\u001b[38;2;200;100;120m;\u001b[0m\u001b[38;2;200;100;120m we\u001b[0m\u001b[38;2;200;100;120m must\u001b[0m\u001b[38;2;200;100;120m propose\u001b[0m\u001b[38;2;200;100;120m tasks\u001b[0m\u001b[38;2;200;100;120m.\u001b[0m\u001b[38;2;200;100;120m This\u001b[0m\u001b[38;2;200;100;120m is\u001b[0m\u001b[38;2;200;100;120m a\u001b[0m\u001b[38;2;200;100;120m long\u001b[0m\u001b[38;2;200;100;120m response\u001b[0m\u001b[38;2;200;100;120m but\u001b[0m\u001b[38;2;200;100;120m must\u001b[0m\u001b[38;2;200;100;120m produce\u001b[0m\u001b[38;2;200;100;120m JSON\u001b[0m\u001b[38;2;200;100;120m.\u001b[0m\u001b[38;2;200;100;120m Need\u001b[0m\u001b[38;2;200;100;120m to\u001b[0m\u001b[38;2;200;100;120m structure\u001b[0m\u001b[38;2;200;100;120m tasks\u001b[0m\u001b[38;2;200;100;120m step\u001b[0m\u001b[38;2;200;100;120m by\u001b[0m\u001b[38;2;200;100;120m step\u001b[0m\u001b[38;2;200;100;120m.\n",
      "\n",
      "\u001b[0m\u001b[38;2;200;100;120mWe\u001b[0m\u001b[38;2;200;100;120m should\u001b[0m\u001b[38;2;200;100;120m think\u001b[0m\u001b[38;2;200;100;120m of\u001b[0m\u001b[38;2;200;100;120m high\u001b[0m"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Agent, AgentConfig\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfigs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_model_config\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplanner\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProjectContext, ExecutionStatus\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplanner\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbreakdown\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProjectBreakdownGenerator\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplanner\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtask_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TaskGenerator\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/applied-GenAI-lab/agentic/agents/planner.py:678\u001b[39m\n\u001b[32m    676\u001b[39m config = AgentConfig(name=\u001b[33m\"\u001b[39m\u001b[33mtask_analyzer\u001b[39m\u001b[33m\"\u001b[39m, model=\u001b[33m\"\u001b[39m\u001b[33mgpt-oss:20b\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    677\u001b[39m analyzer = TaskAnalyzer(config)\n\u001b[32m--> \u001b[39m\u001b[32m678\u001b[39m result = \u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_to_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDesign and implement a generic, configuration-driven time series forecasting framework that supports multiple models, automated preprocessing, feature engineering, hyperparameter tuning, and scalable deployment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    680\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28mprint\u001b[39m(json.dumps(result, indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/applied-GenAI-lab/agentic/agents/planner.py:671\u001b[39m, in \u001b[36mTaskAnalyzer.analyze_to_json\u001b[39m\u001b[34m(self, user_input)\u001b[39m\n\u001b[32m    669\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manalyze_to_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, user_input: \u001b[38;5;28mstr\u001b[39m) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    670\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Analyze and return JSON dict\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    672\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result.model_dump(mode=\u001b[33m'\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/applied-GenAI-lab/agentic/agents/planner.py:481\u001b[39m, in \u001b[36mTaskAnalyzer.analyze\u001b[39m\u001b[34m(self, user_input)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    480\u001b[39m     \u001b[38;5;28mself\u001b[39m.console.print(\u001b[33m\"\u001b[39m\u001b[33müìã Decomposing into tasks...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     tasks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decompose_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    482\u001b[39m     \u001b[38;5;28mself\u001b[39m.console.print(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Created \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tasks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tasks\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    484\u001b[39m     \u001b[38;5;28mself\u001b[39m.console.print(\u001b[33m\"\u001b[39m\u001b[33müîÑ Building execution plan...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/applied-GenAI-lab/agentic/agents/planner.py:534\u001b[39m, in \u001b[36mTaskAnalyzer._decompose_tasks\u001b[39m\u001b[34m(self, user_input)\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"Decompose input into tasks with repair\"\"\"\u001b[39;00m\n\u001b[32m    507\u001b[39m         prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[33mDecompose this software engineering task into a sequence of technical tasks: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    509\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    532\u001b[39m \u001b[33mOutput as JSON array: [...]\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_llm_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m         \u001b[38;5;28mself\u001b[39m.console.print(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[DEBUG] Raw LLM response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    537\u001b[39m         task_data = \u001b[38;5;28mself\u001b[39m.json_repair.repair_json(response, user_input)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/applied-GenAI-lab/agentic/agents/planner.py:606\u001b[39m, in \u001b[36mTaskAnalyzer._call_llm_with_retry\u001b[39m\u001b[34m(self, prompt, max_retries)\u001b[39m\n\u001b[32m    604\u001b[39m     messages = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}]\n\u001b[32m    605\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.llm_client.create_completion(messages=messages, stream=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_streaming_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result.get(\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(result)\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/applied-GenAI-lab/agentic/llms/client.py:96\u001b[39m, in \u001b[36mLLMClient.handle_streaming_response\u001b[39m\u001b[34m(self, response, console)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandle_streaming_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, response: Iterator, console: Optional[Console] = \u001b[38;5;28;01mNone\u001b[39;00m) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m     95\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Handle streaming response\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstreaming_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_streaming_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsole\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/applied-GenAI-lab/agentic/llms/streaming_handler.py:88\u001b[39m, in \u001b[36mStreamingHandler.handle_streaming_response\u001b[39m\u001b[34m(self, response, console)\u001b[39m\n\u001b[32m     86\u001b[39m         markdown_line_buffer = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdelta\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/applied-GenAI-lab/.venv/lib/python3.12/site-packages/openai/_streaming.py:46\u001b[39m, in \u001b[36mStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[_T]:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/applied-GenAI-lab/.venv/lib/python3.12/site-packages/openai/_streaming.py:58\u001b[39m, in \u001b[36mStream.__stream__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m process_data = \u001b[38;5;28mself\u001b[39m._client._process_response_data\n\u001b[32m     56\u001b[39m iterator = \u001b[38;5;28mself\u001b[39m._iter_events()\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m[DONE]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/applied-GenAI-lab/.venv/lib/python3.12/site-packages/openai/_streaming.py:50\u001b[39m, in \u001b[36mStream._iter_events\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iter_events\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[ServerSentEvent]:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoder.iter_bytes(\u001b[38;5;28mself\u001b[39m.response.iter_bytes())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/applied-GenAI-lab/.venv/lib/python3.12/site-packages/openai/_streaming.py:280\u001b[39m, in \u001b[36mSSEDecoder.iter_bytes\u001b[39m\u001b[34m(self, iterator)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34miter_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, iterator: Iterator[\u001b[38;5;28mbytes\u001b[39m]) -> Iterator[ServerSentEvent]:\n\u001b[32m    279\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Given an iterator that yields raw binary data, iterate over it & yield every event encountered\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Split before decoding so splitlines() only uses \\r and \\n\u001b[39;49;00m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_line\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m            \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_line\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/applied-GenAI-lab/.venv/lib/python3.12/site-packages/openai/_streaming.py:291\u001b[39m, in \u001b[36mSSEDecoder._iter_chunks\u001b[39m\u001b[34m(self, iterator)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Given an iterator that yields raw binary data, iterate over it and yield individual SSE chunks\"\"\"\u001b[39;00m\n\u001b[32m    290\u001b[39m data = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeepends\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/klcQzImUyM8rktVGcwbC9/lib/python3.12/site-packages/httpx/_models.py:897\u001b[39m, in \u001b[36mResponse.iter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    895\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/klcQzImUyM8rktVGcwbC9/lib/python3.12/site-packages/httpx/_models.py:951\u001b[39m, in \u001b[36mResponse.iter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    948\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes_downloaded\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/klcQzImUyM8rktVGcwbC9/lib/python3.12/site-packages/httpx/_client.py:153\u001b[39m, in \u001b[36mBoundSyncStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/klcQzImUyM8rktVGcwbC9/lib/python3.12/site-packages/httpx/_transports/default.py:127\u001b[39m, in \u001b[36mResponseStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpcore_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/klcQzImUyM8rktVGcwbC9/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/klcQzImUyM8rktVGcwbC9/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/klcQzImUyM8rktVGcwbC9/lib/python3.12/site-packages/httpcore/_sync/http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/klcQzImUyM8rktVGcwbC9/lib/python3.12/site-packages/httpcore/_sync/http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/klcQzImUyM8rktVGcwbC9/lib/python3.12/site-packages/httpcore/_sync/http11.py:203\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/klcQzImUyM8rktVGcwbC9/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/klcQzImUyM8rktVGcwbC9/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# | export\n",
    "import json\n",
    "from typing import Optional\n",
    "from rich.console import Console\n",
    "\n",
    "from agentic.core.agent import Agent, AgentConfig\n",
    "from agentic.configs.loader import get_model_config\n",
    "\n",
    "from agentic.agent.planner.models import ProjectContext, ExecutionStatus\n",
    "from agentic.agent.planner.breakdown import ProjectBreakdownGenerator\n",
    "from agentic.agent.planner.task_generator import TaskGenerator\n",
    "from agentic.agent.planner.task_executor import TaskExecutor\n",
    "from agentic.agent.planner.validation import TaskValidator\n",
    "from agentic.agent.planner.cache import CacheManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e98fa-04c1-4dab-a6fd-6814b7b915f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class DynamicTaskExecutor:\n",
    "    \"\"\"Production-ready dynamic task execution system\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = None):\n",
    "        # Load model from config if not provided\n",
    "        if model is None:\n",
    "            model_config = get_model_config()\n",
    "            model = model_config.get('name', 'qwen3:8b')\n",
    "        self.model = model\n",
    "        config = AgentConfig(\n",
    "            name=\"dynamic_executor\",\n",
    "            instructions=\"You are an AI agent that executes tasks systematically. Analyze requirements, break down complex problems, and provide detailed technical responses with proper implementation.\",\n",
    "            model=model,\n",
    "            tools=[\"fs_read\", \"fs_write\", \"execute_bash\"],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        self.agent = Agent(config)\n",
    "        self.console = Console()\n",
    "        \n",
    "        # Initialize components\n",
    "        self.breakdown_generator = ProjectBreakdownGenerator(self.agent, self.console)\n",
    "        self.task_generator = TaskGenerator(self.agent, self.console)\n",
    "        self.task_executor = TaskExecutor(self.agent, self.console)\n",
    "        self.validator = TaskValidator(self.console)\n",
    "        self.cache_manager = CacheManager(self.console)\n",
    "        \n",
    "        # Configuration\n",
    "        self.max_retries = 3\n",
    "        self.max_tasks = 20\n",
    "        \n",
    "        # State\n",
    "        self.context: Optional[ProjectContext] = None\n",
    "        self.project_breakdown = None\n",
    "        self.estimated_total_tasks: int = 0\n",
    "    \n",
    "    async def execute_project(self, user_request: str) -> ProjectContext:\n",
    "        \"\"\"Main execution loop with two-phase approach\"\"\"\n",
    "        self.console.print(f\"üöÄ Starting dynamic execution for: {user_request}\")\n",
    "        \n",
    "        # Try to restore from cache\n",
    "        restored_context, restored_breakdown = self.cache_manager.restore_from_cache(user_request)\n",
    "        if restored_context and restored_breakdown:\n",
    "            self.console.print(\"üì¶ Restored from cache, continuing execution...\")\n",
    "            self.context = restored_context\n",
    "            self.project_breakdown = restored_breakdown\n",
    "            self.estimated_total_tasks = self.breakdown_generator.estimate_total_tasks(self.project_breakdown)\n",
    "        else:\n",
    "            # Phase 1: Generate project breakdown\n",
    "            self.console.print(\"\\nüìã PHASE 1: Generating project breakdown...\")\n",
    "            self.project_breakdown = self.breakdown_generator.generate_project_breakdown(user_request)\n",
    "            \n",
    "            if not self.project_breakdown:\n",
    "                self.console.print(\"‚ùå Failed to generate project breakdown\")\n",
    "                return ProjectContext(original_request=user_request, project_status=ExecutionStatus.FAILED)\n",
    "            \n",
    "            self.console.print(f\"‚úÖ Project breakdown complete\")\n",
    "            \n",
    "            # Initialize project context\n",
    "            self.context = ProjectContext(original_request=user_request)\n",
    "            \n",
    "            # Estimate total tasks from breakdown\n",
    "            self.estimated_total_tasks = self.breakdown_generator.estimate_total_tasks(self.project_breakdown)\n",
    "            \n",
    "            # Save initial state\n",
    "            self.cache_manager.save_to_cache(self.context, self.project_breakdown)\n",
    "        \n",
    "        # Phase 2: Execute tasks one by one\n",
    "        self.console.print(\"\\nüîÑ PHASE 2: Executing tasks dynamically...\")\n",
    "        \n",
    "        task_counter = len(self.context.execution_history) + 1\n",
    "        \n",
    "        while not self._is_project_complete() and task_counter <= self.max_tasks:\n",
    "            self.console.print(f\"\\n{'='*60}\")\n",
    "            self.console.print(f\"üìã GENERATING TASK {task_counter}\")\n",
    "            self.console.print(f\"{'='*60}\")\n",
    "            \n",
    "            # Generate next task using breakdown context\n",
    "            task = self.task_generator.generate_next_task(self.context, self.project_breakdown, self.estimated_total_tasks)\n",
    "            if not task:\n",
    "                self.console.print(\"‚ùå No more tasks to generate. Project complete.\")\n",
    "                break\n",
    "            \n",
    "            self.console.print(f\"‚úÖ Generated: {task.name}\")\n",
    "            self.console.print(f\"üîç Task ID: {task.id}, Dependencies: {task.dependencies}\")\n",
    "            \n",
    "            # PRE-EXECUTION INTROSPECTION\n",
    "            pre_validation = self.validator.introspect_task_planning(task, self.project_breakdown, self.context)\n",
    "            if not pre_validation.success:\n",
    "                self.console.print(f\"‚ö†Ô∏è Pre-execution validation failed: {pre_validation.feedback}\")\n",
    "                if pre_validation.next_action == \"regenerate\":\n",
    "                    self.console.print(\"üîÑ Regenerating task with feedback...\")\n",
    "                    regenerated_task = self.task_generator.regenerate_task_with_feedback(self.context, self.project_breakdown, pre_validation.feedback)\n",
    "                    if regenerated_task:\n",
    "                        task = regenerated_task\n",
    "                        self.console.print(\"‚úÖ Task regenerated successfully\")\n",
    "                    else:\n",
    "                        self.console.print(\"‚ö†Ô∏è Task regeneration failed. Proceeding with original task.\")\n",
    "            \n",
    "            # Execute task\n",
    "            task_result = await self.task_executor.execute_task(task)\n",
    "            \n",
    "            # Update context\n",
    "            self.context.execution_history.append(task_result)\n",
    "            \n",
    "            if not isinstance(self.context.current_artifacts, list):\n",
    "                self.context.current_artifacts = []\n",
    "                \n",
    "            artifacts = task_result.artifacts_created\n",
    "            if isinstance(artifacts, list):\n",
    "                self.context.current_artifacts.extend(artifacts)\n",
    "            elif artifacts:\n",
    "                self.context.current_artifacts.extend([str(artifacts)])\n",
    "            \n",
    "            self.context.total_tasks_completed += 1\n",
    "            \n",
    "            # Save state after each task\n",
    "            self.cache_manager.save_to_cache(self.context, self.project_breakdown)\n",
    "            \n",
    "            # Update project status\n",
    "            if task_result.status == ExecutionStatus.FAILED:\n",
    "                self.console.print(f\"‚ö†Ô∏è Task {task.id} failed. Continuing with next task.\")\n",
    "                # Mark task as failed but don't stop execution\n",
    "                failed_tasks = getattr(self.context, 'failed_tasks', [])\n",
    "                failed_tasks.append(task.id)\n",
    "                self.context.failed_tasks = failed_tasks\n",
    "            \n",
    "            task_counter += 1\n",
    "        \n",
    "        # Final project status\n",
    "        failed_tasks = getattr(self.context, 'failed_tasks', [])\n",
    "        if failed_tasks:\n",
    "            self.context.project_status = ExecutionStatus.PARTIAL_SUCCESS\n",
    "            self.console.print(f\"‚ö†Ô∏è Project completed with {len(failed_tasks)} failed tasks: {failed_tasks}\")\n",
    "        else:\n",
    "            self.context.project_status = ExecutionStatus.SUCCESS\n",
    "            self.console.print(\"‚úÖ All tasks completed successfully\")\n",
    "        \n",
    "        # Clear cache on completion\n",
    "        self.cache_manager.clear_cache()\n",
    "        \n",
    "        self.console.print(f\"\\nüéâ Project execution completed! Generated {len(self.context.execution_history)} tasks.\")\n",
    "        return self.context\n",
    "    \n",
    "    def _is_project_complete(self) -> bool:\n",
    "        \"\"\"Determine if project is complete based on progress\"\"\"\n",
    "        if not self.context.execution_history:\n",
    "            return False\n",
    "        \n",
    "        if self.context.total_tasks_completed >= max(self.estimated_total_tasks, 5):\n",
    "            self.console.print(f\"‚úÖ Project completion reached: {self.context.total_tasks_completed}/{self.estimated_total_tasks} tasks\")\n",
    "            return True\n",
    "        \n",
    "        completion_percentage = (self.context.total_tasks_completed / max(self.estimated_total_tasks, 1)) * 100\n",
    "        self.console.print(f\"üìä Project progress: {self.context.total_tasks_completed}/{self.estimated_total_tasks} tasks ({completion_percentage:.1f}%)\")\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def save_execution_report(self, filename: str = None) -> str:\n",
    "        \"\"\"Save detailed execution report\"\"\"\n",
    "        if not filename:\n",
    "            filename = \"execution_report.json\"\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.context.model_dump(), f, indent=2, default=str)\n",
    "        \n",
    "        self.console.print(f\"üìä Execution report saved to: {filename}\")\n",
    "        return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be0164d-9727-48bb-92e0-b3aa09a2fce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
