{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf998cdc-0e43-4f6b-a49e-356c2feb7227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp core.agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ceb513-a7e0-4a12-b3db-0968785c69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict, Any, Optional, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "from agentic.llms.client import LLMClient\n",
    "from agentic.configs.prompt_manager import get_system_prompt\n",
    "from agentic.configs.loader import get_model_config, get_settings_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599b7390-c9c5-4b67-9d41-fa917fa2c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    role: str\n",
    "    content: str\n",
    "    tool_calls: Optional[List[Dict]] = None\n",
    "    tool_call_id: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentConfig:\n",
    "    name: str\n",
    "    instructions: str = \"\"\n",
    "    model: Optional[str] = None\n",
    "    tools: List[str] = field(default_factory=list)\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: Optional[int] = None\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Core Agent class with tool execution and conversation management\"\"\"\n",
    "    \n",
    "    def __init__(self, config: AgentConfig, llm_client: Optional[LLMClient] = None):\n",
    "        self.config = config\n",
    "        self.llm_client = llm_client or self._create_default_llm_client()\n",
    "        self.conversation_history: List[Message] = []\n",
    "        self.tools_registry: Dict[str, Callable] = {}\n",
    "        self.guardrails: List[Callable] = []\n",
    "        \n",
    "        # Initialize with system message\n",
    "        system_prompt = config.instructions or get_system_prompt()\n",
    "        self.conversation_history.append(Message(role=\"system\", content=system_prompt))\n",
    "    \n",
    "    def _create_default_llm_client(self) -> LLMClient:\n",
    "        \"\"\"Create default LLM client from config\"\"\"\n",
    "        model_config = get_model_config()\n",
    "        return LLMClient(\n",
    "            model=self.config.model or model_config.get('name'),\n",
    "            base_url=model_config.get('url'),\n",
    "            api_key=model_config.get('api_key')\n",
    "        )\n",
    "    \n",
    "    def add_tool(self, name: str, tool_func: Callable):\n",
    "        \"\"\"Register a tool with the agent\"\"\"\n",
    "        self.tools_registry[name] = tool_func\n",
    "    \n",
    "    def add_guardrail(self, guardrail_func: Callable):\n",
    "        \"\"\"Add a guardrail function\"\"\"\n",
    "        self.guardrails.append(guardrail_func)\n",
    "    \n",
    "    def run(self, message: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Execute agent with message and return response\"\"\"\n",
    "        # Apply guardrails\n",
    "        for guardrail in self.guardrails:\n",
    "            if not guardrail(message):\n",
    "                return {\"content\": \"Request blocked by guardrails\", \"blocked\": True}\n",
    "        \n",
    "        # Add user message\n",
    "        self.conversation_history.append(Message(role=\"user\", content=message))\n",
    "        \n",
    "        # Get available tools\n",
    "        available_tools = self._get_available_tools()\n",
    "        \n",
    "        # Create completion\n",
    "        messages = self._format_messages_for_llm()\n",
    "        stream = kwargs.get('stream', True)\n",
    "        response = self.llm_client.create_completion(\n",
    "            messages=messages,\n",
    "            tools=available_tools,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Process response based on streaming mode\n",
    "        if stream:\n",
    "            result = self.llm_client.handle_streaming_response(response)\n",
    "        else:\n",
    "            result = self.llm_client.process_response(response)\n",
    "        \n",
    "        # Execute tool calls if any\n",
    "        if result.get(\"tool_calls\"):\n",
    "            executed_calls = self._execute_tool_calls(result[\"tool_calls\"])\n",
    "            result[\"tool_calls\"] = executed_calls\n",
    "        \n",
    "        # Add to conversation history\n",
    "        assistant_message = Message(\n",
    "            role=\"assistant\", \n",
    "            content=result.get(\"content\", \"\"),\n",
    "            tool_calls=result.get(\"tool_calls\")\n",
    "        )\n",
    "        self.conversation_history.append(assistant_message)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _get_available_tools(self) -> List[Dict]:\n",
    "        \"\"\"Get OpenAI-formatted tools for the configured tool names\"\"\"\n",
    "        from ..tools.core import ToolManager\n",
    "        tool_manager = ToolManager()\n",
    "        return tool_manager.get_tools(self.config.tools)\n",
    "    \n",
    "    def _execute_tool_calls(self, tool_calls: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Execute tool calls using registered tools\"\"\"\n",
    "        from ..tools.display import ToolExecutionDisplay\n",
    "        display = ToolExecutionDisplay()\n",
    "        \n",
    "        executed_calls = []\n",
    "        \n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call[\"function\"][\"name\"]\n",
    "            arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "            \n",
    "            # Show tool execution start\n",
    "            display.show_tool_start(function_name, trusted=True)\n",
    "            \n",
    "            if function_name in self.tools_registry:\n",
    "                try:\n",
    "                    result = self.tools_registry[function_name](**arguments)\n",
    "                    tool_call[\"result\"] = result\n",
    "                    executed_calls.append(tool_call)\n",
    "                except Exception as e:\n",
    "                    tool_call[\"error\"] = str(e)\n",
    "                    display.show_tool_error(f\"Error in {function_name}\", str(e))\n",
    "                    executed_calls.append(tool_call)\n",
    "            else:\n",
    "                from ..tools.core import ToolManager\n",
    "                tool_manager = ToolManager()\n",
    "                try:\n",
    "                    result = tool_manager.execute_tool(function_name, arguments)\n",
    "                    tool_call[\"result\"] = result\n",
    "                    executed_calls.append(tool_call)\n",
    "                except Exception as e:\n",
    "                    tool_call[\"error\"] = str(e)\n",
    "                    display.show_tool_error(f\"Error in {function_name}\", str(e))\n",
    "                    executed_calls.append(tool_call)\n",
    "        \n",
    "        return executed_calls\n",
    "    \n",
    "    def _format_messages_for_llm(self) -> List[Dict]:\n",
    "        \"\"\"Convert Message objects to LLM format\"\"\"\n",
    "        messages = []\n",
    "        for msg in self.conversation_history:\n",
    "            message_dict = {\"role\": msg.role, \"content\": msg.content}\n",
    "            if msg.tool_calls:\n",
    "                message_dict[\"tool_calls\"] = msg.tool_calls\n",
    "            if msg.tool_call_id:\n",
    "                message_dict[\"tool_call_id\"] = msg.tool_call_id\n",
    "            messages.append(message_dict)\n",
    "        return messages\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history except system message\"\"\"\n",
    "        system_msg = self.conversation_history[0]\n",
    "        self.conversation_history = [system_msg]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bc8ad7-1a02-4194-b271-c72e442b4dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
