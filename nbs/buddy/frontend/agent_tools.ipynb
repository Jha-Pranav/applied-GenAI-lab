{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7b4c9-8374-4fb5-8e79-fa6bb265599e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Any\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfigs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_model_config, get_reasoning_config\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseTool, ToolMetadata, ToolCategory\n\u001b[32m      7\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAgentToolConfig\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from typing import Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from agentic.configs.loader import get_model_config, get_reasoning_config\n",
    "from agentic.tools.base import BaseTool, ToolMetadata, ToolCategory\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentToolConfig:\n",
    "    \"\"\"Configuration for agent-as-tool\"\"\"\n",
    "    agent_name: str\n",
    "    description: str\n",
    "    parameters_schema: Dict[str, Any]\n",
    "\n",
    "\n",
    "class SimpleAgentTool(BaseTool):\n",
    "    \"\"\"Simple wrapper to use agents as tools with BaseTool compatibility\"\"\"\n",
    "    \n",
    "    def __init__(self, config: AgentToolConfig, llm_client):\n",
    "        # Create metadata for BaseTool\n",
    "        metadata = ToolMetadata(\n",
    "            name=config.agent_name,\n",
    "            description=config.description,\n",
    "            category=ToolCategory.INTELLIGENCE,\n",
    "            requires_approval=False,\n",
    "            is_dangerous=False\n",
    "        )\n",
    "        super().__init__(metadata)\n",
    "        \n",
    "        self.config = config\n",
    "        self.llm_client = llm_client\n",
    "        \n",
    "        # Load config for agent behavior\n",
    "        self.model_config = get_model_config()\n",
    "        self.reasoning_config = get_reasoning_config()\n",
    "    \n",
    "    def get_parameters_schema(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get parameters schema for this agent tool\"\"\"\n",
    "        return self.config.parameters_schema\n",
    "    \n",
    "    def execute(self, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Execute the agent with provided parameters\"\"\"\n",
    "        try:\n",
    "            if self.config.agent_name == \"debate_agent\":\n",
    "                return self._execute_debate(**kwargs)\n",
    "            elif self.config.agent_name == \"task_planner\":\n",
    "                return self._execute_planner(**kwargs)\n",
    "            else:\n",
    "                return {\"error\": f\"Unknown agent: {self.config.agent_name}\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Agent execution failed: {str(e)}\"}\n",
    "    \n",
    "    def _execute_debate(self, topic: str, context: str, perspectives: list = None) -> Dict[str, Any]:\n",
    "        \"\"\"Execute debate analysis with config-driven behavior\"\"\"\n",
    "        if not perspectives:\n",
    "            perspectives = [\"pros\", \"cons\", \"technical\", \"practical\"]\n",
    "        \n",
    "        # Use reasoning config for confidence and retry behavior\n",
    "        max_reasoning_steps = self.reasoning_config.get('max_reasoning_steps', 10)\n",
    "        \n",
    "        try:\n",
    "            # This is a simplified implementation\n",
    "            # In production, you'd use the actual debate agent with LLM\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"topic\": topic,\n",
    "                \"analysis\": {\n",
    "                    \"perspectives\": [\n",
    "                        {\n",
    "                            \"name\": p,\n",
    "                            \"arguments\": [f\"Argument for {p} perspective on {topic}\"],\n",
    "                            \"evidence\": [f\"Evidence supporting {p} view\"],\n",
    "                            \"assessment\": \"neutral\"\n",
    "                        } for p in perspectives\n",
    "                    ],\n",
    "                    \"recommendation\": f\"Based on analysis of '{topic}', consider multiple factors before proceeding. Context: {context}\",\n",
    "                    \"confidence\": 0.7\n",
    "                },\n",
    "                \"message\": \"Debate analysis completed (simplified implementation)\",\n",
    "                \"model_used\": self.model_config.get('name', 'unknown')\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Debate execution failed: {str(e)}\"}\n",
    "    \n",
    "    def _execute_planner(self, request: str, complexity: str = \"moderate\") -> Dict[str, Any]:\n",
    "        \"\"\"Execute task planning with config-driven behavior\"\"\"\n",
    "        \n",
    "        # Use reasoning config for planning depth\n",
    "        max_reasoning_steps = self.reasoning_config.get('max_reasoning_steps', 10)\n",
    "        retry_count = self.reasoning_config.get('retry_count', 2)\n",
    "        \n",
    "        try:\n",
    "            # This is a simplified implementation\n",
    "            # In production, you'd use the actual planner agent with LLM\n",
    "            \n",
    "            # Simple task breakdown based on keywords\n",
    "            tasks = []\n",
    "            task_count = 0\n",
    "            \n",
    "            if \"file\" in request.lower() or \"read\" in request.lower():\n",
    "                task_count += 1\n",
    "                tasks.append({\n",
    "                    \"id\": f\"task_{task_count}\",\n",
    "                    \"name\": \"File Operations\",\n",
    "                    \"description\": \"Handle file reading/writing operations\",\n",
    "                    \"tools\": [\"fs_read\", \"fs_write\"],\n",
    "                    \"dependencies\": [],\n",
    "                    \"success_criteria\": \"Files processed successfully\"\n",
    "                })\n",
    "            \n",
    "            if \"run\" in request.lower() or \"execute\" in request.lower() or \"command\" in request.lower():\n",
    "                task_count += 1\n",
    "                tasks.append({\n",
    "                    \"id\": f\"task_{task_count}\", \n",
    "                    \"name\": \"Command Execution\",\n",
    "                    \"description\": \"Execute system commands\",\n",
    "                    \"tools\": [\"execute_bash\"],\n",
    "                    \"dependencies\": [f\"task_{task_count-1}\"] if task_count > 1 else [],\n",
    "                    \"success_criteria\": \"Commands executed without errors\"\n",
    "                })\n",
    "            \n",
    "            if \"create\" in request.lower() or \"build\" in request.lower():\n",
    "                task_count += 1\n",
    "                tasks.append({\n",
    "                    \"id\": f\"task_{task_count}\",\n",
    "                    \"name\": \"Creation Task\",\n",
    "                    \"description\": f\"Create or build components for: {request}\",\n",
    "                    \"tools\": [\"fs_write\", \"execute_bash\"],\n",
    "                    \"dependencies\": [f\"task_{i}\" for i in range(1, task_count)],\n",
    "                    \"success_criteria\": \"Components created successfully\"\n",
    "                })\n",
    "            \n",
    "            if not tasks:\n",
    "                tasks.append({\n",
    "                    \"id\": \"task_1\",\n",
    "                    \"name\": \"General Task\",\n",
    "                    \"description\": f\"Handle request: {request}\",\n",
    "                    \"tools\": [\"fs_read\", \"fs_write\"],\n",
    "                    \"dependencies\": [],\n",
    "                    \"success_criteria\": \"Request completed successfully\"\n",
    "                })\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"request\": request,\n",
    "                \"complexity\": complexity,\n",
    "                \"tasks\": tasks,\n",
    "                \"execution_order\": [task[\"id\"] for task in tasks],\n",
    "                \"estimated_time\": f\"{len(tasks) * 2}-{len(tasks) * 5} minutes\",\n",
    "                \"max_retries\": retry_count,\n",
    "                \"message\": \"Task plan created (simplified implementation)\",\n",
    "                \"model_used\": self.model_config.get('name', 'unknown')\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Planning execution failed: {str(e)}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d9fae-d9a1-4585-94d1-fbbc06eaa7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
