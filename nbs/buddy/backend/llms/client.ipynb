{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ec34ab-ef7b-4f99-9d09-00934f31feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp backend.llms.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d47375d7-1f2c-4efe-b2c6-9b55db3f61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "from typing import Dict, Any, List, Optional, Iterator\n",
    "from openai import OpenAI\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63494c70-07c9-47c6-8874-ce027082137f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to load config from /home/pranav-pc/projects/applied-GenAI-lab/agentic/configs/config.toml: SettingsConfig.__init__() got an unexpected keyword argument 'temperature'\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "from agentic.configs.loader import get_model_config, get_settings_config\n",
    "from agentic.llms.response_processor import ResponseProcessor\n",
    "from agentic.llms.streaming_handler import StreamingHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caac836d-f04d-428e-8f4f-f47be660f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class LLMClient:\n",
    "    \"\"\"Enhanced LLM client\"\"\"\n",
    "    \n",
    "    def __init__(self, model: Optional[str] = None, base_url: Optional[str] = None, \n",
    "                 api_key: Optional[str] = None):\n",
    "        # Load config defaults\n",
    "        model_config = get_model_config()\n",
    "        settings_config = get_settings_config()\n",
    "        \n",
    "        # Set parameters with fallbacks\n",
    "        self.model = model or model_config.get('name', 'qwen3:8b')\n",
    "        self.base_url = base_url or model_config.get('url', 'http://localhost:11434/v1')\n",
    "        self.api_key = api_key or model_config.get('api_key', 'ollama')\n",
    "\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI(base_url=self.base_url, \n",
    "                             api_key=self.api_key)\n",
    "        \n",
    "        # Initialize processors\n",
    "        self.response_processor = ResponseProcessor()\n",
    "        self.streaming_handler = StreamingHandler()\n",
    "        \n",
    "        # Validate connection\n",
    "        self._validate_connection()\n",
    "    \n",
    "    def _validate_connection(self):\n",
    "        \"\"\"Validate LLM connection\"\"\"\n",
    "        try:\n",
    "            # Test with a simple request\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "                max_tokens=1,\n",
    "                timeout=5\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ LLM connection warning: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def create_completion(self, messages: List[Dict[str, Any]], \n",
    "                         tools: Optional[List[Dict]] = None,\n",
    "                         stream: bool = True, **kwargs) -> Any:\n",
    "        \"\"\"Create chat completion with optional tools\"\"\"\n",
    "        completion_params = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": stream,\n",
    "            **kwargs\n",
    "        }\n",
    "        \n",
    "        if tools:\n",
    "            completion_params[\"tools\"] = tools\n",
    "            completion_params[\"tool_choice\"] = \"auto\"\n",
    "        \n",
    "        try:\n",
    "            return self.client.chat.completions.create(**completion_params)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"LLM completion failed: {e}\")\n",
    "    \n",
    "    def process_response(self, response: Any, console: Optional[Console] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Process non-streaming response\"\"\"\n",
    "        return self.response_processor.process_response(response, console)\n",
    "    \n",
    "    def handle_streaming_response(self, response: Iterator, console: Optional[Console] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Handle streaming response\"\"\"\n",
    "        return self.streaming_handler.handle_streaming_response(response, console)\n",
    "    \n",
    "    def get_model_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get information about the current model\"\"\"\n",
    "        return {\n",
    "            \"model\": self.model,\n",
    "            \"base_url\": self.base_url,\n",
    "            \"api_key_set\": bool(self.api_key),\n",
    "            \"connection_valid\": self._validate_connection()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7112dc4-a77b-4f5c-9516-e3328eb5f165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;2;200;100;120m╭─────────────────────── 🤔 Thinking ───────────────────────╮\u001b[0m\n",
      "\u001b[38;2;200;100;120mOkay, I need to explain how a neural network works. Let me start by recalling what I know. Neural networks are inspired by the human brain, right? They consist of layers of nodes, like an input layer, hidden layers, and an output layer. Each node is connected to the next, and these connections have weights.\n",
      "\n",
      "First, I should break down the components. The input layer receives the data. Then, each hidden layer processes the data through weights and biases, applying an activation function. The output layer gives the final result. But how do they actually process the data?\n",
      "\n",
      "Maybe I should explain the forward propagation step. The input data is multiplied by the weights, summed with biases, and passed through an activation function. This process happens layer by layer. The activation function introduces non-linearity, which is crucial because without it, the network can't learn complex patterns.\n",
      "\n",
      "Wait, but what are the activation functions? Common ones include ReLU, sigmoid, and tanh. Each has different use cases. I should mention them briefly. Also, the loss function measures how wrong the output is, and the optimization algorithm like gradient descent adjusts the weights to minimize this loss.\n",
      "\n",
      "Don't forget the training process. It involves forward propagation, calculating loss, backpropagation to compute gradients, and updating weights. Maybe include a simple example, like a perceptron for binary classification, to make it more tangible.\n",
      "\n",
      "Oh, and the role of the loss function is important. Different problems use different losses, like MSE for regression and cross-entropy for classification. Also, hyperparameters like learning rate and number of layers affect performance.\n",
      "\n",
      "Wait, should I mention overfitting and regularization techniques? Maybe not in the basic explanation, but perhaps in a more advanced section. The user asked for an explanation, so maybe keep it focused on the core mechanism without diving too deep.\n",
      "\n",
      "I should structure the answer with headers: Introduction, Components, Forward Propagation, Training Process, Activation Functions, and maybe a simple example. Use bullet points for lists and code blocks to show equations. Make sure the explanation is clear and step-by-step, avoiding jargon where possible.\u001b[0m\n",
      "\u001b[38;2;200;100;120m╰────────────────────────────────────────────────────────────╯\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                                           <span style=\"font-weight: bold\">How a Neural Network Works</span>                                            ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "\n",
       "A neural network is a computational model inspired by the human brain, designed to recognize patterns and make     \n",
       "predictions. Here's a breakdown of its core mechanics:                                                             \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                                <span style=\"font-weight: bold; text-decoration: underline\">1. Core Components</span>                                                 \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Input Layer</span>: Receives raw data (e.g., pixel values for images).                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Hidden Layers</span>: Intermediate layers that process data through weighted connections.                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Output Layer</span>: Produces the final result (e.g., class labels or numerical predictions).                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Weights &amp; Biases</span>: Parameters that determine the strength of connections between nodes.                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Activation Functions</span>: Introduce non-linearity to enable complex pattern recognition.                            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                              <span style=\"font-weight: bold; text-decoration: underline\">2. Forward Propagation</span>                                               \n",
       "\n",
       "Data flows through the network in stages:                                                                          \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">Input → Hidden Layer</span>:                                                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Each node computes:                                                                                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>$$ z = (W \\cdot x) + b $$                                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>where $ W $ = weights, $ x $ = input, $ b $ = bias.                                                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Apply an activation function:                                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>$$ a = f(z) $$                                                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>(e.g., ReLU, sigmoid, or tanh).                                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">Hidden → Output Layer</span>:                                                                                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Repeat the same process to compute the final output.                                                         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                                <span style=\"font-weight: bold; text-decoration: underline\">3. Training Process</span>                                                \n",
       "\n",
       "The network learns by minimizing errors through <span style=\"font-weight: bold\">backpropagation</span>:                                                   \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">Loss Function</span>:                                                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Measures prediction error (e.g., MSE for regression, cross-entropy for classification).                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">Gradient Descent</span>:                                                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Calculates gradients of the loss with respect to weights using the chain rule.                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Updates weights to reduce loss:                                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>$$ W_{\\text{new}} = W_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial W} $$                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>where $ \\eta $ = learning rate.                                                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                                  <span style=\"font-weight: bold; text-decoration: underline\">4. Key Concepts</span>                                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Non-linearity</span>: Activation functions allow the network to model complex relationships.                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Overfitting</span>: The network memorizes training data. Techniques like dropout or regularization mitigate this.      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Deep Learning</span>: Networks with many hidden layers can capture hierarchical patterns (e.g., edges → shapes →       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>objects).                                                                                                       \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                           <span style=\"font-weight: bold; text-decoration: underline\">5. Simple Example: Perceptron</span>                                           \n",
       "\n",
       "For binary classification:                                                                                         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Input: $ x = [x_1, x_2] $                                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Weights: $ W = [w_1, w_2] $                                                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Output:                                                                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>$$ y = \\text{sigmoid}(w_1 x_1 + w_2 x_2 + b) $$                                                                 \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                                 <span style=\"font-weight: bold; text-decoration: underline\">6. Visualization</span>                                                  \n",
       "\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Input Layer → Hidden Layer (with ReLU) → Output Layer</span><span style=\"background-color: #272822\">                                                             </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">           ↑                        ↑</span><span style=\"background-color: #272822\">                                                                             </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">           |                        |</span><span style=\"background-color: #272822\">                                                                             </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        Weights (W1)           Weights (W2)</span><span style=\"background-color: #272822\">                                                                       </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">           ↓                        ↓</span><span style=\"background-color: #272822\">                                                                             </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">       Biases (b1)              Biases (b2)</span><span style=\"background-color: #272822\">                                                                       </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "\n",
       "By iteratively adjusting weights via backpropagation, the network improves its predictions over time.              \n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                                           \u001b[1mHow a Neural Network Works\u001b[0m                                            ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "\n",
       "A neural network is a computational model inspired by the human brain, designed to recognize patterns and make     \n",
       "predictions. Here's a breakdown of its core mechanics:                                                             \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                                \u001b[1;4m1. Core Components\u001b[0m                                                 \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mInput Layer\u001b[0m: Receives raw data (e.g., pixel values for images).                                                 \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mHidden Layers\u001b[0m: Intermediate layers that process data through weighted connections.                              \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mOutput Layer\u001b[0m: Produces the final result (e.g., class labels or numerical predictions).                          \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mWeights & Biases\u001b[0m: Parameters that determine the strength of connections between nodes.                          \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mActivation Functions\u001b[0m: Introduce non-linearity to enable complex pattern recognition.                            \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                              \u001b[1;4m2. Forward Propagation\u001b[0m                                               \n",
       "\n",
       "Data flows through the network in stages:                                                                          \n",
       "\n",
       "\u001b[1;33m 1 \u001b[0m\u001b[1mInput → Hidden Layer\u001b[0m:                                                                                           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mEach node computes:                                                                                          \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m$$ z = (W \\cdot x) + b $$                                                                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mwhere $ W $ = weights, $ x $ = input, $ b $ = bias.                                                          \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mApply an activation function:                                                                                \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m$$ a = f(z) $$                                                                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m(e.g., ReLU, sigmoid, or tanh).                                                                              \n",
       "\u001b[1;33m 2 \u001b[0m\u001b[1mHidden → Output Layer\u001b[0m:                                                                                          \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mRepeat the same process to compute the final output.                                                         \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                                \u001b[1;4m3. Training Process\u001b[0m                                                \n",
       "\n",
       "The network learns by minimizing errors through \u001b[1mbackpropagation\u001b[0m:                                                   \n",
       "\n",
       "\u001b[1;33m 1 \u001b[0m\u001b[1mLoss Function\u001b[0m:                                                                                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mMeasures prediction error (e.g., MSE for regression, cross-entropy for classification).                      \n",
       "\u001b[1;33m 2 \u001b[0m\u001b[1mGradient Descent\u001b[0m:                                                                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mCalculates gradients of the loss with respect to weights using the chain rule.                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mUpdates weights to reduce loss:                                                                              \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m$$ W_{\\text{new}} = W_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial W} $$                             \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mwhere $ \\eta $ = learning rate.                                                                              \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                                  \u001b[1;4m4. Key Concepts\u001b[0m                                                  \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mNon-linearity\u001b[0m: Activation functions allow the network to model complex relationships.                           \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mOverfitting\u001b[0m: The network memorizes training data. Techniques like dropout or regularization mitigate this.      \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mDeep Learning\u001b[0m: Networks with many hidden layers can capture hierarchical patterns (e.g., edges → shapes →       \n",
       "\u001b[1;33m   \u001b[0mobjects).                                                                                                       \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                           \u001b[1;4m5. Simple Example: Perceptron\u001b[0m                                           \n",
       "\n",
       "For binary classification:                                                                                         \n",
       "\n",
       "\u001b[1;33m • \u001b[0mInput: $ x = [x_1, x_2] $                                                                                       \n",
       "\u001b[1;33m • \u001b[0mWeights: $ W = [w_1, w_2] $                                                                                     \n",
       "\u001b[1;33m • \u001b[0mOutput:                                                                                                         \n",
       "\u001b[1;33m   \u001b[0m$$ y = \\text{sigmoid}(w_1 x_1 + w_2 x_2 + b) $$                                                                 \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                                 \u001b[1;4m6. Visualization\u001b[0m                                                  \n",
       "\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mInput Layer → Hidden Layer (with ReLU) → Output Layer\u001b[0m\u001b[48;2;39;40;34m                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m           ↑                        ↑\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m           |                        |\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m        Weights (W1)           Weights (W2)\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m           ↓                        ↓\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m       Biases (b1)              Biases (b2)\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\n",
       "By iteratively adjusting weights via backpropagation, the network improves its predictions over time.              \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | hide\n",
    "# Example usage \n",
    "\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\n",
    "        \"You are a helpful assistant. \"\n",
    "        \"Return all your responses using valid **Markdown syntax**, including:\\n\"\n",
    "        \"- Headers (`#`, `##`)\\n\"\n",
    "        \"- Bullet points\\n\"\n",
    "        \"- Code blocks (triple backticks)\\n\"\n",
    "        \"- Bold / italic text\\n\"\n",
    "        \"- Quotes and tables if needed\\n\\n\"\n",
    "        \"Also, try to use formatting that works well with color rendering in terminals using `rich`.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "from rich.console import Console\n",
    "client = LLMClient()\n",
    "response = client.create_completion(\n",
    "    messages=[\n",
    "    system_prompt,\n",
    "    {\"role\": \"user\", \"content\": \"Explain how a neural network works.\"}\n",
    "    ],\n",
    "    tools=[],\n",
    "    stream=False\n",
    ")\n",
    "# result = client.handle_streaming_response(response,Console() )\n",
    "result = client.process_response(response,Console())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48bc44e-e8d9-4c2b-92f0-ee23043e480b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
