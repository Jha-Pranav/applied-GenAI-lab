{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a855603-179f-4778-89a2-81554cdc4f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d5ffb4-d88e-45d6-9a44-1298ce4783da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'agentic.analyzer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrich\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpanel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Panel\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrich\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprogress\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Progress, SpinnerColumn, TextColumn\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToolManager\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMClient\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magentic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfigs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_model_config, get_settings_config, get_tools_config, get_reasoning_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/applied-GenAI-lab/agentic/tools/manager.py:25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocGeneratorTool\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdebate_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DebateAgentTool\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtask_planner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TaskPlannerTool\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# %% ../../nbs/buddy/backend/tools/core/manager.ipynb 2\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mToolManager\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/applied-GenAI-lab/agentic/tools/task_planner.py:15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Any\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseTool\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manalyzer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgentTaskAnalyzer\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTaskPlannerTool\u001b[39;00m(BaseTool):\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'agentic.analyzer'"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "import json\n",
    "import re\n",
    "import asyncio\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "from rich.theme import Theme\n",
    "from rich.panel import Panel\n",
    "from rich.progress import Progress, SpinnerColumn, TextColumn\n",
    "\n",
    "from agentic.tools.manager import ToolManager\n",
    "from agentic.llms.client import LLMClient\n",
    "from agentic.configs.loader import get_model_config, get_settings_config, get_tools_config, get_reasoning_config\n",
    "from agentic.configs.prompts import get_system_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ff6e41-296e-44d4-a2fd-643453c3fe6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ToolManager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m     requires_debate: \u001b[38;5;28mbool\u001b[39m\n\u001b[32m     15\u001b[39m     suggested_tools: List[\u001b[38;5;28mstr\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mBuddyClient\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;250;43m    \u001b[39;49m\u001b[33;43;03m\"\"\"Production-grade AI assistant with intelligent request routing\"\"\"\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Load configurations from config.toml\u001b[39;49;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mBuddyClient\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mself\u001b[39m.show_thinking = \u001b[38;5;28mself\u001b[39m.reasoning_config.get(\u001b[33m'\u001b[39m\u001b[33mshow_thinking\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     66\u001b[39m     \u001b[38;5;28mself\u001b[39m.retry_count = \u001b[38;5;28mself\u001b[39m.reasoning_config.get(\u001b[33m'\u001b[39m\u001b[33mretry_count\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_initialize_tools\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[43mToolManager\u001b[49m:\n\u001b[32m     69\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Initialize core tools using existing ToolManager\"\"\"\u001b[39;00m\n\u001b[32m     70\u001b[39m     tool_manager = ToolManager()\n",
      "\u001b[31mNameError\u001b[39m: name 'ToolManager' is not defined"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "class RequestComplexity(Enum):\n",
    "    SIMPLE = \"simple\"\n",
    "    MODERATE = \"moderate\"\n",
    "    COMPLEX = \"complex\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AnalysisResult:\n",
    "    complexity: RequestComplexity\n",
    "    confidence: float\n",
    "    reasoning: str\n",
    "    requires_planning: bool\n",
    "    requires_debate: bool\n",
    "    suggested_tools: List[str]\n",
    "\n",
    "\n",
    "class BuddyClient:\n",
    "    \"\"\"Production-grade AI assistant with intelligent request routing\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = None, base_url: str = None, api_key: str = None):\n",
    "        # Load configurations from config.toml\n",
    "        self.model_config = get_model_config()\n",
    "        self.settings_config = get_settings_config()\n",
    "        self.tools_config = get_tools_config()\n",
    "        self.reasoning_config = get_reasoning_config()\n",
    "        \n",
    "        # Initialize LLM client with config values\n",
    "        self.llm_client = LLMClient(\n",
    "            model or self.model_config.get('name'),\n",
    "            base_url or self.model_config.get('url'),\n",
    "            api_key or self.model_config.get('api_key')\n",
    "        )\n",
    "        \n",
    "        # Initialize console for rich output\n",
    "        self.console = Console(theme=Theme({\n",
    "            \"info\": \"cyan\",\n",
    "            \"warning\": \"yellow\",\n",
    "            \"error\": \"red\",\n",
    "            \"success\": \"green\",\n",
    "            \"highlight\": \"magenta\"\n",
    "        }))\n",
    "        \n",
    "        # Initialize tool manager with core tools\n",
    "        self.tool_manager = self._initialize_tools()\n",
    "        \n",
    "        # Import actual agent implementations for streaming\n",
    "        from agentic.agent.planner.main import main as planner_main\n",
    "        from agentic.agent.debater import create_debate\n",
    "        \n",
    "        # Store references for later use\n",
    "        self.planner_main = planner_main\n",
    "        self.create_debate = create_debate\n",
    "        \n",
    "        # Conversation history\n",
    "        self.conversation_history: List[Dict[str, str]] = []\n",
    "        \n",
    "        # Settings from config\n",
    "        self.auto_approve = self.settings_config.get('auto_approve', False)\n",
    "        self.stream = self.settings_config.get('stream', True)\n",
    "        self.debug = self.settings_config.get('debug', False)\n",
    "        self.max_history = self.settings_config.get('max_history', 100)\n",
    "        \n",
    "        # Reasoning settings\n",
    "        self.show_thinking = self.reasoning_config.get('show_thinking', True)\n",
    "        self.retry_count = self.reasoning_config.get('retry_count', 2)\n",
    "    \n",
    "    def _initialize_tools(self) -> ToolManager:\n",
    "        \"\"\"Initialize core tools using existing ToolManager\"\"\"\n",
    "        tool_manager = ToolManager()\n",
    "        return tool_manager\n",
    "    \n",
    "    def is_tool_dangerous(self, tool_name: str) -> bool:\n",
    "        \"\"\"Check if tool is marked as dangerous\"\"\"\n",
    "        tool_info = self.tool_manager.get_tool_info(tool_name)\n",
    "        return tool_info.get('is_dangerous', False) if tool_info else False\n",
    "    \n",
    "    def requires_approval(self, tool_name: str) -> bool:\n",
    "        \"\"\"Check if tool requires approval\"\"\"\n",
    "        tool_info = self.tool_manager.get_tool_info(tool_name)\n",
    "        return tool_info.get('requires_approval', False) if tool_info else False\n",
    "    \n",
    "    async def analyze_request_complexity(self, request: str) -> AnalysisResult:\n",
    "        \"\"\"Analyze request complexity and determine routing strategy\"\"\"\n",
    "        \n",
    "        analysis_prompt = f\"\"\"\n",
    "        Analyze this user request and determine its complexity level:\n",
    "        \n",
    "        Request: \"{request}\"\n",
    "        \n",
    "        Consider these factors:\n",
    "        1. Number of steps required (1-2: simple, 3-5: moderate, 6+: complex)\n",
    "        2. Technical complexity and domain expertise needed\n",
    "        3. Need for decision-making or trade-off analysis\n",
    "        4. File system operations, code generation, or system commands\n",
    "        5. Planning and coordination requirements\n",
    "        6. Debate and multi-perspective analysis needs\n",
    "        \n",
    "        IMPORTANT: If the request contains words like \"debate\", \"perspectives\", \"argue\", \"pros and cons\", \"should\", \"regulated\", \"analysis\", \"multiple viewpoints\", \"structured debate\", or asks for decision analysis, set requires_debate=true and requires_planning=false.\n",
    "        \n",
    "        Respond with JSON:\n",
    "        {{\n",
    "            \"complexity\": \"simple|moderate|complex\",\n",
    "            \"confidence\": 0.0-1.0,\n",
    "            \"reasoning\": \"Brief explanation\",\n",
    "            \"requires_planning\": true/false,\n",
    "            \"requires_debate\": true/false,\n",
    "            \"suggested_tools\": [\"tool1\", \"tool2\"]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm_client.create_completion(\n",
    "                messages=[{\"role\": \"user\", \"content\": analysis_prompt}],\n",
    "                stream=False,\n",
    "                temperature=0.1,\n",
    "                max_tokens=self.model_config.get('max_tokens'),\n",
    "                timeout=self.model_config.get('timeout', 60)\n",
    "            )\n",
    "            \n",
    "            # Extract JSON from response\n",
    "            content = response.choices[0].message.content\n",
    "            json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "            \n",
    "            if json_match:\n",
    "                analysis_data = json.loads(json_match.group())\n",
    "                return AnalysisResult(\n",
    "                    complexity=RequestComplexity(analysis_data[\"complexity\"]),\n",
    "                    confidence=analysis_data[\"confidence\"],\n",
    "                    reasoning=analysis_data[\"reasoning\"],\n",
    "                    requires_planning=analysis_data[\"requires_planning\"],\n",
    "                    requires_debate=analysis_data[\"requires_debate\"],\n",
    "                    suggested_tools=analysis_data[\"suggested_tools\"]\n",
    "                )\n",
    "            else:\n",
    "                # Fallback analysis\n",
    "                return self._fallback_analysis(request)\n",
    "                \n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                self.console.print(f\"[error]Analysis error: {e}[/error]\")\n",
    "            return self._fallback_analysis(request)\n",
    "    \n",
    "    def _fallback_analysis(self, request: str) -> AnalysisResult:\n",
    "        \"\"\"Fallback complexity analysis using heuristics\"\"\"\n",
    "        words = request.split()\n",
    "        \n",
    "        # Simple heuristics\n",
    "        complexity_indicators = {\n",
    "            'simple': ['list', 'show', 'read', 'display', 'get'],\n",
    "            'moderate': ['create', 'write', 'modify', 'update', 'generate'],\n",
    "            'complex': ['build', 'develop', 'implement', 'design', 'architecture', 'system']\n",
    "        }\n",
    "        \n",
    "        scores = {'simple': 0, 'moderate': 0, 'complex': 0}\n",
    "        \n",
    "        for word in words:\n",
    "            word_lower = word.lower()\n",
    "            for level, indicators in complexity_indicators.items():\n",
    "                if any(indicator in word_lower for indicator in indicators):\n",
    "                    scores[level] += 1\n",
    "        \n",
    "        # Determine complexity\n",
    "        max_score = max(scores.values())\n",
    "        if max_score == 0:\n",
    "            complexity = RequestComplexity.SIMPLE\n",
    "        else:\n",
    "            complexity = RequestComplexity([k for k, v in scores.items() if v == max_score][0])\n",
    "        \n",
    "        return AnalysisResult(\n",
    "            complexity=complexity,\n",
    "            confidence=0.6,\n",
    "            reasoning=\"Heuristic analysis based on keywords\",\n",
    "            requires_planning=complexity in [RequestComplexity.MODERATE, RequestComplexity.COMPLEX],\n",
    "            requires_debate='decision' in request.lower() or 'choose' in request.lower(),\n",
    "            suggested_tools=['fs_read', 'fs_write'] if any(word in request.lower() for word in ['file', 'code', 'script']) else []\n",
    "        )\n",
    "    \n",
    "    async def execute_simple_request(self, request: str, suggested_tools: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Execute simple requests directly with tools\"\"\"\n",
    "        \n",
    "        self.console.print(Panel(\n",
    "            f\"[bold green]Executing simple request with direct tool usage[/bold green]\",\n",
    "            title=\"[bold yellow]🚀 Simple Execution[/bold yellow]\",\n",
    "            border_style=\"yellow\",\n",
    "            padding=(0, 1)\n",
    "        ))\n",
    "        \n",
    "        # Prepare messages for LLM with tool calling\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": get_system_prompt()},\n",
    "            {\"role\": \"user\", \"content\": request}\n",
    "        ]\n",
    "        \n",
    "        # Get available tools\n",
    "        available_tools = self.tool_manager.get_tools()\n",
    "        \n",
    "        try:\n",
    "            # Call LLM with tools and config parameters\n",
    "            stream = self.settings_config.get('stream', True)\n",
    "            response = self.llm_client.create_completion(\n",
    "                messages=messages,\n",
    "                tools=available_tools,\n",
    "                stream=stream,\n",
    "                temperature=self.model_config.get('temperature', 0.7),\n",
    "                max_tokens=self.model_config.get('max_tokens'),\n",
    "                timeout=self.model_config.get('timeout', 60)\n",
    "            )\n",
    "            \n",
    "            if stream:\n",
    "                result = self.llm_client.handle_streaming_response(response, self.console)\n",
    "                # Create message-like object from streaming result\n",
    "                class StreamMessage:\n",
    "                    def __init__(self, result):\n",
    "                        self.content = result.get('content', '')\n",
    "                        raw_tool_calls = result.get('tool_calls', [])\n",
    "                        # Convert dict tool_calls to object-like structure\n",
    "                        self.tool_calls = []\n",
    "                        for tc in raw_tool_calls:\n",
    "                            if tc and tc.get('function'):\n",
    "                                class ToolCall:\n",
    "                                    def __init__(self, tc_dict):\n",
    "                                        self.id = tc_dict.get('id', '')\n",
    "                                        self.type = tc_dict.get('type', 'function')\n",
    "                                        class Function:\n",
    "                                            def __init__(self, func_dict):\n",
    "                                                self.name = func_dict.get('name', '')\n",
    "                                                self.arguments = func_dict.get('arguments', '{}')\n",
    "                                        self.function = Function(tc_dict['function'])\n",
    "                                self.tool_calls.append(ToolCall(tc))\n",
    "                message = StreamMessage(result)\n",
    "            else:\n",
    "                result = self.llm_client.process_response(response, self.console)\n",
    "                message = result.get('message')\n",
    "            \n",
    "            # Handle tool calls\n",
    "            if message.tool_calls:\n",
    "                return await self._handle_tool_calls(message.tool_calls, messages)\n",
    "            else:\n",
    "                # Direct response - already printed by streaming/process_response\n",
    "                return {\"success\": True, \"response\": message.content}\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Simple execution failed: {str(e)}\"\n",
    "            self.console.print(f\"[error]{error_msg}[/error]\")\n",
    "            return {\"success\": False, \"error\": error_msg}\n",
    "    \n",
    "    async def execute_complex_request(self, request: str, analysis: AnalysisResult) -> Dict[str, Any]:\n",
    "        \"\"\"Execute complex requests through planning and coordination\"\"\"\n",
    "        \n",
    "        self.console.print(Panel(\n",
    "            f\"[warning]Complex request detected - initiating planning phase[/warning]\\n\"\n",
    "            f\"Complexity: {analysis.complexity.value}\\n\"\n",
    "            f\"Confidence: {analysis.confidence:.2f}\\n\"\n",
    "            f\"Reasoning: {analysis.reasoning}\",\n",
    "            title=\"🧠 Complex Execution\"\n",
    "        ))\n",
    "        \n",
    "        results = {\"success\": True, \"phases\": []}\n",
    "        \n",
    "        # Phase 1: Planning (if required)\n",
    "        if analysis.requires_planning:\n",
    "            planning_result = await self._execute_planning_phase(request)\n",
    "            results[\"phases\"].append({\"phase\": \"planning\", \"result\": planning_result})\n",
    "            \n",
    "            if not planning_result.get(\"success\", False):\n",
    "                results[\"success\"] = False\n",
    "                return results\n",
    "        \n",
    "        # Phase 2: Debate (if required)\n",
    "        if analysis.requires_debate:\n",
    "            debate_result = await self._execute_debate_phase(request)\n",
    "            results[\"phases\"].append({\"phase\": \"debate\", \"result\": debate_result})\n",
    "        \n",
    "        # Phase 3: Execution\n",
    "        execution_result = await self._execute_with_coordination(request, analysis)\n",
    "        results[\"phases\"].append({\"phase\": \"execution\", \"result\": execution_result})\n",
    "        \n",
    "        if not execution_result.get(\"success\", False):\n",
    "            results[\"success\"] = False\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def _execute_planning_phase(self, request: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute planning phase using planner agent\"\"\"\n",
    "        \n",
    "        self.console.print(\"[info]🔄 Initiating planning phase...[/info]\")\n",
    "        \n",
    "        try:\n",
    "            # Use actual planner from agentic/agent/planner\n",
    "            import asyncio\n",
    "            from agentic.agent.planner.executor import DynamicTaskExecutor\n",
    "            \n",
    "            executor = DynamicTaskExecutor()\n",
    "            planning_result = await executor.execute_project(request)\n",
    "            \n",
    "            if planning_result:\n",
    "                self.console.print(\"[success]✅ Planning completed successfully[/success]\")\n",
    "                return {\"success\": True, \"plan\": planning_result}\n",
    "            else:\n",
    "                return {\"success\": False, \"error\": \"Planning failed\"}\n",
    "                self.console.print(f\"[error]❌ Planning failed: {planning_result.get('error', 'Unknown error')}[/error]\")\n",
    "                return {\"success\": False, \"error\": \"Planning phase failed\"}\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Planning phase error: {str(e)}\"\n",
    "            self.console.print(f\"[error]{error_msg}[/error]\")\n",
    "            return {\"success\": False, \"error\": error_msg}\n",
    "    \n",
    "    async def _execute_debate_phase(self, request: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute debate phase for decision analysis\"\"\"\n",
    "        \n",
    "        self.console.print(\"[info]🔄 Initiating debate phase...[/info]\")\n",
    "        \n",
    "        try:\n",
    "            # Use actual debater from agentic/agent/debater.py\n",
    "            debate_result = await self.create_debate(\n",
    "                topic=f\"Decision analysis for: {request}\",\n",
    "                context=f\"User request: {request}\"\n",
    "            )\n",
    "            \n",
    "            if debate_result:\n",
    "                self.console.print(\"[success]✅ Debate analysis completed[/success]\")\n",
    "                return {\"success\": True, \"analysis\": debate_result}\n",
    "            else:\n",
    "                self.console.print(\"[warning]⚠️ Debate analysis had issues[/warning]\")\n",
    "                return {\"success\": True, \"warning\": \"Debate analysis incomplete\"}\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Debate phase error: {str(e)}\"\n",
    "            self.console.print(f\"[error]{error_msg}[/error]\")\n",
    "            return {\"success\": False, \"error\": error_msg}\n",
    "    \n",
    "    async def _execute_with_coordination(self, request: str, analysis: AnalysisResult) -> Dict[str, Any]:\n",
    "        \"\"\"Execute request with intelligent tool coordination\"\"\"\n",
    "        \n",
    "        self.console.print(\"[info]🔄 Executing with tool coordination...[/info]\")\n",
    "        \n",
    "        # Use production-grade system prompt\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": get_system_prompt()},\n",
    "            {\"role\": \"user\", \"content\": f\"Execute this complex request: {request}\"}\n",
    "        ]\n",
    "        \n",
    "        # Add conversation history for context\n",
    "        messages.extend(self.conversation_history[-5:])  # Last 5 messages for context\n",
    "        \n",
    "        try:\n",
    "            # Execute with tool coordination and config parameters\n",
    "            stream = self.settings_config.get('stream', True)\n",
    "            response = self.llm_client.create_completion(\n",
    "                messages=messages,\n",
    "                tools=self.tool_manager.get_tools(),\n",
    "                stream=stream,\n",
    "                temperature=self.model_config.get('temperature', 0.3),\n",
    "                max_tokens=self.model_config.get('max_tokens'),\n",
    "                timeout=self.model_config.get('timeout', 60)\n",
    "            )\n",
    "            \n",
    "            if stream:\n",
    "                result = self.llm_client.handle_streaming_response(response, self.console)\n",
    "                # Create message-like object from streaming result  \n",
    "                class StreamMessage:\n",
    "                    def __init__(self, result):\n",
    "                        self.content = result.get('content', '')\n",
    "                        raw_tool_calls = result.get('tool_calls', [])\n",
    "                        # Convert dict tool_calls to object-like structure\n",
    "                        self.tool_calls = []\n",
    "                        for tc in raw_tool_calls:\n",
    "                            if tc and tc.get('function'):\n",
    "                                class ToolCall:\n",
    "                                    def __init__(self, tc_dict):\n",
    "                                        self.id = tc_dict.get('id', '')\n",
    "                                        self.type = tc_dict.get('type', 'function')\n",
    "                                        class Function:\n",
    "                                            def __init__(self, func_dict):\n",
    "                                                self.name = func_dict.get('name', '')\n",
    "                                                self.arguments = func_dict.get('arguments', '{}')\n",
    "                                        self.function = Function(tc_dict['function'])\n",
    "                                self.tool_calls.append(ToolCall(tc))\n",
    "                message = StreamMessage(result)\n",
    "            else:\n",
    "                result = self.llm_client.process_response(response, self.console)\n",
    "                message = result.get('message')\n",
    "            \n",
    "            if message.tool_calls:\n",
    "                return await self._handle_tool_calls(message.tool_calls, messages)\n",
    "            else:\n",
    "                # Direct response - already printed by streaming/process_response\n",
    "                return {\"success\": True, \"response\": message.content}\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Coordinated execution failed: {str(e)}\"\n",
    "            self.console.print(f\"[error]{error_msg}[/error]\")\n",
    "            return {\"success\": False, \"error\": error_msg}\n",
    "    \n",
    "    def _execute_streaming_tool(self, tool_name: str, parameters: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Execute streaming tools with real-time output\"\"\"\n",
    "        try:\n",
    "            if tool_name == \"task_planner\":\n",
    "                return self._stream_planner_output(parameters)\n",
    "            elif tool_name == \"debate_agent\":\n",
    "                return self._stream_debate_output(parameters)\n",
    "            else:\n",
    "                # Execute normally for other tools\n",
    "                result = self.tool_manager.execute_tool(tool_name, parameters)\n",
    "                return result\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Streaming execution failed: {str(e)}\"}\n",
    "    \n",
    "    def _stream_planner_output(self, parameters: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Stream planner output using actual planner\"\"\"\n",
    "        try:\n",
    "            import asyncio\n",
    "            from agentic.agent.planner.executor import DynamicTaskExecutor\n",
    "            \n",
    "            request = parameters.get(\"request\", \"Unknown task\")\n",
    "            self.console.print(f\"[dim]🔄 Planning: {request}[/dim]\")\n",
    "            \n",
    "            executor = DynamicTaskExecutor()\n",
    "            result = asyncio.run(executor.execute_project(request))\n",
    "            \n",
    "            # Stream the result\n",
    "            if result:\n",
    "                result_text = str(result)\n",
    "                import time\n",
    "                for char in result_text:\n",
    "                    self.console.print(char, end=\"\", style=\"dim cyan\")\n",
    "                    time.sleep(0.005)\n",
    "                self.console.print(\"\\n\")\n",
    "            \n",
    "            return {\"success\": True, \"response\": result}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Planner streaming failed: {str(e)}\"}\n",
    "    \n",
    "    def _stream_debate_output(self, parameters: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Stream debate output using actual debater\"\"\"\n",
    "        try:\n",
    "            import asyncio\n",
    "            \n",
    "            topic = parameters.get(\"topic\", \"Unknown topic\")\n",
    "            context = parameters.get(\"context\", \"\")\n",
    "            \n",
    "            self.console.print(f\"[dim]🎯 Debating: {topic}[/dim]\")\n",
    "            \n",
    "            # Use actual debater\n",
    "            result = asyncio.run(self.create_debate(topic=topic, context=context))\n",
    "            \n",
    "            # Stream the result\n",
    "            if result:\n",
    "                result_text = str(result)\n",
    "                import time\n",
    "                for char in result_text:\n",
    "                    self.console.print(char, end=\"\", style=\"dim yellow\")\n",
    "                    time.sleep(0.005)\n",
    "                self.console.print(\"\\n\")\n",
    "            \n",
    "            return {\"success\": True, \"response\": result}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Debate streaming failed: {str(e)}\"}\n",
    "\n",
    "    def _get_tool_description(self, tool_name: str, parameters: Dict) -> str:\n",
    "        \"\"\"Generate human-readable description of what the tool will do\"\"\"\n",
    "        descriptions = {\n",
    "            \"execute_bash\": f\"run the following shell command:\\n{parameters.get('command', 'unknown command')}\",\n",
    "            \"fs_read\": self._get_fs_read_description(parameters),\n",
    "            \"fs_write\": f\"write to file: {parameters.get('path', 'unknown path')}\",\n",
    "            \"code_interpreter\": \"execute Python code for analysis and computation\",\n",
    "            \"task_planner\": f\"analyze and create a detailed plan for: {parameters.get('request', 'complex task')}\",\n",
    "            \"debate_agent\": f\"perform multi-perspective analysis on: {parameters.get('topic', 'decision topic')}\",\n",
    "            \"introspect\": \"analyze project capabilities and structure\"\n",
    "        }\n",
    "        return descriptions.get(tool_name, f\"execute {tool_name}\")\n",
    "    \n",
    "    def _get_fs_read_description(self, parameters: Dict) -> str:\n",
    "        \"\"\"Get specific description for fs_read operations\"\"\"\n",
    "        operations = parameters.get('operations', [{}])\n",
    "        if not operations:\n",
    "            return \"read unknown file\"\n",
    "        \n",
    "        op = operations[0]\n",
    "        mode = op.get('mode', 'unknown')\n",
    "        path = op.get('path', 'unknown path')\n",
    "        \n",
    "        if mode == 'Directory':\n",
    "            return f\"list directory contents: {path}\"\n",
    "        elif mode == 'Line':\n",
    "            return f\"read file contents: {path}\"\n",
    "        elif mode == 'Search':\n",
    "            pattern = op.get('pattern', 'unknown pattern')\n",
    "            return f\"search for '{pattern}' in: {path}\"\n",
    "        else:\n",
    "            return f\"read file or directory: {path}\"\n",
    "\n",
    "    async def _handle_tool_calls(self, tool_calls: List, messages: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"Handle tool calls with progress tracking and config-based approval\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        with Progress(\n",
    "            SpinnerColumn(),\n",
    "            TextColumn(\"[progress.description]{task.description}\"),\n",
    "            console=self.console\n",
    "        ) as progress:\n",
    "            \n",
    "            for tool_call in tool_calls:\n",
    "                task = progress.add_task(f\"Executing {tool_call.function.name}...\", total=1)\n",
    "                \n",
    "                try:\n",
    "                    # Parse tool parameters\n",
    "                    parameters = json.loads(tool_call.function.arguments)\n",
    "                    \n",
    "                    # Check if tool requires approval\n",
    "                    if (not self.auto_approve and \n",
    "                        self.requires_approval(tool_call.function.name)):\n",
    "                        \n",
    "                        # Show tool details and ask for approval\n",
    "                        self.console.print(Panel(\n",
    "                            f\"[bold yellow]Tool:[/bold yellow] {tool_call.function.name}\\n\"\n",
    "                            f\"[bold yellow]Parameters:[/bold yellow] {json.dumps(parameters, indent=2)}\\n\"\n",
    "                            f\"[bold yellow]Dangerous:[/bold yellow] {self.is_tool_dangerous(tool_call.function.name)}\",\n",
    "                            title=\"[bold red]🔐 Tool Approval Required[/bold red]\",\n",
    "                            border_style=\"red\",\n",
    "                            padding=(0, 1)\n",
    "                        ))\n",
    "                        \n",
    "                        approval = input(\"Approve this tool execution? (y/n): \").lower().strip()\n",
    "                        if approval != 'y':\n",
    "                            results.append({\n",
    "                                \"tool\": tool_call.function.name,\n",
    "                                \"success\": False,\n",
    "                                \"error\": \"Tool execution denied by user\"\n",
    "                            })\n",
    "                            progress.update(task, completed=1)\n",
    "                            continue\n",
    "                    \n",
    "                    # Show tool execution in Amazon Q style\n",
    "                    self.console.print(f\"\\n[bold cyan]🛠️  Using tool:[/bold cyan] [bold]{tool_call.function.name}[/bold] [dim](trusted)[/dim]\")\n",
    "                    self.console.print(\"[dim] ⋮[/dim]\")\n",
    "                    \n",
    "                    # Show what the tool will do\n",
    "                    tool_description = self._get_tool_description(tool_call.function.name, parameters)\n",
    "                    self.console.print(f\"[dim] ●[/dim] [bold]I will {tool_description}[/bold]\")\n",
    "                    \n",
    "                    # Check if this is a streaming tool\n",
    "                    if tool_call.function.name in [\"task_planner\", \"debate_agent\"]:\n",
    "                        self.console.print(f\"[dim]Streaming {tool_call.function.name} output...[/dim]\\n\")\n",
    "                        result = self._execute_streaming_tool(tool_call.function.name, parameters)\n",
    "                    else:\n",
    "                        # Execute tool normally\n",
    "                        result = self.tool_manager.execute_tool(\n",
    "                            tool_call.function.name, \n",
    "                            parameters\n",
    "                        )\n",
    "                    \n",
    "                    results.append({\n",
    "                        \"tool\": tool_call.function.name,\n",
    "                        \"success\": not result.get(\"error\"),\n",
    "                        \"result\": result\n",
    "                    })\n",
    "                    \n",
    "                    # Add tool result to messages for context\n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"content\": json.dumps(result)\n",
    "                    })\n",
    "                    \n",
    "                    progress.update(task, completed=1)\n",
    "                    \n",
    "                    if result.get(\"error\"):\n",
    "                        self.console.print(f\"[bold red]❌ Tool {tool_call.function.name} failed:[/bold red] {result['error']}\")\n",
    "                    else:\n",
    "                        self.console.print(f\"[bold green]✅ {tool_call.function.name} completed[/bold green]\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Tool execution error: {str(e)}\"\n",
    "                    self.console.print(f\"[error]{error_msg}[/error]\")\n",
    "                    results.append({\n",
    "                        \"tool\": tool_call.function.name,\n",
    "                        \"success\": False,\n",
    "                        \"error\": error_msg\n",
    "                    })\n",
    "                    progress.update(task, completed=1)\n",
    "        \n",
    "        # Get final response from LLM with tool results\n",
    "        try:\n",
    "            final_response = self.llm_client.create_completion(\n",
    "                messages=messages,\n",
    "                stream=False,\n",
    "                temperature=self.model_config.get('temperature', 0.2)\n",
    "            )\n",
    "            \n",
    "            final_content = final_response.choices[0].message.content\n",
    "            self.console.print(Markdown(final_content))\n",
    "            \n",
    "            return {\n",
    "                \"success\": all(r.get(\"success\", False) for r in results),\n",
    "                \"tool_results\": results,\n",
    "                \"final_response\": final_content\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"tool_results\": results,\n",
    "                \"error\": f\"Final response generation failed: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    async def process_request(self, request: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main request processing pipeline\"\"\"\n",
    "        \n",
    "        self.console.print(Panel(\n",
    "            f\"[bold cyan]Processing request:[/bold cyan] {request}\",\n",
    "            title=\"[bold blue]🤖 Buddy AI[/bold blue]\",\n",
    "            border_style=\"blue\",\n",
    "            padding=(0, 1)\n",
    "        ))\n",
    "        \n",
    "        # Add to conversation history with config-based limit\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": request})\n",
    "        \n",
    "        # Trim history if it exceeds max_history from config\n",
    "        if len(self.conversation_history) > self.max_history:\n",
    "            # Keep system message and trim from the beginning\n",
    "            self.conversation_history = self.conversation_history[-self.max_history:]\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Analyze request complexity\n",
    "            analysis = await self.analyze_request_complexity(request)\n",
    "            \n",
    "            if self.debug:\n",
    "                self.console.print(f\"[info]Analysis: {analysis.complexity.value} (confidence: {analysis.confidence:.2f})[/info]\")\n",
    "            \n",
    "            # Step 2: Route based on complexity\n",
    "            if analysis.complexity == RequestComplexity.SIMPLE:\n",
    "                result = await self.execute_simple_request(request, analysis.suggested_tools)\n",
    "            else:\n",
    "                result = await self.execute_complex_request(request, analysis)\n",
    "            \n",
    "            # Step 3: Add result to conversation history\n",
    "            if result.get(\"success\", False):\n",
    "                response_content = result.get(\"response\", result.get(\"final_response\", \"Task completed\"))\n",
    "                self.conversation_history.append({\"role\": \"assistant\", \"content\": response_content})\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Request processing failed: {str(e)}\"\n",
    "            self.console.print(f\"[error]{error_msg}[/error]\")\n",
    "            return {\"success\": False, \"error\": error_msg}\n",
    "    \n",
    "    def display_config(self):\n",
    "        \"\"\"Display current configuration\"\"\"\n",
    "        self.console.print(Panel(\n",
    "            f\"[bold]Model Configuration:[/bold]\\n\"\n",
    "            f\"• Model: {self.model_config.get('name')}\\n\"\n",
    "            f\"• URL: {self.model_config.get('url')}\\n\"\n",
    "            f\"• Temperature: {self.model_config.get('temperature')}\\n\"\n",
    "            f\"• Timeout: {self.model_config.get('timeout')}s\\n\\n\"\n",
    "            f\"[bold]Settings:[/bold]\\n\"\n",
    "            f\"• Auto Approve: {self.auto_approve}\\n\"\n",
    "            f\"• Stream: {self.stream}\\n\"\n",
    "            f\"• Debug: {self.debug}\\n\"\n",
    "            f\"• Max History: {self.max_history}\\n\\n\"\n",
    "            f\"[bold]Tools:[/bold]\\n\"\n",
    "            f\"• Available: {', '.join(self.tool_manager.list_tools())}\\n\"\n",
    "            f\"• Dangerous: {', '.join(self.tools_config.get('dangerous_tools', []))}\\n\"\n",
    "            f\"• Require Approval: {', '.join(self.tools_config.get('require_approval', []))}\\n\\n\"\n",
    "            f\"[bold]Reasoning:[/bold]\\n\"\n",
    "            f\"• Show Thinking: {self.show_thinking}\\n\"\n",
    "            f\"• Retry Count: {self.retry_count}\",\n",
    "            title=\"🔧 Configuration\"\n",
    "        ))\n",
    "    \n",
    "    async def interactive_session(self):\n",
    "        \"\"\"Interactive chat session with config integration\"\"\"\n",
    "        \n",
    "        self.console.print(Panel(\n",
    "            f\"[bold green]Welcome to Buddy AI![/bold green]\\n\"\n",
    "            f\"I'm your intelligent assistant with adaptive complexity handling.\\n\\n\"\n",
    "            f\"[bold]Available Commands:[/bold]\\n\"\n",
    "            f\"• Type your request to get started\\n\"\n",
    "            f\"• '/config' - Show current configuration\\n\"\n",
    "            f\"• '/tools' - List available tools\\n\"\n",
    "            f\"• '/history' - Show conversation history\\n\"\n",
    "            f\"• '/clear' - Clear conversation history\\n\"\n",
    "            f\"• 'quit' or 'exit' - End the session\\n\\n\"\n",
    "            f\"[bold cyan]Current Model:[/bold cyan] {self.model_config.get('name')}\\n\"\n",
    "            f\"[bold cyan]Auto Approve:[/bold cyan] {self.auto_approve}\",\n",
    "            title=\"[bold blue]🤖 Buddy AI Assistant[/bold blue]\",\n",
    "            border_style=\"blue\",\n",
    "            padding=(0, 1)\n",
    "        ))\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Get user input\n",
    "                user_input = input(\"\\n💬 You: \").strip()\n",
    "                \n",
    "                if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                    self.console.print(\"[info]👋 Goodbye![/info]\")\n",
    "                    break\n",
    "                \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                # Handle special commands\n",
    "                if user_input.startswith('/'):\n",
    "                    await self._handle_command(user_input)\n",
    "                    continue\n",
    "                \n",
    "                # Process request\n",
    "                result = await self.process_request(user_input)\n",
    "                \n",
    "                if not result.get(\"success\", False):\n",
    "                    self.console.print(f\"[error]❌ Error: {result.get('error', 'Unknown error')}[/error]\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                self.console.print(\"\\n[warning]Session interrupted by user[/warning]\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                self.console.print(f\"[error]Session error: {str(e)}[/error]\")\n",
    "    \n",
    "    async def _handle_command(self, command: str):\n",
    "        \"\"\"Handle special commands\"\"\"\n",
    "        cmd = command.lower().strip()\n",
    "        \n",
    "        if cmd == '/config':\n",
    "            self.display_config()\n",
    "        elif cmd == '/tools':\n",
    "            tools = self.tool_manager.list_tools()\n",
    "            self.console.print(Panel(\n",
    "                f\"[bold]Available Tools ({len(tools)}):[/bold]\\n\" + \n",
    "                \"\\n\".join([f\"• {tool}\" for tool in tools]),\n",
    "                title=\"🛠️ Tools\"\n",
    "            ))\n",
    "        elif cmd == '/history':\n",
    "            if self.conversation_history:\n",
    "                history_text = \"\\n\".join([\n",
    "                    f\"[bold]{msg['role'].title()}:[/bold] {msg['content'][:100]}{'...' if len(msg['content']) > 100 else ''}\"\n",
    "                    for msg in self.conversation_history[-10:]  # Last 10 messages\n",
    "                ])\n",
    "                self.console.print(Panel(history_text, title=\"📜 Recent History\"))\n",
    "            else:\n",
    "                self.console.print(\"[info]No conversation history yet[/info]\")\n",
    "        elif cmd == '/clear':\n",
    "            self.conversation_history.clear()\n",
    "            self.console.print(\"[success]✅ Conversation history cleared[/success]\")\n",
    "        else:\n",
    "            self.console.print(f\"[error]Unknown command: {command}[/error]\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main entry point with config integration\"\"\"\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"Buddy AI Assistant\")\n",
    "    parser.add_argument(\"-c\", \"--command\", help=\"Single command to execute\")\n",
    "    parser.add_argument(\"--model\", help=\"Override LLM model from config\")\n",
    "    parser.add_argument(\"--base-url\", help=\"Override LLM base URL from config\")\n",
    "    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug mode\")\n",
    "    parser.add_argument(\"--auto-approve\", action=\"store_true\", help=\"Auto-approve all tool executions\")\n",
    "    parser.add_argument(\"--config\", action=\"store_true\", help=\"Show configuration and exit\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Initialize client with config\n",
    "    client = BuddyClient(model=args.model, base_url=args.base_url)\n",
    "    \n",
    "    # Override settings if provided\n",
    "    if args.debug:\n",
    "        client.debug = True\n",
    "    if args.auto_approve:\n",
    "        client.auto_approve = True\n",
    "    \n",
    "    # Show config and exit if requested\n",
    "    if args.config:\n",
    "        client.display_config()\n",
    "        return\n",
    "    \n",
    "    if args.command:\n",
    "        # Single command execution\n",
    "        client.console.print(f\"[info]Executing command with model: {client.model_config.get('name')}[/info]\")\n",
    "        result = await client.process_request(args.command)\n",
    "        if not result.get(\"success\", False):\n",
    "            exit(1)\n",
    "    else:\n",
    "        # Interactive session\n",
    "        await client.interactive_session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dcbc16-a2c6-4b42-a7e5-35aafb581a81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied-genai-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
