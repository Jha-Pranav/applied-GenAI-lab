{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38885ffa-374e-44a2-9206-5af15d1d901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp tools.fs_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102930a-9bb5-489d-8be3-05042a6591da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mFsReadParams\u001b[39;00m(BaseModel):\n\u001b[32m     81\u001b[39m     operations: List[FsReadOperation]\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseTool, ToolMetadata, ToolCategory\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mFsReadTool\u001b[39;00m(BaseTool):\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, log_level: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mINFO\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mImportError\u001b[39m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "__all__ = ['logger', 'ToolCallMode', 'FsReadOperation', 'FsReadParams', 'FsReadTool']\n",
    "\n",
    "# %% ../../nbs/buddy/backend/tools/filesystem/fs_read.ipynb 1\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "import fnmatch\n",
    "import json\n",
    "from pydantic import BaseModel, field_validator, Field, ValidationInfo, ValidationError\n",
    "from enum import Enum\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set up logging (configurable level)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ToolCallMode(str, Enum):\n",
    "    DISCOVER = \"discover\"\n",
    "    EXTRACT = \"extract\"\n",
    "\n",
    "class FsReadOperation(BaseModel):\n",
    "    mode: ToolCallMode\n",
    "    path: str = Field(..., description=\"Specific file or directory path to operate on, e.g., 'src' or 'app.py'. Relative to project root if not absolute.\")\n",
    "    query: Optional[str] = Field(None, description=\"Search query as regex for DISCOVER (file name) or EXTRACT (content). Required for EXTRACT.\")\n",
    "    file_pattern: str = Field(\"*\", description=\"Glob filter for files, e.g., '*.py' or '*.go|*.rs'.\")\n",
    "    max_depth: Optional[int] = Field(10, ge=1, description=\"Maximum recursion depth for DISCOVER mode. Value SHOULD BE >= 1\")\n",
    "    max_files: Optional[int] = Field(50, ge=1, description=\"Maximum number of files to return in DISCOVER mode\")\n",
    "\n",
    "    @field_validator(\"path\")\n",
    "    @classmethod\n",
    "    def validate_path(cls, value: str, info: ValidationInfo) -> str:\n",
    "        try:\n",
    "            project_root = os.getcwd()\n",
    "            if not value:\n",
    "                value = project_root\n",
    "                logger.info(f\"Path empty; defaulting to project root: {value}\")\n",
    "            path_obj = Path(value).resolve()\n",
    "            if not path_obj.exists():\n",
    "                raise ValueError(f\"Path {value} does not exist\")\n",
    "            if not str(path_obj).startswith(project_root):\n",
    "                raise ValueError(f\"Path {value} is outside project root; explicit permission is required to access external paths.\")\n",
    "            return str(path_obj)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Path validation failed: {str(e)}\")\n",
    "            raise ValueError(f\"Invalid path: {str(e)}\")\n",
    "\n",
    "    @field_validator(\"query\")\n",
    "    @classmethod\n",
    "    def validate_query(cls, value: Any, info: ValidationInfo) -> Any:\n",
    "        mode = info.data.get(\"mode\")\n",
    "        if mode == ToolCallMode.EXTRACT and value is None:\n",
    "            raise ValueError(\"Query is required for extract mode to perform regex matching.\")\n",
    "        if mode == ToolCallMode.DISCOVER and value is None:\n",
    "            return \"\"\n",
    "        if value:\n",
    "            try:\n",
    "                re.compile(value)\n",
    "            except re.error as e:\n",
    "                logger.error(f\"Invalid regex query: {value} ({str(e)})\")\n",
    "                raise ValueError(f\"Invalid regex query: {value} ({str(e)})\")\n",
    "        return value\n",
    "\n",
    "    @field_validator(\"file_pattern\")\n",
    "    @classmethod\n",
    "    def validate_file_pattern(cls, value: Any) -> Any:\n",
    "        if not isinstance(value, str):\n",
    "            raise ValueError(\"file_pattern must be a string\")\n",
    "        try:\n",
    "            for pattern in value.split('|'):\n",
    "                fnmatch.fnmatch(\"test.txt\", pattern)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Invalid glob pattern: {value} ({str(e)})\")\n",
    "            raise ValueError(f\"Invalid glob pattern: {value} ({str(e)})\")\n",
    "        return value\n",
    "\n",
    "class FsReadParams(BaseModel):\n",
    "    operations: List[FsReadOperation]\n",
    "\n",
    "from agentic.tools.base import BaseTool, ToolMetadata, ToolCategory\n",
    "\n",
    "class FsReadTool(BaseTool):\n",
    "    def __init__(self, log_level: str = \"INFO\"):\n",
    "        metadata = ToolMetadata(\n",
    "            name=\"fs_read\",\n",
    "            description=\"Read filesystem with regex search and exclusions, supporting file discovery or content extraction\",\n",
    "            category=ToolCategory.FILESYSTEM\n",
    "        )\n",
    "        super().__init__(metadata)\n",
    "        logging.getLogger().setLevel(getattr(logging, log_level, logging.INFO))\n",
    "        self.project_root = os.getcwd()\n",
    "        self.exclusion_patterns = [\n",
    "            \".*\", \"*.pyc\", \"*.o\", \"*.obj\", \"*.class\", \"*.exe\", \"*.dll\", \"*.so\",\n",
    "            \"*.lock\", \"node_modules/*\", \"dist/*\", \"build/*\", \"__pycache__/*\",\n",
    "            \"*.bin\", \"*.zip\", \"*.tar.gz\", \"*.log\"\n",
    "        ]\n",
    "        self._load_gitignore()\n",
    "\n",
    "    def _load_gitignore(self) -> None:\n",
    "        \"\"\"Load .gitignore patterns to exclude files.\"\"\"\n",
    "        gitignore_path = Path(self.project_root) / \".gitignore\"\n",
    "        if gitignore_path.exists():\n",
    "            try:\n",
    "                with open(gitignore_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip()\n",
    "                        if line and not line.startswith('#'):\n",
    "                            self.exclusion_patterns.append(line)\n",
    "            except (OSError, UnicodeDecodeError) as e:\n",
    "                logger.debug(f\"Failed to read {gitignore_path}: {e}\")\n",
    "\n",
    "    def _is_excluded(self, rel_path: str) -> bool:\n",
    "        \"\"\"Check if a path matches exclusion patterns.\"\"\"\n",
    "        return any(fnmatch.fnmatch(rel_path, p) for p in self.exclusion_patterns)\n",
    "\n",
    "    def _get_exclusion_weight(self, rel_path: str) -> int:\n",
    "        \"\"\"Assign a weight to prioritize non-excluded files.\"\"\"\n",
    "        if rel_path.startswith('.'):\n",
    "            return 5\n",
    "        lower_path = rel_path.lower()\n",
    "        if any(term in lower_path for term in ['venv', 'env', 'node_modules', 'dist', 'build']):\n",
    "            return 1\n",
    "        if any(term in lower_path for term in ['__pycache__', '.cache', 'temp']):\n",
    "            return 2\n",
    "        return 3  # Default for other excluded\n",
    "\n",
    "    def _get_file_info(self, path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get metadata about a file, including size and binary status.\"\"\"\n",
    "        path_obj = Path(path)\n",
    "        if not path_obj.exists():\n",
    "            return {\"error\": \"File not found\"}\n",
    "\n",
    "        try:\n",
    "            stat = path_obj.stat()\n",
    "        except OSError as e:\n",
    "            return {\"error\": f\"Stat failed: {e}\"}\n",
    "\n",
    "        rel_path = os.path.relpath(path, self.project_root)\n",
    "        if self._is_excluded(rel_path):\n",
    "            return {\"error\": \"Excluded by patterns\"}\n",
    "\n",
    "        file_size = stat.st_size\n",
    "        is_binary = False\n",
    "        lines = 0\n",
    "\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                chunk = f.read(2048)  # Larger sample for better binary detection\n",
    "                if b'\\0' in chunk:\n",
    "                    is_binary = True\n",
    "                else:\n",
    "                    try:\n",
    "                        chunk.decode('utf-8')\n",
    "                        with open(path, 'r', encoding='utf-8', errors='ignore') as f_text:\n",
    "                            lines = sum(1 for _ in f_text)\n",
    "                    except UnicodeDecodeError:\n",
    "                        is_binary = True\n",
    "        except (OSError, UnicodeDecodeError) as e:\n",
    "            logger.debug(f\"Error checking file {path}: {str(e)}\")\n",
    "            is_binary = True\n",
    "\n",
    "        return {\n",
    "            \"size\": file_size,\n",
    "            \"is_binary\": is_binary,\n",
    "            \"file_type\": path_obj.suffix.lower(),\n",
    "            \"is_large\": file_size > 1024 * 1024,\n",
    "            \"lines\": lines if not is_binary else 0\n",
    "        }\n",
    "\n",
    "    def _build_tree(self, rel_paths: List[str]) -> str:\n",
    "        \"\"\"Build a tree representation of file paths.\"\"\"\n",
    "        tree = {}\n",
    "        for rel_path in rel_paths:\n",
    "            parts = rel_path.split(os.sep)\n",
    "            current = tree\n",
    "            for part in parts[:-1]:  # Dirs\n",
    "                if part not in current:\n",
    "                    current[part] = {}\n",
    "                current = current[part]\n",
    "            if parts:  # File\n",
    "                current[parts[-1]] = {}  # Empty dict for file\n",
    "\n",
    "        def print_tree(node, prefix: str = \"\") -> List[str]:\n",
    "            lines = []\n",
    "            items = sorted(node.keys())\n",
    "            for i, item in enumerate(items):\n",
    "                is_last = i == len(items) - 1\n",
    "                connector = \"└── \" if is_last else \"├── \"\n",
    "                lines.append(prefix + connector + item)\n",
    "                sub_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "                lines.extend(print_tree(node[item], sub_prefix))\n",
    "            return lines\n",
    "\n",
    "        tree_lines = print_tree(tree)\n",
    "        return \"\\n\".join(tree_lines) if tree_lines else \"No files found\"\n",
    "\n",
    "    def _discover_files(self, path: str, file_pattern: str, query: str, max_depth: int, max_files: int) -> str:\n",
    "        \"\"\"Discover files matching patterns and query, returning a tree structure.\"\"\"\n",
    "        start_time = time.time()\n",
    "        path_obj = Path(path)\n",
    "        if not path_obj.is_dir():\n",
    "            return json.dumps({\"error\": \"Path is not a directory for discovery\"})\n",
    "\n",
    "        query_pattern = re.compile(query, re.IGNORECASE) if query else None\n",
    "        patterns = [fnmatch.translate(p) for p in file_pattern.split('|')]\n",
    "        pattern_regexes = [re.compile(p) for p in patterns]\n",
    "        candidates = []\n",
    "\n",
    "        def _collect_with_walk(dir_path: str, current_depth: int, file_count: List[int]) -> None:\n",
    "            if current_depth > max_depth or file_count[0] >= max_files:\n",
    "                return\n",
    "            try:\n",
    "                for entry in os.scandir(dir_path):\n",
    "                    rel_path = entry.name\n",
    "                    try:\n",
    "                        rel_path = str(Path(entry.path).relative_to(self.project_root))\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                    if self._is_excluded(rel_path):\n",
    "                        continue\n",
    "                    if entry.is_file() and any(p.match(entry.name) for p in pattern_regexes):\n",
    "                        if query_pattern and not query_pattern.search(entry.name):\n",
    "                            continue\n",
    "                        weight = 10 if not self._is_excluded(rel_path) else self._get_exclusion_weight(rel_path)\n",
    "                        candidates.append((entry.path, weight))\n",
    "                        file_count[0] += 1\n",
    "                    if entry.is_dir():\n",
    "                        _collect_with_walk(entry.path, current_depth + 1, file_count)\n",
    "            except (OSError, PermissionError) as e:\n",
    "                logger.debug(f\"Scan error in {dir_path}: {e}\")\n",
    "\n",
    "        file_count = [0]\n",
    "        _collect_with_walk(str(path_obj), 0, file_count)\n",
    "\n",
    "        if not candidates:\n",
    "            return json.dumps({\"error\": \"No files matched the criteria\"})\n",
    "\n",
    "        candidates.sort(key=lambda x: (-x[1], x[0]))\n",
    "        selected_paths = [p for p, w in candidates[:max_files]]\n",
    "        rel_paths = [os.path.relpath(p, path) for p in selected_paths]\n",
    "        tree_str = self._build_tree(rel_paths)\n",
    "\n",
    "        logger.info(f\"Discovered {len(selected_paths)} files in {time.time() - start_time:.2f}s\")\n",
    "        return json.dumps({\"data\": tree_str})\n",
    "\n",
    "    def _extract_content(self, path: str, query: str, file_pattern: str) -> str:\n",
    "        \"\"\"Extract content from files matching query and pattern.\"\"\"\n",
    "        path_obj = Path(path)\n",
    "        if not path_obj.exists():\n",
    "            return json.dumps({\"error\": f\"Path {path} does not exist\"})\n",
    "\n",
    "        if path_obj.is_file():\n",
    "            rel_path = os.path.relpath(path, self.project_root)\n",
    "            if self._is_excluded(rel_path):\n",
    "                return json.dumps({\"error\": f\"Path {path} is excluded by patterns\"})\n",
    "            content = self._extract_from_file(path, query)\n",
    "            return json.dumps({\"data\": content}, ensure_ascii=False)\n",
    "        else:\n",
    "            snippets = []\n",
    "            for root, dirs, files in os.walk(path):\n",
    "                for file in files:\n",
    "                    if any(fnmatch.fnmatch(file, p) for p in file_pattern.split('|')):\n",
    "                        full_path = os.path.join(root, file)\n",
    "                        rel_path = os.path.relpath(full_path, self.project_root)\n",
    "                        if not self._is_excluded(rel_path):\n",
    "                            file_snip = self._extract_from_file(full_path, query)\n",
    "                            if file_snip and not file_snip.startswith(\"[Binary\") and not file_snip.startswith(\"Error\"):\n",
    "                                snippets.append({\"file\": rel_path, \"snippet\": file_snip[:1000]})  # Limit snippet size\n",
    "            if not snippets:\n",
    "                return json.dumps({\"error\": f\"No valid text files matched the criteria in {path}\"})\n",
    "            try:\n",
    "                content = json.dumps({\"data\": snippets}, ensure_ascii=False)\n",
    "                if len(content) > 16 * 1024:\n",
    "                    # Truncate snippets instead of the JSON\n",
    "                    truncated_snippets = snippets[:10]  # Limit to first 10 files\n",
    "                    content = json.dumps({\"data\": truncated_snippets, \"truncated\": True}, ensure_ascii=False)\n",
    "                return content\n",
    "            except Exception as e:\n",
    "                return json.dumps({\"error\": f\"JSON serialization failed: {str(e)}\"})\n",
    "\n",
    "    def _extract_from_file(self, file_path: str, query: str) -> str:\n",
    "        \"\"\"Extract content from a single file with regex matching.\"\"\"\n",
    "        file_info = self._get_file_info(file_path)\n",
    "        if \"error\" in file_info:\n",
    "            return f\"Error: {file_info['error']}\"\n",
    "        if file_info[\"is_binary\"]:\n",
    "            return f\"[Binary file: {file_info['size']} bytes]\"\n",
    "        try:\n",
    "            pattern = re.compile(query, re.IGNORECASE | re.MULTILINE) if query else None\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                lines = f.readlines()\n",
    "            if not query:\n",
    "                content = \"\".join(lines)\n",
    "                if len(content) > 1024:  # Smaller limit\n",
    "                    content = content[:1024] + \"... [truncated]\"\n",
    "                return content + f\"\\n--- File Info: {file_info['lines']} lines, {file_info['size']} bytes ---\"\n",
    "            matches = []\n",
    "            for line_num, line in enumerate(lines, 1):\n",
    "                if pattern and pattern.search(line):\n",
    "                    # Clean the line to avoid JSON issues\n",
    "                    clean_line = line.strip().replace('\"', '\\\\\"').replace('\\n', '\\\\n').replace('\\r', '\\\\r')\n",
    "                    matches.append(f\"Line {line_num}: {clean_line}\")\n",
    "                    if len(matches) >= 20:  # Limit matches per file\n",
    "                        break\n",
    "            if not matches:\n",
    "                return \"No matches found\"\n",
    "            content = \"\\n\".join(matches)\n",
    "            if len(content) > 16 * 1024:\n",
    "                content = content[:16 * 1024] + \"... [truncated]\"\n",
    "            return content + f\"\\n--- File Info: {file_info['lines']} lines, {file_info['size']} bytes ---\"\n",
    "        except (OSError, UnicodeDecodeError) as e:\n",
    "            logger.error(f\"Error reading file {file_path}: {str(e)}\")\n",
    "            return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "    def get_parameters_schema(self, verbose: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Return OpenAI-compatible schema for the tool.\"\"\"\n",
    "        schema = {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"fs_read\",\n",
    "                \"description\": \"Discover files in tree structure or extract regex-matched snippets. Uses exclusions with fallback sorting.\" if verbose else \"\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"operations\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"mode\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"enum\": [mode.value for mode in ToolCallMode],\n",
    "                                        \"description\": \"Select 'discover' to list files in tree or 'extract' to pull snippets.\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"path\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"File or directory path relative to project root, e.g., 'src' or 'app.py'.\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"query\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"Regex for fuzzy file name (DISCOVER) or content (EXTRACT). Required for EXTRACT.\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"file_pattern\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"Glob filter, e.g., '*.py' or '*.go|*.rs'.\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"max_depth\": {\n",
    "                                        \"type\": \"integer\",\n",
    "                                        \"description\": \"Maximum recursion depth for DISCOVER mode (default: 10).\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"max_files\": {\n",
    "                                        \"type\": \"integer\",\n",
    "                                        \"description\": \"Maximum number of files to return in DISCOVER mode (default: 50).\" if verbose else \"\"\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\"mode\", \"path\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"operations\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return schema[\"function\"][\"parameters\"]\n",
    "\n",
    "    def execute(self, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Execute filesystem read operations with robust error handling.\"\"\"\n",
    "        try:\n",
    "            if 'params' in kwargs:\n",
    "                params = kwargs['params']\n",
    "            else:\n",
    "                if 'operations' not in kwargs and ('mode' in kwargs or 'path' in kwargs):\n",
    "                    kwargs = {'operations': [kwargs]}\n",
    "                params = FsReadParams(**kwargs)\n",
    "            result = self._execute_internal(params)\n",
    "            # Simplify response for single successful operation\n",
    "            if len(params.operations) == 1 and result[\"success\"] and not result[\"error\"]:\n",
    "                op_result = result[\"data\"][0]\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"data\": json.loads(op_result[\"data\"])[\"data\"] if op_result[\"data\"] else None,\n",
    "                    \"error\": None,\n",
    "                    \"metadata\": {\"processed_files\": len(result[\"data\"])}\n",
    "                }\n",
    "            return result\n",
    "        except ValidationError as e:\n",
    "            logger.error(f\"Input validation failed: {str(e)}\")\n",
    "            return {\"success\": False, \"data\": [], \"error\": {\"type\": \"ValidationError\", \"message\": str(e)}, \"metadata\": {}}\n",
    "        except AttributeError as e:\n",
    "            logger.error(f\"Attribute error in execution: {str(e)}\")\n",
    "            return {\"success\": False, \"data\": [], \"error\": {\"type\": \"AttributeError\", \"message\": str(e)}, \"metadata\": {}}\n",
    "        except Exception as e:\n",
    "            logger.critical(f\"Unexpected execution error: {str(e)}\")\n",
    "            return {\"success\": False, \"data\": [], \"error\": {\"type\": \"UnexpectedError\", \"message\": str(e)}, \"metadata\": {}}\n",
    "\n",
    "    def _execute_internal(self, params: FsReadParams) -> Dict[str, Any]:\n",
    "        \"\"\"Internal execution logic for batch operations.\"\"\"\n",
    "        results = []\n",
    "        total_processed = 0\n",
    "\n",
    "        for op in params.operations:\n",
    "            mode = op.mode.value\n",
    "            path = op.path\n",
    "            output_data = None\n",
    "            error = None\n",
    "\n",
    "            try:\n",
    "                if mode == \"discover\":\n",
    "                    output_data = self._discover_files(path, op.file_pattern, op.query or \"\", op.max_depth, op.max_files)\n",
    "                    total_processed += op.max_files  # Approximate\n",
    "                elif mode == \"extract\":\n",
    "                    output_data = self._extract_content(path, op.query, op.file_pattern)\n",
    "                    total_processed += 1\n",
    "            except ValueError as e:\n",
    "                if \"outside project root\" in str(e):\n",
    "                    error = {\"type\": \"PermissionError\", \"message\": str(e)}\n",
    "                else:\n",
    "                    error = {\"type\": \"ValueError\", \"message\": str(e)}\n",
    "                logger.error(f\"Error in {mode} on {path}: {str(e)}\")\n",
    "            except (OSError, UnicodeDecodeError, re.error) as e:\n",
    "                error = {\"type\": type(e).__name__, \"message\": str(e)}\n",
    "                logger.error(f\"Error in {mode} on {path}: {str(e)}\")\n",
    "\n",
    "            results.append({\n",
    "                \"mode\": mode,\n",
    "                \"path\": path,\n",
    "                \"data\": output_data,\n",
    "                \"error\": error\n",
    "            })\n",
    "\n",
    "        overall_success = all(r.get(\"error\") is None for r in results)\n",
    "        overall_error = None if overall_success else {\"type\": \"BatchError\", \"message\": \"Some operations failed—check individual errors.\"}\n",
    "        return {\n",
    "            \"success\": overall_success,\n",
    "            \"data\": results,\n",
    "            \"error\": overall_error,\n",
    "            \"metadata\": {\"processed_files\": total_processed}\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a97086-235d-46f5-a102-2ef99f1b8d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
