# Multi-Agent Software Development Benchmark

This benchmark suite contains 100 comprehensive problem statements designed to test the autonomy and capabilities of AI agents in software development, with focus on data science, data engineering, DevOps, MLOps, GitOps, and Kubernetes.

## Structure

- **01-project-analysis/**: Project understanding and code analysis tasks (1-15)
- **02-bug-fixing/**: Issue resolution and debugging tasks (16-30)  
- **03-feature-enhancement/**: Feature addition and system enhancement tasks (31-50)
- **04-data-science-ml/**: Data science and ML engineering tasks (51-70)
- **05-devops-infrastructure/**: DevOps and infrastructure tasks (71-85)
- **06-advanced-scenarios/**: Complex multi-agent scenarios (86-100)

## Usage

Each folder contains procedural documents with:
- Problem statement
- Detailed requirements
- Success criteria
- Expected deliverables
- Complexity indicators

## Complexity Levels

- **Beginner**: Single-domain tasks with clear requirements
- **Intermediate**: Multi-domain tasks requiring coordination
- **Advanced**: Complex systems with real-time requirements
- **Expert**: Enterprise-scale with safety/compliance requirements

## Testing Framework

Use these benchmarks to evaluate:
- Agent autonomy and decision-making
- Multi-agent coordination capabilities
- Technical depth and accuracy
- Problem-solving approaches
- Code quality and best practices



## Key Dimensions to Evaluate

Here are the features youâ€™ll want your benchmarking setup to test:

1. **Retrieval quality** â€” ability to fetch relevant documents / data given a user query (or command).
2. **Answer correctness / completeness** â€” is the answer accurate, complete, and meets the userâ€™s query?
3. **Groundedness / hallucination** â€” are answers supported by retrieved material (or known data)?
4. **Tool / command invocation / function calling** â€” does the system correctly decide when to call tools, which ones, with the correct parameters?
5. **CLI / shell interactions** â€” ability to run commands, chain commands, handle errors, parse outputs, possibly escalate privileges, etc.
6. **Conversational & multiâ€‘step context** â€” followâ€‘ups, clarifying questions, context retention across multiple steps.
7. **Latency / response time / resource usage** â€” important for CLI UX.
8. **Safety / robustness** â€” what happens with invalid/malicious input, edgeâ€‘cases.
9. **User experience / interpretability** â€” clarity of responses, error messages, prompts, logs etc.

---
Here's a **consolidated list** of all the datasets we've discussed so far, grouped by **task type**, with descriptions and sources:

---

## ğŸ§  A. **Project-Level Code Generation Datasets**

| Dataset                    | Description                                                                                                                                                        | Source / Link                                                                                                            |
| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------ |
| **ProjectEval**            | Benchmark for *end-to-end project generation* tasks. Each task has natural language prompts, checklists, optional skeletons, and test suites.                      | [ğŸ“„ Paper](https://arxiv.org/abs/2503.07010) Â· [ğŸ’» GitHub (planned)](https://projecteval.github.io)                      |
| **CoderEval**              | 460 programming tasks (Python & Java) with varying context dependencies â€” larger than isolated functions, but not full projects.                                   | [ğŸ“„ Paper](https://arxiv.org/abs/2302.00288)                                                                             |
| **TransCoder / TransRepo** | Repository-level code translation tasks (e.g. Java â†’ C#), includes structure, comments, and build config â€” not gen-from-scratch but useful for project-scale eval. | [ğŸ“„ TransCoder](https://arxiv.org/abs/2006.03511) Â· [ğŸ“„ TransRepo](https://arxiv.org/abs/2501.16050)                     |
| **OpenCodeInstruct**       | Massive multi-task dataset with problems, solutions, and test cases. Mostly function-level, but supports composing tasks into projects.                            | [ğŸ“„ Paper](https://arxiv.org/abs/2504.04030) Â· [ğŸ¤— HF Dataset](https://huggingface.co/datasets/Intel/open-code-instruct) |

---

## ğŸ§ª B. **Bug Fixing / Software Maintenance (SWE-bench Family)**

| Dataset                    | Description                                                                                                                    | Source / Link                                                                                                             |
| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------- |
| **SWE-bench**              | ~2.3K GitHub issues with test cases and patches. Models must generate a patch to fix the issue and pass the test suite.        | [ğŸŒ Website](https://www.swebench.com) Â· [ğŸ¤— Dataset](https://huggingface.co/datasets/princeton-nlp/SWE-bench)            |
| **SWE-bench_Lite**         | Smaller, faster-to-evaluate version (534 issues).                                                                              | [Same source as above](https://www.swebench.com/SWE-bench/guides/datasets/)                                               |
| **SWE-bench_Verified**     | Human-curated subset (~500) with verified issue/patch pairs and reliable test coverage. Used by Amazon Q Developer and others. | [ğŸ¤— Dataset](https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified)                                            |
| **SWE-bench_Multilingual** | SWE-bench extension for multiple programming languages (e.g., Java, JS, TS).                                                   | [ğŸŒ Info](https://www.swebench.com/multilingual)                                                                          |
| **SWE-bench_Multimodal**   | Tasks with UI elements/screenshots (i.e. multimodal inputs).                                                                   | [ğŸŒ Info](https://www.swebench.com/SWE-bench/guides/datasets/)                                                            |
| **SWE-PolyBench**          | Multilingual repo-level benchmark with multiple task types (bug fix, feature add, refactoring). Created by Amazon Science.     | [ğŸ“„ Paper](https://arxiv.org/abs/2504.08703) Â· [ğŸ¤— Dataset](https://huggingface.co/datasets/amazon-science/swe-polybench) |

---

## ğŸ” C. **RAG / Retrieval & Knowledge Benchmarks (Used by Amazon Q Business)**

| Dataset                         | Description                                                                                                             | Source / Link                                                                                           |
| ------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- |
| **KILT**                        | Unified benchmark for knowledge-intensive NLP tasks (QA, fact-checking, etc.) using Wikipedia as retrieval base.        | [ğŸ“„ Paper](https://arxiv.org/abs/2009.02252) Â· [ğŸ“ GitHub](https://github.com/facebookresearch/KILT)    |
| **RAGChecker**                  | Tests hallucination and attribution for RAG models. Measures grounding vs. hallucination. Used in Amazon Qâ€™s RAG evals. | [ğŸ“„ Paper](https://arxiv.org/abs/2310.03659) Â· [ğŸ“ GitHub](https://github.com/sileod/rag-checker)       |
| **BBQ (Bias Benchmark for QA)** | Evaluates social bias in QA models using ambiguous/disambiguated questions.                                             | [ğŸ“ GitHub](https://github.com/nyu-mll/BBQ) Â· [ğŸ¤— Dataset](https://huggingface.co/datasets/nyu-mll/BBQ) |
| **OpenAI Moderation Dataset**   | Dataset used to test model refusal/safety performance. Amazon Q Business uses this to evaluate content moderation.      | [ğŸ“ GitHub](https://github.com/openai/openai-content-moderation)                                        |

---

## ğŸ“Œ Summary by Task Type

| Task Type                     | Datasets                                            |
| ----------------------------- | --------------------------------------------------- |
| **Project Generation**        | ProjectEval, CoderEval, TransRepo, OpenCodeInstruct |
| **Bug Fixing / Maintenance**  | SWE-bench, SWE-bench_Verified, SWE-PolyBench        |
| **Multilingual / Multimodal** | SWE-bench_Multilingual, SWE-bench_Multimodal        |
| **Bias / Safety / RAG**       | KILT, RAGChecker, BBQ, OpenAI Moderation            |

---

