{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d1e6cc-45ec-41e4-bb48-a28b7c75f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp agent.introspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba94e9bf-830c-4615-8357-0418e57d1cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export \n",
    "from typing import Dict, Any, Optional\n",
    "from rich.console import Console\n",
    "\n",
    "\n",
    "class IntrospectAgent:\n",
    "    \"\"\"LLM-powered task execution evaluation agent\"\"\"\n",
    "    \n",
    "    def __init__(self, agent, console: Console):\n",
    "        self.agent = agent\n",
    "        self.console = console\n",
    "    \n",
    "    def evaluate_execution(\n",
    "        self, \n",
    "        task_context: Dict[str, Any],\n",
    "        execution_result: str,\n",
    "        success_criteria: str,\n",
    "        expected_outputs: list = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate task execution quality and provide feedback\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "TASK EXECUTION EVALUATION\n",
    "\n",
    "TASK CONTEXT:\n",
    "- Task ID: {task_context.get('task_id', 'unknown')}\n",
    "- Description: {task_context.get('description', '')}\n",
    "- Purpose: {task_context.get('purpose', '')}\n",
    "\n",
    "EXECUTION DETAILS:\n",
    "- Result: {execution_result}\n",
    "- Success Criteria: {success_criteria}\n",
    "- Expected Outputs: {expected_outputs or []}\n",
    "\n",
    "EVALUATION FRAMEWORK:\n",
    "Analyze the execution across these dimensions:\n",
    "\n",
    "1. COMPLETENESS (0-10): Did it fulfill all requirements and create expected outputs?\n",
    "2. QUALITY (0-10): Code quality, documentation, best practices followed?\n",
    "3. CORRECTNESS (0-10): Does the solution work as intended?\n",
    "4. EFFICIENCY (0-10): Is the approach optimal and well-structured?\n",
    "\n",
    "SCORING GUIDELINES:\n",
    "- 9-10: Excellent, exceeds expectations\n",
    "- 7-8: Good, meets requirements with minor issues\n",
    "- 5-6: Acceptable but needs improvement\n",
    "- 3-4: Poor, significant issues\n",
    "- 0-2: Failed, major problems\n",
    "\n",
    "RESPONSE FORMAT (JSON):\n",
    "{{\n",
    "  \"overall_score\": <0-10>,\n",
    "  \"category_scores\": {{\n",
    "    \"completeness\": <0-10>,\n",
    "    \"quality\": <0-10>, \n",
    "    \"correctness\": <0-10>,\n",
    "    \"efficiency\": <0-10>\n",
    "  }},\n",
    "  \"success\": <true/false>,\n",
    "  \"issues_found\": [\"issue1\", \"issue2\"],\n",
    "  \"recommendations\": [\"recommendation1\", \"recommendation2\"],\n",
    "  \"feedback_for_retry\": \"Specific guidance for improvement\",\n",
    "  \"next_action\": \"proceed|retry|major_revision\",\n",
    "  \"reasoning\": \"Detailed explanation of the evaluation\"\n",
    "}}\n",
    "\n",
    "Provide thorough, constructive evaluation focusing on actionable feedback.\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.agent.run(prompt, stream=True, max_iterations=1)\n",
    "            content = response.get(\"content\", \"\")\n",
    "            \n",
    "            # Try to parse JSON response\n",
    "            import json\n",
    "            try:\n",
    "                result = json.loads(content)\n",
    "                # Ensure required fields exist\n",
    "                result.setdefault(\"success\", result.get(\"overall_score\", 0) >= 7)\n",
    "                result.setdefault(\"performance_score\", result.get(\"overall_score\", 0))\n",
    "                return result\n",
    "            except json.JSONDecodeError:\n",
    "                # Fallback if JSON parsing fails\n",
    "                return {\n",
    "                    \"overall_score\": 5.0,\n",
    "                    \"success\": False,\n",
    "                    \"feedback_for_retry\": \"Evaluation parsing failed - please retry\",\n",
    "                    \"next_action\": \"retry\",\n",
    "                    \"reasoning\": f\"Raw response: {content}\"\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"âŒ Introspection error: {e}\")\n",
    "            return {\n",
    "                \"overall_score\": 0.0,\n",
    "                \"success\": False,\n",
    "                \"feedback_for_retry\": f\"Introspection failed: {str(e)}\",\n",
    "                \"next_action\": \"retry\",\n",
    "                \"reasoning\": \"Agent evaluation failed\"\n",
    "            }\n",
    "\n",
    "\n",
    "def create_introspection(\n",
    "    task_context: Dict[str, Any],\n",
    "    execution_result: str, \n",
    "    success_criteria: str,\n",
    "    expected_outputs: list = None,\n",
    "    agent = None,\n",
    "    console: Console = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Convenience function to create and run introspection\"\"\"\n",
    "    \n",
    "    if not console:\n",
    "        console = Console()\n",
    "    \n",
    "    introspect_agent = IntrospectAgent(agent, console)\n",
    "    return introspect_agent.evaluate_execution(\n",
    "        task_context, execution_result, success_criteria, expected_outputs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff9d88-a6e4-4df5-b5fd-202770d83a20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
