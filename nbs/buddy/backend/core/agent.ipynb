{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf998cdc-0e43-4f6b-a49e-356c2feb7227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp core.agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51acfc9-1396-440a-ac13-ad81e7398e23",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass, field\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMClient\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfigs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_model_config\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToolManager\n",
      "\u001b[31mImportError\u001b[39m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "from typing import List, Dict, Any, Optional, Callable\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "from agentic.llms.client import LLMClient\n",
    "from agentic.configs.loader import get_model_config\n",
    "from agentic.tools.manager import ToolManager\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c59eb6b-b464-4621-b204-bb6660334494",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LLMClient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m     temperature: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.7\u001b[39m\n\u001b[32m     17\u001b[39m     max_tokens: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mAgent\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;250;43m    \u001b[39;49m\u001b[33;43;03m\"\"\"Core Agent class with tool execution and conversation management\"\"\"\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAgentConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_client\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mLLMClient\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mAgent\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAgent\u001b[39;00m:\n\u001b[32m     20\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Core Agent class with tool execution and conversation management\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: AgentConfig, llm_client: Optional[\u001b[43mLLMClient\u001b[49m] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     23\u001b[39m         \u001b[38;5;28mself\u001b[39m.config = config\n\u001b[32m     24\u001b[39m         \u001b[38;5;28mself\u001b[39m.system_prompt = config.instructions\n",
      "\u001b[31mNameError\u001b[39m: name 'LLMClient' is not defined"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    role: str\n",
    "    content: str\n",
    "    tool_calls: Optional[List[Dict]] = None\n",
    "    tool_call_id: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class AgentConfig:\n",
    "    name: str\n",
    "    instructions: str = \"\"\n",
    "    model: Optional[str] = None\n",
    "    tools: List[str] = field(default_factory=list)\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: Optional[int] = None\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Core Agent class with tool execution and conversation management\"\"\"\n",
    "\n",
    "    def __init__(self, config: AgentConfig, llm_client: Optional[LLMClient] = None):\n",
    "        self.config = config\n",
    "        self.system_prompt = config.instructions\n",
    "        self.llm_client = llm_client or self._create_default_llm_client()\n",
    "        logger.info(f\"Initialized LLM client with model: {self.llm_client.model}\")\n",
    "        self.conversation_history: List[Message] = [Message(role=\"system\", content=self.system_prompt)]\n",
    "        self.tools_registry: Dict[str, Callable] = {}\n",
    "        self.guardrails: List[Callable] = []\n",
    "        self.tool_manager = ToolManager()\n",
    "\n",
    "    def _create_default_llm_client(self) -> LLMClient:\n",
    "        \"\"\"Create default LLM client from config.\"\"\"\n",
    "        model_config = get_model_config()\n",
    "        model_name = self.config.model or model_config.get('name')\n",
    "        if not model_name:\n",
    "            raise ValueError(\"No model specified in config\")\n",
    "        return LLMClient(\n",
    "            model=model_name,\n",
    "            base_url=model_config.get('url'),\n",
    "            api_key=model_config.get('api_key')\n",
    "        )\n",
    "\n",
    "    def add_tool(self, name: str, tool_func: Callable) -> None:\n",
    "        \"\"\"Register a tool with the agent.\"\"\"\n",
    "        if not callable(tool_func):\n",
    "            raise ValueError(f\"Tool function '{name}' must be callable\")\n",
    "        self.tools_registry[name] = tool_func\n",
    "\n",
    "    def add_guardrail(self, guardrail_func: Callable) -> None:\n",
    "        \"\"\"Add a guardrail function.\"\"\"\n",
    "        self.guardrails.append(guardrail_func)\n",
    "\n",
    "    def run(self, message: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Execute agent with message and return response.\"\"\"\n",
    "        # Apply guardrails\n",
    "        for guardrail in self.guardrails:\n",
    "            result = guardrail(message)\n",
    "            if not isinstance(result, bool) or not result:\n",
    "                return {\"content\": \"Request blocked by guardrails\", \"blocked\": True}\n",
    "\n",
    "        # Add user message\n",
    "        self.conversation_history.append(Message(role=\"user\", content=message))\n",
    "\n",
    "    def _is_conversation_complete(self, result: Dict[str, Any], iteration_count: int) -> bool:\n",
    "        \"\"\"Smart detection of conversation completion\"\"\"\n",
    "        \n",
    "        content = result.get(\"content\", \"\").strip()\n",
    "        tool_calls = result.get(\"tool_calls\", [])\n",
    "        finish_reason = result.get(\"finish_reason\")\n",
    "        \n",
    "        # 1. Explicit completion signals\n",
    "        if finish_reason in [\"stop\", \"length\", \"content_filter\"]:\n",
    "            return True\n",
    "            \n",
    "        # 2. Tool calls present - continue to execute them\n",
    "        if tool_calls:\n",
    "            return False\n",
    "            \n",
    "        # 3. Content analysis for completion indicators\n",
    "        completion_phrases = [\n",
    "            \"task completed\", \"done\", \"finished\", \"complete\",\n",
    "            \"that's all\", \"nothing more\", \"no further\", \n",
    "            \"task is finished\", \"successfully completed\"\n",
    "        ]\n",
    "        \n",
    "        if any(phrase in content.lower() for phrase in completion_phrases):\n",
    "            logger.debug(\"Detected completion phrase in content\")\n",
    "            return True\n",
    "            \n",
    "        # 4. Empty or minimal response (likely done)\n",
    "        if len(content) < 10 and not tool_calls:\n",
    "            logger.debug(\"Minimal response with no tool calls - likely complete\")\n",
    "            return True\n",
    "            \n",
    "        # 5. Repetitive responses (stuck in loop)\n",
    "        if iteration_count > 2:\n",
    "            recent_messages = self.conversation_history[-3:]\n",
    "            if len(recent_messages) >= 2:\n",
    "                last_content = recent_messages[-1].content\n",
    "                prev_content = recent_messages[-2].content\n",
    "                \n",
    "                # Check for identical or very similar responses\n",
    "                if last_content == prev_content or (\n",
    "                    len(last_content) > 20 and \n",
    "                    len(set(last_content.split()) & set(prev_content.split())) / \n",
    "                    max(len(last_content.split()), len(prev_content.split())) > 0.8\n",
    "                ):\n",
    "                    logger.debug(\"Detected repetitive responses - likely stuck\")\n",
    "                    return True\n",
    "        \n",
    "        # 6. Question without tool calls (asking for clarification)\n",
    "        if content.endswith('?') and not tool_calls and len(content) > 20:\n",
    "            logger.debug(\"LLM asking question - likely needs user input\")\n",
    "            return True\n",
    "            \n",
    "        # 7. Default: continue if we have meaningful content\n",
    "        return False\n",
    "\n",
    "    def run(self, message: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Execute agent with message and return response.\"\"\"\n",
    "        # Apply guardrails\n",
    "        for guardrail in self.guardrails:\n",
    "            result = guardrail(message)\n",
    "            if not isinstance(result, bool) or not result:\n",
    "                return {\"content\": \"Request blocked by guardrails\", \"blocked\": True}\n",
    "\n",
    "        # Add user message\n",
    "        self.conversation_history.append(Message(role=\"user\", content=message))\n",
    "\n",
    "        # Initialize result\n",
    "        final_result = {\"content\": \"\", \"tool_calls\": [], \"blocked\": False}\n",
    "        iteration_count = 0\n",
    "\n",
    "        while True:\n",
    "            iteration_count += 1\n",
    "            logger.debug(f\"Agent iteration {iteration_count}\")\n",
    "            \n",
    "            # Get available tools\n",
    "            available_tools = self._get_available_tools()\n",
    "\n",
    "            # Create completion\n",
    "            messages = self._format_messages_for_llm()\n",
    "            stream = kwargs.get('stream', True)\n",
    "            \n",
    "            # Filter out Agent-specific parameters before passing to LLM\n",
    "            llm_kwargs = {k: v for k, v in kwargs.items() \n",
    "                         if k not in ['max_iterations', 'stream']}\n",
    "            llm_kwargs['stream'] = stream  # Add stream back\n",
    "            \n",
    "            try:\n",
    "                response = self.llm_client.create_completion(\n",
    "                    messages=messages,\n",
    "                    tools=available_tools,\n",
    "                    **llm_kwargs\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.error(f\"LLM completion failed: {str(e)}\")\n",
    "                return {\"content\": f\"Error: {str(e)}\", \"blocked\": True}\n",
    "\n",
    "            # Process response\n",
    "            if stream:\n",
    "                if not hasattr(response, '__iter__'):\n",
    "                    raise ValueError(\"Streaming response expected but non-iterable response received\")\n",
    "                try:\n",
    "                    result = self.llm_client.handle_streaming_response(response)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing streaming response: {str(e)}\")\n",
    "                    return {\"content\": f\"Streaming error: {str(e)}\", \"blocked\": True}\n",
    "            else:\n",
    "                try:\n",
    "                    result = self.llm_client.process_response(response)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing response: {str(e)}\")\n",
    "                    return {\"content\": f\"Response error: {str(e)}\", \"blocked\": True}\n",
    "\n",
    "            # Add assistant response to history\n",
    "            assistant_message = Message(\n",
    "                role=\"assistant\",\n",
    "                content=result.get(\"content\", \"\"),\n",
    "                tool_calls=result.get(\"tool_calls\")\n",
    "            )\n",
    "            self.conversation_history.append(assistant_message)\n",
    "            final_result[\"content\"] = result.get(\"content\", \"\")\n",
    "            final_result[\"tool_calls\"].extend(result.get(\"tool_calls\", []))\n",
    "\n",
    "            # Smart completion detection\n",
    "            if self._is_conversation_complete(result, iteration_count):\n",
    "                logger.debug(\"Conversation detected as complete\")\n",
    "                break\n",
    "\n",
    "            # Handle tool calls if present\n",
    "            if result.get(\"tool_calls\"):\n",
    "                logger.debug(f\"Executing {len(result['tool_calls'])} tool calls\")\n",
    "                executed_calls = self._execute_tool_calls(result[\"tool_calls\"])\n",
    "                final_result[\"tool_calls\"] = executed_calls\n",
    "                continue  # Continue loop to process tool results\n",
    "\n",
    "        # Limit conversation history\n",
    "        if len(self.conversation_history) > 50:\n",
    "            self.conversation_history = [self.conversation_history[0]] + self.conversation_history[-49:]\n",
    "\n",
    "        return final_result\n",
    "\n",
    "    def _execute_tool_calls(self, tool_calls: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Execute tool calls and append results to conversation history.\"\"\"\n",
    "        from agentic.tools.display import ToolExecutionDisplay\n",
    "        display = ToolExecutionDisplay()\n",
    "        executed_calls = []\n",
    "\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call[\"function\"][\"name\"]\n",
    "            tool_call_id = tool_call.get(\"id\")\n",
    "            try:\n",
    "                arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.error(f\"Invalid arguments for {function_name}: {str(e)}\")\n",
    "                display.show_tool_error(f\"Error in {function_name}\", str(e))\n",
    "                tool_call[\"error\"] = str(e)\n",
    "                executed_calls.append(tool_call)\n",
    "                continue\n",
    "\n",
    "            display.show_tool_start(function_name, trusted=True, args=arguments)\n",
    "\n",
    "            try:\n",
    "                if function_name in self.tools_registry:\n",
    "                    result = self.tools_registry[function_name](**arguments)\n",
    "                else:\n",
    "                    result = self.tool_manager.execute_tool(function_name, arguments)\n",
    "                tool_call[\"result\"] = result\n",
    "                executed_calls.append(tool_call)\n",
    "\n",
    "                # Append tool result to conversation history\n",
    "                self.conversation_history.append(Message(\n",
    "                    role=\"tool\",\n",
    "                    content=str(result),\n",
    "                    tool_call_id=tool_call_id\n",
    "                ))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error executing {function_name}: {str(e)}\")\n",
    "                display.show_tool_error(f\"Error in {function_name}\", str(e))\n",
    "                tool_call[\"error\"] = str(e)\n",
    "                executed_calls.append(tool_call)\n",
    "\n",
    "        return executed_calls\n",
    "\n",
    "    def _get_available_tools(self) -> List[Dict]:\n",
    "        \"\"\"Get OpenAI-formatted tools for the configured tool names.\"\"\"\n",
    "        return self.tool_manager.get_tools(self.config.tools)\n",
    "\n",
    "    def _format_messages_for_llm(self) -> List[Dict]:\n",
    "        \"\"\"Convert Message objects to a format suitable for the LLM client.\"\"\"\n",
    "        messages = []\n",
    "        for msg in self.conversation_history:\n",
    "            message_dict = {\"role\": msg.role, \"content\": msg.content}\n",
    "            if msg.tool_calls:\n",
    "                message_dict[\"tool_calls\"] = msg.tool_calls\n",
    "            if msg.tool_call_id:\n",
    "                message_dict[\"tool_call_id\"] = msg.tool_call_id\n",
    "            messages.append(message_dict)\n",
    "        return messages\n",
    "\n",
    "    def clear_history(self) -> None:\n",
    "        \"\"\"Clear conversation history except system message.\"\"\"\n",
    "        self.conversation_history = [Message(role=\"system\", content=self.system_prompt)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2738fcf9-5abe-412e-9a81-a5b717472822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
