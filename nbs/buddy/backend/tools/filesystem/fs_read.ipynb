{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38885ffa-374e-44a2-9206-5af15d1d901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp tools.fs_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c102930a-9bb5-489d-8be3-05042a6591da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "import fnmatch\n",
    "import json\n",
    "from pydantic import BaseModel, field_validator, Field, ValidationInfo, ValidationError\n",
    "from enum import Enum\n",
    "from agentic.tools.base import create_success_response, create_error_response, extract_validation_error\n",
    "from agentic.tools.base import BaseTool, ToolMetadata, ToolCategory\n",
    "\n",
    "# Set up logging with a clear format\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ToolCallMode(str, Enum):\n",
    "    DISCOVER = \"discover\"\n",
    "    EXTRACT = \"extract\"\n",
    "\n",
    "class FsReadOperation(BaseModel):\n",
    "    mode: ToolCallMode\n",
    "    path: str = Field(\n",
    "        ...,\n",
    "        description=\"Specific file or directory path to operate on, e.g., 'src' or 'app.py'. Relative to project root if not absolute.\"\n",
    "    )\n",
    "    query: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Search query as regex for DISCOVER (file name) or EXTRACT (content). Required for EXTRACT.\"\n",
    "    )\n",
    "    file_pattern: str = Field(\n",
    "        \"*\",\n",
    "        description=\"Glob filter for files, e.g., '*.py|*.ipynb' or '*.go|*.rs'.\"\n",
    "    )\n",
    "    max_depth: Optional[int] = Field(\n",
    "        10,\n",
    "        ge=1,\n",
    "        description=\"Maximum recursion depth for DISCOVER mode. Value must be >= 1\"\n",
    "    )\n",
    "    max_files: Optional[int] = Field(\n",
    "        50,\n",
    "        ge=1,\n",
    "        description=\"Maximum number of files to return in DISCOVER mode\"\n",
    "    )\n",
    "\n",
    "    @field_validator(\"path\")\n",
    "    @classmethod\n",
    "    def validate_path(cls, value: str, info: ValidationInfo) -> str:\n",
    "        try:\n",
    "            project_root = os.getcwd()\n",
    "            if not value:\n",
    "                value = project_root\n",
    "                logger.info(f\"Path empty; defaulting to project root: {value}\")\n",
    "            path_obj = Path(value).resolve()\n",
    "            if not path_obj.exists():\n",
    "                raise ValueError(f\"Path does not exist: {value}\")\n",
    "            if not str(path_obj).startswith(project_root):\n",
    "                raise ValueError(f\"Path is outside project root: {value}; explicit permission required\")\n",
    "            return str(path_obj)\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"Path validation failed: {str(e)}\")\n",
    "            raise ValueError(f\"Invalid path: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error during path validation: {type(e).__name__} - {str(e)}\")\n",
    "            raise ValueError(f\"Unexpected path validation error: {type(e).__name__} - {str(e)}\")\n",
    "\n",
    "    @field_validator(\"query\")\n",
    "    @classmethod\n",
    "    def validate_query(cls, value: Any, info: ValidationInfo) -> Any:\n",
    "        mode = info.data.get(\"mode\")\n",
    "        if mode == ToolCallMode.EXTRACT and value is None:\n",
    "            raise ValueError(\"Query is required for EXTRACT mode to perform regex matching\")\n",
    "        if mode == ToolCallMode.DISCOVER and value is None:\n",
    "            return \"\"\n",
    "        if value:\n",
    "            try:\n",
    "                re.compile(value)\n",
    "            except re.error as e:\n",
    "                logger.error(f\"Invalid regex query: {value} ({str(e)})\")\n",
    "                raise ValueError(f\"Invalid regex query: {value} ({str(e)})\")\n",
    "        return value\n",
    "\n",
    "    @field_validator(\"file_pattern\")\n",
    "    @classmethod\n",
    "    def validate_file_pattern(cls, value: Any) -> Any:\n",
    "        if not isinstance(value, str):\n",
    "            raise ValueError(\"file_pattern must be a string\")\n",
    "        try:\n",
    "            for pattern in value.split('|'):\n",
    "                fnmatch.fnmatch(\"test.txt\", pattern)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Invalid glob pattern: {value} ({str(e)})\")\n",
    "            raise ValueError(f\"Invalid glob pattern: {value} ({str(e)})\")\n",
    "        return value\n",
    "\n",
    "class FsReadParams(BaseModel):\n",
    "    operations: List[FsReadOperation]\n",
    "\n",
    "class FsReadTool(BaseTool):\n",
    "    def __init__(self, log_level: str = \"INFO\"):\n",
    "        metadata = ToolMetadata(\n",
    "            name=\"fs_read\",\n",
    "            description=\"Read filesystem with regex search and exclusions, supporting file discovery or content extraction\",\n",
    "            category=ToolCategory.FILESYSTEM\n",
    "        )\n",
    "        super().__init__(metadata)\n",
    "        logging.getLogger().setLevel(getattr(logging, log_level, logging.INFO))\n",
    "        self.project_root = os.getcwd()\n",
    "        self.exclusion_patterns = [\n",
    "            \".*\", \"*.pyc\", \"*.o\", \"*.obj\", \"*.class\", \"*.exe\", \"*.dll\", \"*.so\",\n",
    "            \"*.lock\", \"node_modules/*\", \"dist/*\", \"build/*\", \"__pycache__/*\",\n",
    "            \"*.bin\", \"*.zip\", \"*.tar.gz\", \"*.log\"\n",
    "        ]\n",
    "        self._load_gitignore()\n",
    "\n",
    "    def _load_gitignore(self) -> None:\n",
    "        \"\"\"Load .gitignore patterns to exclude files.\"\"\"\n",
    "        gitignore_path = Path(self.project_root) / \".gitignore\"\n",
    "        try:\n",
    "            if gitignore_path.exists():\n",
    "                with open(gitignore_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip()\n",
    "                        if line and not line.startswith('#'):\n",
    "                            self.exclusion_patterns.append(line)\n",
    "            else:\n",
    "                logger.debug(f\".gitignore file does not exist at {gitignore_path}\")\n",
    "        except OSError as e:\n",
    "            logger.debug(f\"Failed to open .gitignore at {gitignore_path}: {type(e).__name__} - {str(e)}\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            logger.debug(f\"Failed to decode .gitignore at {gitignore_path}: {type(e).__name__} - {str(e)}\")\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Unexpected error reading .gitignore at {gitignore_path}: {type(e).__name__} - {str(e)}\")\n",
    "\n",
    "    def _is_excluded(self, rel_path: str) -> bool:\n",
    "        \"\"\"Check if a path matches exclusion patterns.\"\"\"\n",
    "        try:\n",
    "            return any(fnmatch.fnmatch(rel_path, p) for p in self.exclusion_patterns)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking exclusion for path {rel_path}: {type(e).__name__} - {str(e)}\")\n",
    "            return True  # Conservatively exclude on error\n",
    "\n",
    "    def _get_file_info(self, path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get metadata about a file, including size and binary status.\"\"\"\n",
    "        path_obj = Path(path)\n",
    "        try:\n",
    "            if not path_obj.exists():\n",
    "                return {\"error\": f\"File does not exist: {path}\"}\n",
    "            stat = path_obj.stat()\n",
    "        except OSError as e:\n",
    "            return {\"error\": f\"Failed to get file stats for {path}: {type(e).__name__} - {str(e)}\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Unexpected error getting stats for {path}: {type(e).__name__} - {str(e)}\"}\n",
    "\n",
    "        try:\n",
    "            rel_path = os.path.relpath(path, self.project_root)\n",
    "            if self._is_excluded(rel_path):\n",
    "                return {\"error\": f\"File is excluded by patterns: {path}\"}\n",
    "        except ValueError as e:\n",
    "            return {\"error\": f\"Failed to compute relative path for {path}: {type(e).__name__} - {str(e)}\"}\n",
    "\n",
    "        file_size = stat.st_size\n",
    "        is_binary = False\n",
    "        lines = 0\n",
    "\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                chunk = f.read(2048)\n",
    "            if b'\\0' in chunk:\n",
    "                is_binary = True\n",
    "            else:\n",
    "                chunk.decode('utf-8')\n",
    "                with open(path, 'r', encoding='utf-8', errors='ignore') as f_text:\n",
    "                    lines = sum(1 for _ in f_text)\n",
    "        except OSError as e:\n",
    "            return {\"error\": f\"Failed to open file {path}: {type(e).__name__} - {str(e)}\"}\n",
    "        except UnicodeDecodeError:\n",
    "            is_binary = True\n",
    "            lines = 0\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Unexpected error reading file {path}: {type(e).__name__} - {str(e)}\"}\n",
    "\n",
    "        return {\n",
    "            \"size\": file_size,\n",
    "            \"is_binary\": is_binary,\n",
    "            \"file_type\": path_obj.suffix.lower(),\n",
    "            \"is_large\": file_size > 1024 * 1024,\n",
    "            \"lines\": lines if not is_binary else 0\n",
    "        }\n",
    "\n",
    "    def _build_tree(self, rel_paths: List[str]) -> str:\n",
    "        \"\"\"Build a tree representation of file paths.\"\"\"\n",
    "        try:\n",
    "            tree = {}\n",
    "            for rel_path in rel_paths:\n",
    "                parts = rel_path.split(os.sep)\n",
    "                current = tree\n",
    "                for part in parts[:-1]:\n",
    "                    if part not in current:\n",
    "                        current[part] = {}\n",
    "                    current = current[part]\n",
    "                if parts:\n",
    "                    current[parts[-1]] = {}\n",
    "\n",
    "            def print_tree(node, prefix: str = \"\") -> List[str]:\n",
    "                lines = []\n",
    "                items = sorted(node.keys())\n",
    "                for i, item in enumerate(items):\n",
    "                    is_last = i == len(items) - 1\n",
    "                    connector = \"└── \" if is_last else \"├── \"\n",
    "                    lines.append(prefix + connector + item)\n",
    "                    sub_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "                    lines.extend(print_tree(node[item], sub_prefix))\n",
    "                return lines\n",
    "\n",
    "            tree_lines = print_tree(tree)\n",
    "            return \"\\n\".join(tree_lines) if tree_lines else \"No files found\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to build file tree: {type(e).__name__} - {str(e)}\")\n",
    "            return \"Error: Failed to build file tree structure\"\n",
    "\n",
    "    def _discover_files(self, path: str, file_pattern: str, query: str, max_depth: int, max_files: int, is_suggestion: bool = False) -> str:\n",
    "        \"\"\"Discover files matching patterns and query, returning a tree structure or suggestions.\"\"\"\n",
    "        start_time = time.time()\n",
    "        path_obj = Path(path)\n",
    "        suggestions = []\n",
    "\n",
    "        # Handle non-existent path by listing files in parent directory\n",
    "        try:\n",
    "            if not path_obj.exists():\n",
    "                parent_path = path_obj.parent if path_obj.parent.exists() else Path.cwd()\n",
    "                # Use broad file pattern for suggestions\n",
    "                suggestion_result = self._discover_files(\n",
    "                    str(parent_path), \"*\", \"\", max_depth=2, max_files=10, is_suggestion=True\n",
    "                )\n",
    "                try:\n",
    "                    suggestion_data = json.loads(suggestion_result)\n",
    "                    if suggestion_data.get(\"data\"):\n",
    "                        suggestions = suggestion_data[\"data\"][0].split(\"\\n\") if suggestion_data[\"data\"] else []\n",
    "                    message = f\"The path doesn't exist: {path}. The directory contains these files; check for potential candidates: {', '.join(suggestions[:10]) if suggestions else 'No files found'}\"\n",
    "                    return json.dumps({\n",
    "                        \"data\": [],\n",
    "                        \"message\": message,\n",
    "                        \"suggestions\": suggestions[:10]\n",
    "                    })\n",
    "                except json.JSONDecodeError:\n",
    "                    return json.dumps({\n",
    "                        \"data\": [],\n",
    "                        \"message\": f\"The path doesn't exist: {path}. Failed to generate suggestions.\"\n",
    "                    })\n",
    "            if not path_obj.is_dir():\n",
    "                return json.dumps({\n",
    "                    \"data\": [],\n",
    "                    \"message\": f\"The path is not a directory: {path}. Please provide a valid directory path.\"\n",
    "                })\n",
    "        except Exception as e:\n",
    "            return json.dumps({\n",
    "                \"data\": [],\n",
    "                \"message\": f\"Failed to check path: {type(e).__name__} - {str(e)}\"\n",
    "            })\n",
    "\n",
    "        try:\n",
    "            query_pattern = re.compile(query, re.IGNORECASE) if query else None\n",
    "        except re.error as e:\n",
    "            return json.dumps({\n",
    "                \"data\": [],\n",
    "                \"message\": f\"Invalid regex query '{query}': {str(e)}. Ensure the regex pattern is valid.\"\n",
    "            })\n",
    "\n",
    "        try:\n",
    "            patterns = [fnmatch.translate(p) for p in file_pattern.split('|')]\n",
    "            pattern_regexes = [re.compile(p) for p in patterns]\n",
    "        except Exception as e:\n",
    "            return json.dumps({\n",
    "                \"data\": [],\n",
    "                \"message\": f\"Failed to compile file patterns: {type(e).__name__} - {str(e)}. Check the file_pattern syntax.\"\n",
    "            })\n",
    "\n",
    "        candidates = []\n",
    "        errors = []\n",
    "\n",
    "        def _collect_with_walk(dir_path: str, current_depth: int, file_count: List[int]) -> None:\n",
    "            if current_depth > max_depth or file_count[0] >= max_files:\n",
    "                return\n",
    "            try:\n",
    "                for entry in os.scandir(dir_path):\n",
    "                    rel_path = entry.name\n",
    "                    try:\n",
    "                        rel_path = str(Path(entry.path).relative_to(self.project_root))\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                    if self._is_excluded(rel_path):\n",
    "                        continue\n",
    "                    if entry.is_file() and any(p.match(entry.name) for p in pattern_regexes):\n",
    "                        if query_pattern and not query_pattern.search(entry.name):\n",
    "                            continue\n",
    "                        candidates.append(entry.path)\n",
    "                        file_count[0] += 1\n",
    "                    if entry.is_dir():\n",
    "                        _collect_with_walk(entry.path, current_depth + 1, file_count)\n",
    "            except (OSError, PermissionError) as e:\n",
    "                errors.append(f\"Failed to scan directory {dir_path}: {type(e).__name__} - {str(e)}\")\n",
    "            except Exception as e:\n",
    "                errors.append(f\"Unexpected error scanning directory {dir_path}: {type(e).__name__} - {str(e)}\")\n",
    "\n",
    "        file_count = [0]\n",
    "        _collect_with_walk(str(path_obj), 0, file_count)\n",
    "\n",
    "        if not candidates:\n",
    "            # Generate suggestions by listing files in the directory\n",
    "            suggestion_result = self._discover_files(\n",
    "                path, \"*\", \"\", max_depth=2, max_files=10, is_suggestion=True\n",
    "            )\n",
    "            try:\n",
    "                suggestion_data = json.loads(suggestion_result)\n",
    "                suggestions = suggestion_data[\"data\"][0].split(\"\\n\") if suggestion_data[\"data\"] else []\n",
    "                message = f\"No files matched the criteria in directory {path}. The directory contains these files; check for potential candidates: {', '.join(suggestions[:10]) if suggestions else 'No files found'}\"\n",
    "                if errors:\n",
    "                    message += f\" Errors encountered: {'; '.join(errors)}\"\n",
    "                return json.dumps({\n",
    "                    \"data\": [],\n",
    "                    \"message\": message,\n",
    "                    \"suggestions\": suggestions[:10]\n",
    "                })\n",
    "            except json.JSONDecodeError:\n",
    "                return json.dumps({\n",
    "                    \"data\": [],\n",
    "                    \"message\": f\"No files matched the criteria in directory {path}. Failed to generate suggestions.\"\n",
    "                })\n",
    "\n",
    "        try:\n",
    "            candidates.sort()  # Sort alphabetically\n",
    "            selected_paths = candidates[:max_files]\n",
    "            rel_paths = [os.path.relpath(p, path) for p in selected_paths]\n",
    "            tree_str = self._build_tree(rel_paths)\n",
    "        except Exception as e:\n",
    "            return json.dumps({\n",
    "                \"data\": [],\n",
    "                \"message\": f\"Failed to process discovered files: {type(e).__name__} - {str(e)}. Ensure file paths are valid.\"\n",
    "            })\n",
    "\n",
    "        logger.info(f\"Discovered {len(selected_paths)} files in {time.time() - start_time:.2f}s\")\n",
    "        return json.dumps({\n",
    "            \"data\": [tree_str],\n",
    "            \"message\": \"Operation completed successfully\"\n",
    "        })\n",
    "\n",
    "    def _extract_content(self, path: str, query: str, file_pattern: str) -> str:\n",
    "        \"\"\"Extract content from files matching query and pattern.\"\"\"\n",
    "        path_obj = Path(path)\n",
    "        try:\n",
    "            if not path_obj.exists():\n",
    "                parent_path = path_obj.parent if path_obj.parent.exists() else Path.cwd()\n",
    "                suggestion_result = self._discover_files(\n",
    "                    str(parent_path), \"*\", \"\", max_depth=2, max_files=10, is_suggestion=True\n",
    "                )\n",
    "                try:\n",
    "                    suggestion_data = json.loads(suggestion_result)\n",
    "                    suggestions = suggestion_data[\"data\"][0].split(\"\\n\") if suggestion_data[\"data\"] else []\n",
    "                    message = f\"The path doesn't exist: {path}. The directory contains these files; check for potential candidates: {', '.join(suggestions[:10]) if suggestions else 'No files found'}\"\n",
    "                    return json.dumps({\n",
    "                        \"data\": [],\n",
    "                        \"message\": message,\n",
    "                        \"suggestions\": suggestions\n",
    "                    })\n",
    "                except json.JSONDecodeError:\n",
    "                    return json.dumps({\n",
    "                        \"data\": [],\n",
    "                        \"message\": f\"The path doesn't exist: {path}. Failed to generate suggestions.\"\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            return json.dumps({\n",
    "                \"data\": [],\n",
    "                \"message\": f\"Failed to check path existence: {type(e).__name__} - {str(e)}\"\n",
    "            })\n",
    "\n",
    "        if path_obj.is_file():\n",
    "            try:\n",
    "                rel_path = os.path.relpath(path, self.project_root)\n",
    "                if self._is_excluded(rel_path):\n",
    "                    return json.dumps({\n",
    "                        \"data\": [],\n",
    "                        \"message\": f\"The path is excluded by patterns: {path}. The directory contains these files; check for potential candidates: []\"\n",
    "                    })\n",
    "                content = self._extract_from_file(path, query)\n",
    "                if content == \"No matches found\":\n",
    "                    suggestion_result = self._discover_files(\n",
    "                        str(path_obj.parent), \"*\", \"\", max_depth=2, max_files=10, is_suggestion=True\n",
    "                    )\n",
    "                    try:\n",
    "                        suggestion_data = json.loads(suggestion_result)\n",
    "                        suggestions = suggestion_data[\"data\"][0].split(\"\\n\") if suggestion_data[\"data\"] else []\n",
    "                        message = f\"No matches found for query '{query}' in {path}. The directory contains these files; check for potential candidates: {', '.join(suggestions[:10]) if suggestions else 'No files found'}\"\n",
    "                        return json.dumps({\n",
    "                            \"data\": [],\n",
    "                            \"message\": message\n",
    "                        })\n",
    "                    except json.JSONDecodeError:\n",
    "                        return json.dumps({\n",
    "                            \"data\": [],\n",
    "                            \"message\": f\"No matches found for query '{query}' in {path}. Failed to generate suggestions.\"\n",
    "                        })\n",
    "                return json.dumps({\n",
    "                    \"data\": [{\"file\": rel_path, \"snippet\": content}],\n",
    "                    \"message\": \"Operation completed successfully\"\n",
    "                })\n",
    "            except ValueError as e:\n",
    "                return json.dumps({\n",
    "                    \"data\": [],\n",
    "                    \"message\": f\"Failed to compute relative path for {path}: {type(e).__name__} - {str(e)}\"\n",
    "                })\n",
    "            except Exception as e:\n",
    "                return json.dumps({\n",
    "                    \"data\": [],\n",
    "                    \"message\": f\"Failed to process file {path}: {type(e).__name__} - {str(e)}\"\n",
    "                })\n",
    "        else:\n",
    "            errors = []\n",
    "            file_errors = []\n",
    "            snippets = []\n",
    "\n",
    "            def onerror(e):\n",
    "                filename = getattr(e, 'filename', path)\n",
    "                errors.append(f\"Failed to scan directory {filename}: {type(e).__name__} - {str(e)}\")\n",
    "\n",
    "            try:\n",
    "                for root, dirs, files in os.walk(path, onerror=onerror):\n",
    "                    for file in files:\n",
    "                        if any(fnmatch.fnmatch(file, p) for p in file_pattern.split('|')):\n",
    "                            full_path = os.path.join(root, file)\n",
    "                            try:\n",
    "                                rel_path = os.path.relpath(full_path, self.project_root)\n",
    "                                if not self._is_excluded(rel_path):\n",
    "                                    file_snip = self._extract_from_file(full_path, query)\n",
    "                                    if file_snip.startswith(\"Error\"):\n",
    "                                        file_errors.append(f\"{file_snip} in file {rel_path}\")\n",
    "                                    elif file_snip.startswith(\"[Binary\"):\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        if file_snip and file_snip != \"No matches found\":\n",
    "                                            snippets.append({\"file\": rel_path, \"snippet\": file_snip[:1000]})\n",
    "                            except ValueError as e:\n",
    "                                file_errors.append(f\"Failed to compute relative path for {full_path}: {type(e).__name__} - {str(e)}\")\n",
    "                            except Exception as e:\n",
    "                                file_errors.append(f\"Unexpected error processing file {full_path}: {type(e).__name__} - {str(e)}\")\n",
    "            except Exception as e:\n",
    "                return json.dumps({\n",
    "                    \"data\": [],\n",
    "                    \"message\": f\"Failed to traverse directory {path}: {type(e).__name__} - {str(e)}\"\n",
    "                })\n",
    "\n",
    "            if not snippets:\n",
    "                suggestion_result = self._discover_files(\n",
    "                    path, \"*\", \"\", max_depth=2, max_files=10, is_suggestion=True\n",
    "                )\n",
    "                try:\n",
    "                    suggestion_data = json.loads(suggestion_result)\n",
    "                    suggestions = suggestion_data[\"data\"][0].split(\"\\n\") if suggestion_data[\"data\"] else []\n",
    "                    message = f\"No matches found for query '{query}' in directory {path}. The directory contains these files; check for potential candidates: {', '.join(suggestions[:10]) if suggestions else 'No files found'}\"\n",
    "                    if file_errors:\n",
    "                        message += f\" File errors: {'; '.join(file_errors[:3])}\"\n",
    "                    if errors:\n",
    "                        message += f\" Directory scan errors: {'; '.join(errors)}\"\n",
    "                    return json.dumps({\n",
    "                        \"data\": [],\n",
    "                        \"message\": message\n",
    "                    })\n",
    "                except json.JSONDecodeError:\n",
    "                    return json.dumps({\n",
    "                        \"data\": [],\n",
    "                        \"message\": f\"No matches found for query '{query}' in directory {path}. Failed to generate suggestions.\"\n",
    "                    })\n",
    "\n",
    "            try:\n",
    "                content = [{\"file\": s[\"file\"], \"snippet\": s[\"snippet\"]} for s in snippets]\n",
    "                if len(json.dumps(content)) > 16 * 1024:\n",
    "                    content = content[:10]\n",
    "                    message = \"Operation completed successfully, but output was truncated due to size limits.\"\n",
    "                else:\n",
    "                    message = \"Operation completed successfully\"\n",
    "                if file_errors:\n",
    "                    message += f\" File errors: {'; '.join(file_errors[:3])}\"\n",
    "                if errors:\n",
    "                    message += f\" Directory scan errors: {'; '.join(errors)}\"\n",
    "                return json.dumps({\n",
    "                    \"data\": content,\n",
    "                    \"message\": message\n",
    "                }, ensure_ascii=False)\n",
    "            except Exception as e:\n",
    "                return json.dumps({\n",
    "                    \"data\": [],\n",
    "                    \"message\": f\"Failed to serialize extracted content to JSON: {type(e).__name__} - {str(e)}\"\n",
    "                })\n",
    "\n",
    "    def _extract_from_file(self, file_path: str, query: str) -> str:\n",
    "        \"\"\"Extract content from a single file with regex matching.\"\"\"\n",
    "        file_info = self._get_file_info(file_path)\n",
    "        if \"error\" in file_info:\n",
    "            return f\"Error: {file_info['error']}\"\n",
    "        if file_info[\"is_binary\"]:\n",
    "            return f\"[Binary file: {file_info['size']} bytes]\"\n",
    "\n",
    "        try:\n",
    "            pattern = re.compile(query, re.IGNORECASE | re.MULTILINE) if query else None\n",
    "        except re.error as e:\n",
    "            return f\"Error: Invalid regex pattern in query for file {file_path}: {type(e).__name__} - {str(e)}\"\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                lines = f.readlines()\n",
    "        except OSError as e:\n",
    "            return f\"Error reading file {file_path}: {type(e).__name__} - {str(e)}\"\n",
    "        except UnicodeDecodeError as e:\n",
    "            return f\"Error decoding file {file_path}: {type(e).__name__} - {str(e)}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: Unexpected error reading file {file_path}: {type(e).__name__} - {str(e)}\"\n",
    "\n",
    "        try:\n",
    "            if not query:\n",
    "                content = \"\".join(lines)\n",
    "                if len(content) > 1024:\n",
    "                    content = content[:1024] + \"... [truncated]\"\n",
    "                return content + f\"\\n--- File Info: {file_info['lines']} lines, {file_info['size']} bytes ---\"\n",
    "            matches = []\n",
    "            for line_num, line in enumerate(lines, 1):\n",
    "                if pattern and pattern.search(line):\n",
    "                    clean_line = line.strip().replace('\"', '\\\\\"').replace('\\n', '\\\\n').replace('\\r', '\\\\r')\n",
    "                    matches.append(f\"Line {line_num}: {clean_line}\")\n",
    "                    if len(matches) >= 20:\n",
    "                        break\n",
    "            if not matches:\n",
    "                return \"No matches found\"\n",
    "            content = \"\\n\".join(matches)\n",
    "            if len(content) > 16 * 1024:\n",
    "                content = content[:16 * 1024] + \"... [truncated]\"\n",
    "            return content + f\"\\n--- File Info: {file_info['lines']} lines, {file_info['size']} bytes ---\"\n",
    "        except Exception as e:\n",
    "            return f\"Error processing file content {file_path}: {type(e).__name__} - {str(e)}\"\n",
    "\n",
    "    def get_parameters_schema(self, verbose: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Return OpenAI-compatible schema for the tool.\"\"\"\n",
    "        try:\n",
    "            schema = {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"fs_read\",\n",
    "                    \"description\": \"Discover files in tree structure or extract regex-matched snippets. Uses exclusions with fallback sorting.\" if verbose else \"\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"operations\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"mode\": {\n",
    "                                            \"type\": \"string\",\n",
    "                                            \"enum\": [mode.value for mode in ToolCallMode],\n",
    "                                            \"description\": \"Select 'discover' to list files in tree or 'extract' to pull snippets.\" if verbose else \"\"\n",
    "                                        },\n",
    "                                        \"path\": {\n",
    "                                            \"type\": \"string\",\n",
    "                                            \"description\": \"File or directory path relative to project root, e.g., 'src' or 'app.py'.\" if verbose else \"\"\n",
    "                                        },\n",
    "                                        \"query\": {\n",
    "                                            \"type\": \"string\",\n",
    "                                            \"description\": \"Regex for fuzzy file name (DISCOVER) or content (EXTRACT). Required for EXTRACT.\" if verbose else \"\"\n",
    "                                        },\n",
    "                                        \"file_pattern\": {\n",
    "                                            \"type\": \"string\",\n",
    "                                            \"description\": \"Glob filter, e.g., '*.py|*.ipynb' or '*.go|*.rs'.\" if verbose else \"\"\n",
    "                                        },\n",
    "                                        \"max_depth\": {\n",
    "                                            \"type\": \"integer\",\n",
    "                                            \"description\": \"Maximum recursion depth for DISCOVER mode (default: 10).\" if verbose else \"\"\n",
    "                                        },\n",
    "                                        \"max_files\": {\n",
    "                                            \"type\": \"integer\",\n",
    "                                            \"description\": \"Maximum number of files to return in DISCOVER mode (default: 50).\" if verbose else \"\"\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"required\": [\"mode\", \"path\"]\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"operations\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            return schema[\"function\"][\"parameters\"]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to generate parameters schema: {type(e).__name__} - {str(e)}\")\n",
    "            return {\"error\": f\"Failed to generate parameters schema: {type(e).__name__} - {str(e)}\"}\n",
    "\n",
    "    def execute(self, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Execute filesystem read operations with robust error handling.\"\"\"\n",
    "        try:\n",
    "            if 'params' in kwargs:\n",
    "                params = kwargs['params']\n",
    "            else:\n",
    "                if 'operations' not in kwargs and ('mode' in kwargs or 'path' in kwargs):\n",
    "                    kwargs = {'operations': [kwargs]}\n",
    "                params = FsReadParams(**kwargs)\n",
    "        except ValidationError as e:\n",
    "            error_msg = f\"Invalid parameters: {extract_validation_error(e)}\"\n",
    "            return create_error_response(error_msg)\n",
    "        except Exception as e:\n",
    "            return create_error_response(f\"Parameter processing failed: {type(e).__name__} - {str(e)}\")\n",
    "\n",
    "        try:\n",
    "            result = self._execute_internal(params)\n",
    "            if len(params.operations) == 1 and result[\"success\"]:\n",
    "                op_result = result[\"data\"][0]\n",
    "                try:\n",
    "                    json_data = json.loads(op_result[\"data\"]) if op_result[\"data\"] else {\"data\": [], \"message\": \"Operation completed successfully\"}\n",
    "                    data = json_data.get(\"data\", [])\n",
    "                    message = json_data.get(\"message\", \"Operation completed successfully\")\n",
    "                    suggestions = json_data.get(\"suggestions\", [])\n",
    "                except json.JSONDecodeError as e:\n",
    "                    return create_error_response(f\"Failed to parse operation result: {type(e).__name__} - {str(e)}\")\n",
    "                return create_success_response(\n",
    "                    message=message,\n",
    "                    data=data,\n",
    "                    processed_files=len(result[\"data\"]),\n",
    "                    suggestions=suggestions if suggestions else None\n",
    "                )\n",
    "            elif not result[\"success\"]:\n",
    "                op_result = result[\"data\"][0] if result[\"data\"] else {}\n",
    "                error_data = op_result.get(\"error\", {})\n",
    "                message = error_data.get(\"message\", \"Operation failed\") if isinstance(error_data, dict) else str(error_data)\n",
    "                try:\n",
    "                    json_data = json.loads(op_result.get(\"data\", \"{}\"))\n",
    "                    suggestions = json_data.get(\"suggestions\", [])\n",
    "                except (json.JSONDecodeError, KeyError):\n",
    "                    suggestions = []\n",
    "                return create_error_response(\n",
    "                    message=message,\n",
    "                    suggestions=suggestions if suggestions else None\n",
    "                )\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return create_error_response(f\"Execution failed: {type(e).__name__} - {str(e)}\")\n",
    "\n",
    "    def _execute_internal(self, params: FsReadParams) -> Dict[str, Any]:\n",
    "        \"\"\"Internal execution logic for batch operations.\"\"\"\n",
    "        results = []\n",
    "        total_processed = 0\n",
    "\n",
    "        for op in params.operations:\n",
    "            mode = op.mode.value\n",
    "            path = op.path\n",
    "            output_data = None\n",
    "            error = None\n",
    "\n",
    "            try:\n",
    "                if mode == \"discover\":\n",
    "                    output_data = self._discover_files(path, op.file_pattern, op.query or \"\", op.max_depth, op.max_files)\n",
    "                    total_processed += op.max_files\n",
    "                elif mode == \"extract\":\n",
    "                    output_data = self._extract_content(path, op.query, op.file_pattern)\n",
    "                    total_processed += 1\n",
    "            except ValueError as e:\n",
    "                error = {\n",
    "                    \"type\": \"PermissionError\" if \"outside project root\" in str(e) else \"ValueError\",\n",
    "                    \"message\": str(e)\n",
    "                }\n",
    "                logger.error(f\"Error in {mode} on {path}: {str(e)}\")\n",
    "            except (OSError, PermissionError) as e:\n",
    "                error = {\"type\": type(e).__name__, \"message\": str(e)}\n",
    "                logger.error(f\"OS error in {mode} on {path}: {str(e)}\")\n",
    "            except UnicodeDecodeError as e:\n",
    "                error = {\"type\": \"UnicodeDecodeError\", \"message\": str(e)}\n",
    "                logger.error(f\"Decode error in {mode} on {path}: {str(e)}\")\n",
    "            except re.error as e:\n",
    "                error = {\"type\": \"RegexError\", \"message\": str(e)}\n",
    "                logger.error(f\"Regex error in {mode} on {path}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                error = {\"type\": type(e).__name__, \"message\": str(e)}\n",
    "                logger.error(f\"Unexpected error in {mode} on {path}: {type(e).__name__} - {str(e)}\")\n",
    "\n",
    "            results.append({\n",
    "                \"mode\": mode,\n",
    "                \"path\": path,\n",
    "                \"data\": output_data,\n",
    "                \"error\": error\n",
    "            })\n",
    "\n",
    "        overall_success = all(r.get(\"error\") is None for r in results)\n",
    "        overall_error = None if overall_success else {\n",
    "            \"type\": \"BatchError\",\n",
    "            \"message\": \"Some operations failed—check individual errors for details\"\n",
    "        }\n",
    "        return {\n",
    "            \"success\": overall_success,\n",
    "            \"data\": results,\n",
    "            \"error\": overall_error,\n",
    "            \"metadata\": {\"processed_files\": total_processed}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a97086-235d-46f5-a102-2ef99f1b8d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
