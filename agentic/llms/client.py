# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/buddy/backend/llms/client.ipynb.

# %% auto 0
__all__ = ['LLMClient']

# %% ../../nbs/buddy/backend/llms/client.ipynb 1
import os
from typing import Dict, Any, List, Optional, Iterator
from openai import OpenAI
from rich.console import Console
from rich.markdown import Markdown

# %% ../../nbs/buddy/backend/llms/client.ipynb 2
from ..configs.loader import get_model_config, get_settings_config
from .response_processor import ResponseProcessor
from .streaming_handler import StreamingHandler

# %% ../../nbs/buddy/backend/llms/client.ipynb 3
class LLMClient:
    """Enhanced LLM client"""
    
    def __init__(self, model: Optional[str] = None, base_url: Optional[str] = None, 
                 api_key: Optional[str] = None):
        # Load config defaults
        model_config = get_model_config()
        settings_config = get_settings_config()
        
        # Set parameters with fallbacks
        self.model = model or model_config.get('name', 'qwen3:8b')
        self.base_url = base_url or model_config.get('url', 'http://localhost:11434/v1')
        self.api_key = api_key or model_config.get('api_key', 'ollama')

        
        # Initialize OpenAI client
        self.client = OpenAI(base_url=self.base_url, 
                             api_key=self.api_key)
        
        # Initialize processors
        self.response_processor = ResponseProcessor()
        self.streaming_handler = StreamingHandler()
        
        # Validate connection
        # self._validate_connection()
    
    def _validate_connection(self):
        """Validate LLM connection"""
        try:
            # Test with a simple request
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=1,
                timeout=5
            )
            return True
        except Exception as e:
            print(f"⚠️ LLM connection warning: {e}")
            return False
    
    def create_completion(self, messages: List[Dict[str, Any]], 
                         tools: Optional[List[Dict]] = None,
                         stream: bool = True, **kwargs) -> Any:
        """Create chat completion with optional tools"""
        completion_params = {
            "model": self.model,
            "messages": messages,
            "stream": stream,
            **kwargs
        }
        
        if tools:
            completion_params["tools"] = tools
            completion_params["tool_choice"] = "auto"
        
        try:
            
            # import mlflow
            
            # # Enable auto-tracing for OpenAI
            # mlflow.openai.autolog()
            
            # # Optional: Set a tracking URI and an experiment
            # mlflow.set_tracking_uri("http://localhost:5000")
            # mlflow.set_experiment("tool_test")
            # with mlflow.start_run():
            return self.client.chat.completions.create(**completion_params)
        except Exception as e:
            raise RuntimeError(f"LLM completion failed: {e}")
    
    def process_response(self, response: Any, console: Optional[Console] = None) -> Dict[str, Any]:
        """Process non-streaming response"""
        return self.response_processor.process_response(response, console)
    
    def handle_streaming_response(self, response: Iterator, console: Optional[Console] = None) -> Dict[str, Any]:
        """Handle streaming response"""
        return self.streaming_handler.handle_streaming_response(response, console)
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the current model"""
        return {
            "model": self.model,
            "base_url": self.base_url,
            "api_key_set": bool(self.api_key),
            "connection_valid": self._validate_connection()
        }

