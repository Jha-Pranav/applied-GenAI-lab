{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c875a2ae-425e-4817-990b-d89e49d38d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp tools.introspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fad3da-d351-47c8-aeac-31e3c34e8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from agentic.tools.base import BaseTool\n",
    "from agentic.schemas import IntrospectParams, IntrospectAction, CritiqueResponse\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "class IntrospectTool(BaseTool):\n",
    "    def get_parameters_schema(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return the OpenAI-compatible schema for introspect.\"\"\"\n",
    "        return {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"introspect\",\n",
    "                    \"description\": \"self-reflection and criticism tool. Analyzes actions, provides feedback, and validates decisions with constructive criticism.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"action\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"critique\", \"validate\", \"reflect\", \"improve\"],\n",
    "                                \"description\": \"Type of introspection: critique (analyze action), validate (check decision), reflect (self-assessment), improve (suggest enhancements)\"\n",
    "                            },\n",
    "                            \"context\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"description\": \"Context about the action/decision being analyzed\",\n",
    "                                \"properties\": {\n",
    "                                    \"tool_used\": {\"type\": \"string\"},\n",
    "                                    \"command\": {\"type\": \"string\"},\n",
    "                                    \"result\": {\"type\": \"object\"},\n",
    "                                    \"user_request\": {\"type\": \"string\"},\n",
    "                                    \"decision_type\": {\"type\": \"string\"}\n",
    "                                }\n",
    "                            },\n",
    "                            \"focus_areas\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\"type\": \"string\"},\n",
    "                                \"description\": \"Specific areas to focus criticism on: efficiency, safety, completeness, best_practices, alternatives\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"action\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "    def execute(self, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Execute introspection operations.\"\"\"\n",
    "        print(f\"üîç IntrospectTool.execute called with kwargs: {kwargs}\")\n",
    "        \n",
    "        try:\n",
    "            # Handle task_review action (used by planner)\n",
    "            if kwargs.get('action') == 'task_review':\n",
    "                print(f\"üìã Processing task_review action\")\n",
    "                task_review = kwargs.get('task_review', {})\n",
    "                \n",
    "                # Extract key information\n",
    "                task_id = task_review.get('task_id', 'unknown')\n",
    "                task_description = task_review.get('task_description', '')\n",
    "                solution_provided = task_review.get('solution_provided', '')\n",
    "                success_criteria = task_review.get('success_criteria', [])\n",
    "                \n",
    "                print(f\"üìä Task ID: {task_id}\")\n",
    "                print(f\"üìä Task Description: {task_description}\")\n",
    "                print(f\"üìä Solution length: {len(str(solution_provided))} chars\")\n",
    "                print(f\"üìä Success criteria: {success_criteria}\")\n",
    "                \n",
    "                # Simple validation logic\n",
    "                if solution_provided and len(str(solution_provided)) > 10:\n",
    "                    score = 8.0\n",
    "                    success = True\n",
    "                    feedback = \"Task appears to be completed successfully\"\n",
    "                    next_action = \"proceed\"\n",
    "                else:\n",
    "                    score = 3.0\n",
    "                    success = False\n",
    "                    feedback = \"Solution appears incomplete or empty\"\n",
    "                    next_action = \"retry\"\n",
    "                \n",
    "                result = {\n",
    "                    \"performance_score\": score,\n",
    "                    \"success\": success,\n",
    "                    \"feedback_for_retry\": feedback,\n",
    "                    \"next_action\": next_action,\n",
    "                    \"recommendations\": [\"Ensure solution is complete and detailed\"]\n",
    "                }\n",
    "                \n",
    "                print(f\"‚úÖ Task review result: {result}\")\n",
    "                return result\n",
    "            \n",
    "            # Handle validation_context action (used by pre-execution validation)\n",
    "            elif kwargs.get('action') == 'validate' and 'validation_context' in kwargs:\n",
    "                print(f\"üìã Processing validation_context action\")\n",
    "                context = kwargs.get('validation_context', {})\n",
    "                \n",
    "                # Simple validation\n",
    "                proposed_task = context.get('proposed_task', '')\n",
    "                if proposed_task:\n",
    "                    result = {\n",
    "                        \"performance_score\": 8.0,\n",
    "                        \"success\": True,\n",
    "                        \"feedback_for_retry\": \"Task validation passed\",\n",
    "                        \"next_action\": \"proceed\",\n",
    "                        \"recommendations\": []\n",
    "                    }\n",
    "                else:\n",
    "                    result = {\n",
    "                        \"performance_score\": 3.0,\n",
    "                        \"success\": False,\n",
    "                        \"feedback_for_retry\": \"Task validation failed - no task name provided\",\n",
    "                        \"next_action\": \"regenerate\",\n",
    "                        \"recommendations\": [\"Provide clear task name\"]\n",
    "                    }\n",
    "                \n",
    "                print(f\"‚úÖ Validation result: {result}\")\n",
    "                return result\n",
    "            \n",
    "            # Original introspect logic\n",
    "            params = IntrospectParams(**kwargs)\n",
    "            print(f\"üìã Processing standard introspect action: {params.action}\")\n",
    "            \n",
    "            if params.action == IntrospectAction.CRITIQUE:\n",
    "                # Simplified critique (replace with LLM-based analysis)\n",
    "                return CritiqueResponse(\n",
    "                    overall_assessment=\"Placeholder assessment\",\n",
    "                    strengths=[\"Strength 1\"],\n",
    "                    weaknesses=[\"Weakness 1\"],\n",
    "                    risks=[\"Risk 1\"],\n",
    "                    suggestions=[\"Suggestion 1\"],\n",
    "                    score=8\n",
    "                ).dict()\n",
    "            \n",
    "            elif params.action == IntrospectAction.VALIDATE:\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"is_valid\": True,\n",
    "                    \"message\": \"Context validated\",\n",
    "                    \"details\": params.context or {}\n",
    "                }\n",
    "            \n",
    "            elif params.action == IntrospectAction.REFLECT:\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"reflection\": f\"Reflected on context: {params.context}\",\n",
    "                    \"focus_areas\": params.focus_areas or []\n",
    "                }\n",
    "            \n",
    "            elif params.action == IntrospectAction.IMPROVE:\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"improvements\": [\"Placeholder improvement\"],\n",
    "                    \"message\": \"Generated improvements for context\"\n",
    "                }\n",
    "            \n",
    "            print(f\"‚ùå Unknown action: {params.action}\")\n",
    "            return {\"error\": f\"Unknown action: {params.action}\"}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Introspection failed with error: {str(e)}\")\n",
    "            import traceback\n",
    "            print(f\"üìã Full traceback: {traceback.format_exc()}\")\n",
    "            return {\"error\": f\"Introspection failed: {str(e)}\"}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
