{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a18f208-8954-4853-9b94-a784899063ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp backend.tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebea63d0-764f-4e29-a131-9b8a355285f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from typing import Dict, List, Any, Optional\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4aefbb9-021c-4a4e-82a7-5671ff58b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class TokenManager:\n",
    "    def __init__(self, model_name: str = \"gpt-4\", max_tokens: int = 128000):\n",
    "        self.model_name = model_name\n",
    "        self.max_tokens = max_tokens\n",
    "        self.compression_threshold = 0.8  # 80%\n",
    "        try:\n",
    "            self.encoding = tiktoken.encoding_for_model(model_name)\n",
    "        except:\n",
    "            self.encoding = tiktoken.get_encoding(\"cl100k_base\")  # fallback\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def count_conversation_tokens(self, messages: List[Dict]) -> int:\n",
    "        \"\"\"Count total tokens in conversation history\"\"\"\n",
    "        total = 0\n",
    "        for msg in messages:\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            if isinstance(content, str):\n",
    "                total += self.count_tokens(content)\n",
    "            # Add overhead for message structure\n",
    "            total += 10\n",
    "        return total\n",
    "    \n",
    "    def should_compress(self, messages: List[Dict]) -> bool:\n",
    "        \"\"\"Check if compression is needed\"\"\"\n",
    "        current_tokens = self.count_conversation_tokens(messages)\n",
    "        return current_tokens >= (self.max_tokens * self.compression_threshold)\n",
    "    \n",
    "    def compress_history(self, messages: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Compress conversation history while preserving context\"\"\"\n",
    "        if len(messages) <= 2:  # Keep system + at least 1 message\n",
    "            return messages\n",
    "        \n",
    "        system_msg = messages[0] if messages[0][\"role\"] == \"system\" else None\n",
    "        recent_messages = messages[-4:]  # Keep last 4 messages\n",
    "        \n",
    "        # Compress middle messages into summary\n",
    "        middle_messages = messages[1:-4] if len(messages) > 5 else []\n",
    "        \n",
    "        compressed = []\n",
    "        if system_msg:\n",
    "            compressed.append(system_msg)\n",
    "        \n",
    "        if middle_messages:\n",
    "            summary = self._create_summary(middle_messages)\n",
    "            compressed.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"[COMPRESSED HISTORY - {len(middle_messages)} messages summarized]\\n{summary}\"\n",
    "            })\n",
    "        \n",
    "        compressed.extend(recent_messages)\n",
    "        return compressed\n",
    "    \n",
    "    def _create_summary(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"Create concise summary of conversation history\"\"\"\n",
    "        topics = []\n",
    "        actions = []\n",
    "        \n",
    "        for msg in messages:\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            role = msg.get(\"role\", \"\")\n",
    "            \n",
    "            if role == \"user\":\n",
    "                # Extract key topics/requests\n",
    "                if any(word in content.lower() for word in [\"create\", \"build\", \"make\"]):\n",
    "                    topics.append(\"Creation tasks\")\n",
    "                if any(word in content.lower() for word in [\"fix\", \"debug\", \"error\"]):\n",
    "                    topics.append(\"Debugging\")\n",
    "                if any(word in content.lower() for word in [\"analyze\", \"review\", \"check\"]):\n",
    "                    topics.append(\"Analysis\")\n",
    "                if any(word in content.lower() for word in [\"deploy\", \"install\", \"setup\"]):\n",
    "                    topics.append(\"Deployment\")\n",
    "            \n",
    "            elif role == \"assistant\":\n",
    "                # Extract actions taken\n",
    "                if \"fs_read\" in content:\n",
    "                    actions.append(\"File reading\")\n",
    "                if \"fs_write\" in content:\n",
    "                    actions.append(\"File editing\")\n",
    "                if \"execute_bash\" in content:\n",
    "                    actions.append(\"Command execution\")\n",
    "        \n",
    "        summary_parts = []\n",
    "        if topics:\n",
    "            summary_parts.append(f\"Topics: {', '.join(set(topics))}\")\n",
    "        if actions:\n",
    "            summary_parts.append(f\"Actions: {', '.join(set(actions))}\")\n",
    "        \n",
    "        return \"; \".join(summary_parts) if summary_parts else \"General conversation\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f2df5f3-c969-49c0-8908-86dd48837ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ToolManager:\n",
    "    def __init__(self):\n",
    "        self.tools = {\n",
    "            \"fs_read\": self._fs_read,\n",
    "            \"fs_write\": self._fs_write,\n",
    "            \"execute_bash\": self._execute_bash,\n",
    "            \"introspect\": self._introspect,\n",
    "            \"todo\": self._todo,\n",
    "            \"memory_manager\": self._memory_manager,\n",
    "            \"debate_agent\": self._debate_agent,\n",
    "            \"code_interpreter\": self._code_interpreter,\n",
    "            \"repo_quality_analyzer\": self._repo_quality_analyzer,\n",
    "            \"documentation_generator\": self._documentation_generator\n",
    "        }\n",
    "        self._task_plans = {}\n",
    "        self.token_manager = TokenManager(model_name, max_tokens)\n",
    "\n",
    "    def check_and_compress_history(self, messages: List[Dict]) -> tuple[List[Dict], bool]:\n",
    "        \"\"\"Check token budget and compress if needed\"\"\"\n",
    "        if self.token_manager.should_compress(messages):\n",
    "            compressed = self.token_manager.compress_history(messages)\n",
    "            return compressed, True\n",
    "        return messages, False\n",
    "    \n",
    "    def get_token_info(self, messages: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"Get current token usage information\"\"\"\n",
    "        current_tokens = self.token_manager.count_conversation_tokens(messages)\n",
    "        return {\n",
    "            \"current_tokens\": current_tokens,\n",
    "            \"max_tokens\": self.token_manager.max_tokens,\n",
    "            \"usage_percent\": round((current_tokens / self.token_manager.max_tokens) * 100, 1),\n",
    "            \"compression_threshold\": round(self.token_manager.compression_threshold * 100),\n",
    "            \"tokens_until_compression\": max(0, int(self.token_manager.max_tokens * self.token_manager.compression_threshold) - current_tokens),\n",
    "            \"needs_compression\": self.token_manager.should_compress(messages)\n",
    "        }\n",
    "\n",
    "    def get_tools(self, requested_tools: List[str]) -> List[Dict]:\n",
    "        \"\"\"Convert tools to function calling format\"\"\"\n",
    "        available_tools = []\n",
    "        \n",
    "        if \"fs_read\" in requested_tools:\n",
    "            available_tools.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"fs_read\",\n",
    "                    \"description\": \"Enhanced file system operations: read files with intelligent chunking, list directories with depth, search patterns, find files, grep across files, and generate tree structures. Supports batch operations.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"operations\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"mode\": {\n",
    "                                            \"type\": \"string\", \n",
    "                                            \"enum\": [\"Line\", \"Directory\", \"Search\", \"Find\", \"Grep\", \"Tree\"],\n",
    "                                            \"description\": \"Operation mode: Line (read file), Directory (list with depth), Search (pattern in file), Find (find files by name), Grep (search across files), Tree (directory tree)\"\n",
    "                                        },\n",
    "                                        \"path\": {\"type\": \"string\", \"description\": \"File or directory path\"},\n",
    "                                        \"pattern\": {\"type\": \"string\", \"description\": \"Search pattern for Search/Grep modes\"},\n",
    "                                        \"name_pattern\": {\"type\": \"string\", \"description\": \"File name pattern for Find mode (supports wildcards)\"},\n",
    "                                        \"file_pattern\": {\"type\": \"string\", \"description\": \"File pattern for Grep mode (e.g., '*.py')\"},\n",
    "                                        \"depth\": {\"type\": \"integer\", \"description\": \"Directory depth for Directory mode\"},\n",
    "                                        \"max_depth\": {\"type\": \"integer\", \"description\": \"Maximum depth for Find/Tree modes\"},\n",
    "                                        \"context_lines\": {\"type\": \"integer\", \"description\": \"Context lines around matches\"},\n",
    "                                        \"start_line\": {\"type\": \"integer\", \"description\": \"Start line for Line mode\"},\n",
    "                                        \"end_line\": {\"type\": \"integer\", \"description\": \"End line for Line mode (-1 for end of file)\"},\n",
    "                                        \"show_hidden\": {\"type\": \"boolean\", \"description\": \"Show hidden files/directories\"},\n",
    "                                        \"case_sensitive\": {\"type\": \"boolean\", \"description\": \"Case sensitive search\"},\n",
    "                                        \"recursive\": {\"type\": \"boolean\", \"description\": \"Recursive search for Grep mode\"},\n",
    "                                        \"file_type\": {\"type\": \"string\", \"enum\": [\"file\", \"dir\", \"all\"], \"description\": \"File type filter for Find mode\"},\n",
    "                                        \"smart_chunk\": {\"type\": \"boolean\", \"description\": \"Use intelligent chunking for large files\"}\n",
    "                                    },\n",
    "                                    \"required\": [\"mode\", \"path\"]\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"operations\"]\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if \"fs_write\" in requested_tools:\n",
    "            available_tools.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"fs_write\",\n",
    "                    \"description\": \"Enhanced file operations: create, edit, modify files with diff preview, insert at specific lines, and batch operations. Shows changes in git diff format.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"command\": {\n",
    "                                \"type\": \"string\", \n",
    "                                \"enum\": [\"create\", \"str_replace\", \"append\", \"insert\", \"prepend\", \"delete_lines\"],\n",
    "                                \"description\": \"Operation: create (new file), str_replace (replace text), append (add to end), insert (add at line), prepend (add to start), delete_lines (remove lines)\"\n",
    "                            },\n",
    "                            \"path\": {\"type\": \"string\", \"description\": \"File path\"},\n",
    "                            \"file_text\": {\"type\": \"string\", \"description\": \"Content for create command\"},\n",
    "                            \"old_str\": {\"type\": \"string\", \"description\": \"Text to replace (exact match required)\"},\n",
    "                            \"new_str\": {\"type\": \"string\", \"description\": \"Replacement text\"},\n",
    "                            \"insert_line\": {\"type\": \"integer\", \"description\": \"Line number to insert at (1-based)\"},\n",
    "                            \"start_line\": {\"type\": \"integer\", \"description\": \"Start line for delete_lines\"},\n",
    "                            \"end_line\": {\"type\": \"integer\", \"description\": \"End line for delete_lines\"},\n",
    "                            \"create_backup\": {\"type\": \"boolean\", \"description\": \"Create backup before editing\"},\n",
    "                            \"show_diff\": {\"type\": \"boolean\", \"description\": \"Show diff of changes\"},\n",
    "                            \"summary\": {\"type\": \"string\", \"description\": \"Brief description of the change\"}\n",
    "                        },\n",
    "                        \"required\": [\"command\", \"path\"]\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "\n",
    "\n",
    "        if \"execute_bash\" in requested_tools:\n",
    "            available_tools.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"execute_bash\",\n",
    "                    \"description\": \"Enhanced bash execution with working directory control, environment variables, timeout management, and detailed output formatting.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"command\": {\"type\": \"string\", \"description\": \"Bash command to execute\"},\n",
    "                            \"working_dir\": {\"type\": \"string\", \"description\": \"Working directory for command execution\"},\n",
    "                            \"timeout\": {\"type\": \"integer\", \"description\": \"Timeout in seconds (default: 30)\"},\n",
    "                            \"env_vars\": {\"type\": \"object\", \"description\": \"Additional environment variables\"},\n",
    "                            \"capture_output\": {\"type\": \"boolean\", \"description\": \"Capture stdout/stderr (default: true)\"},\n",
    "                            \"shell\": {\"type\": \"string\", \"description\": \"Shell to use (default: /bin/bash)\"},\n",
    "                            \"summary\": {\"type\": \"string\", \"description\": \"Brief description of what the command does\"}\n",
    "                        },\n",
    "                        \"required\": [\"command\"]\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "\n",
    "        if \"introspect\" in requested_tools:\n",
    "            available_tools.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"introspect\",\n",
    "                    \"description\": \"Enhanced self-reflection and criticism tool. Analyzes actions, provides feedback, and validates decisions with constructive criticism.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"action\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"critique\", \"validate\", \"reflect\", \"improve\"],\n",
    "                                \"description\": \"Type of introspection: critique (analyze action), validate (check decision), reflect (self-assessment), improve (suggest enhancements)\"\n",
    "                            },\n",
    "                            \"context\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"description\": \"Context about the action/decision being analyzed\",\n",
    "                                \"properties\": {\n",
    "                                    \"tool_used\": {\"type\": \"string\"},\n",
    "                                    \"command\": {\"type\": \"string\"},\n",
    "                                    \"result\": {\"type\": \"object\"},\n",
    "                                    \"user_request\": {\"type\": \"string\"},\n",
    "                                    \"decision_type\": {\"type\": \"string\"}\n",
    "                                }\n",
    "                            },\n",
    "                            \"focus_areas\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\"type\": \"string\"},\n",
    "                                \"description\": \"Specific areas to focus criticism on: efficiency, safety, completeness, best_practices, alternatives\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"action\"]\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "\n",
    "        if \"debate_agent\" in requested_tools:\n",
    "            available_tools.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"debate_agent\",\n",
    "                    \"description\": \"Multi-perspective debate agent for design decisions. Presents alternative viewpoints and challenges assumptions.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"decision\": {\"type\": \"string\", \"description\": \"The design decision or approach being considered\"},\n",
    "                            \"context\": {\"type\": \"string\", \"description\": \"Background context and requirements\"},\n",
    "                            \"perspectives\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\"type\": \"string\"},\n",
    "                                \"description\": \"Perspectives to consider: security, performance, maintainability, scalability, cost, user_experience\"\n",
    "                            },\n",
    "                            \"debate_style\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"pros_cons\", \"alternatives\", \"devil_advocate\", \"stakeholder_views\"],\n",
    "                                \"description\": \"Style of debate analysis\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"decision\", \"context\"]\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "        if \"code_interpreter\" in requested_tools:\n",
    "            available_tools.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"code_interpreter\",\n",
    "                    \"description\": \"Execute Python code with visualization support. Handles plotting, data analysis, calculations, and complex computations with result capture.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"code\": {\"type\": \"string\", \"description\": \"Python code to execute\"},\n",
    "                            \"language\": {\"type\": \"string\", \"enum\": [\"python\"], \"description\": \"Programming language\"},\n",
    "                            \"capture_output\": {\"type\": \"boolean\", \"description\": \"Capture stdout/stderr\"},\n",
    "                            \"save_plots\": {\"type\": \"boolean\", \"description\": \"Save matplotlib plots as images\"},\n",
    "                            \"timeout\": {\"type\": \"integer\", \"description\": \"Execution timeout in seconds\"},\n",
    "                            \"working_dir\": {\"type\": \"string\", \"description\": \"Working directory for execution\"},\n",
    "                            \"requirements\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"Required packages to install\"}\n",
    "                        },\n",
    "                        \"required\": [\"code\"]\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "\n",
    "\n",
    "        if \"repo_quality_analyzer\" in requested_tools:\n",
    "            available_tools.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"repo_quality_analyzer\",\n",
    "                    \"description\": \"Comprehensive repository code quality analysis with actionable recommendations for improvements.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"repo_path\": {\"type\": \"string\", \"description\": \"Repository root path\"},\n",
    "                            \"analysis_type\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"full\", \"security\", \"performance\", \"maintainability\", \"documentation\"],\n",
    "                                \"description\": \"Type of analysis to perform\"\n",
    "                            },\n",
    "                            \"file_patterns\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"File patterns to analyze (e.g., '*.py', '*.js')\"},\n",
    "                            \"exclude_patterns\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"Patterns to exclude\"},\n",
    "                            \"severity_threshold\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\", \"critical\"], \"description\": \"Minimum severity for recommendations\"}\n",
    "                        },\n",
    "                        \"required\": [\"repo_path\"]\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "\n",
    "\n",
    "        if \"documentation_generator\" in requested_tools:\n",
    "            available_tools.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"documentation_generator\",\n",
    "                    \"description\": \"Intelligent documentation generator that analyzes entire repositories and creates comprehensive usage documentation.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"repo_path\": {\"type\": \"string\", \"description\": \"Repository root path\"},\n",
    "                            \"doc_type\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"api\", \"usage\", \"architecture\", \"setup\", \"comprehensive\"],\n",
    "                                \"description\": \"Type of documentation to generate\"\n",
    "                            },\n",
    "                            \"output_format\": {\"type\": \"string\", \"enum\": [\"markdown\", \"html\", \"rst\"], \"description\": \"Output format\"},\n",
    "                            \"include_examples\": {\"type\": \"boolean\", \"description\": \"Include code examples\"},\n",
    "                            \"include_diagrams\": {\"type\": \"boolean\", \"description\": \"Generate architecture diagrams\"},\n",
    "                            \"target_audience\": {\"type\": \"string\", \"enum\": [\"developers\", \"users\", \"contributors\"], \"description\": \"Target audience\"},\n",
    "                            \"output_path\": {\"type\": \"string\", \"description\": \"Output directory for documentation\"}\n",
    "                        },\n",
    "                        \"required\": [\"repo_path\", \"doc_type\"]\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "\n",
    "        if \"memory_manager\" in requested_tools:\n",
    "            available_tools.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"memory_manager\",\n",
    "                    \"description\": \"Manage conversation memory and token budget. Check usage, compress history, and optimize memory.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"action\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"status\", \"compress\", \"optimize\"],\n",
    "                                \"description\": \"Action: status (check usage), compress (force compression), optimize (auto-manage)\"\n",
    "                            },\n",
    "                            \"messages\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"description\": \"Conversation history for analysis\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"action\"]\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        if \"todo\" in requested_tools:\n",
    "            available_tools.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"todo\",\n",
    "                    \"description\": \"Break down tasks into steps\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"task\": {\"type\": \"string\"},\n",
    "                            \"action\": {\"type\": \"string\", \"enum\": [\"plan\", \"execute\", \"status\"]}\n",
    "                        },\n",
    "                        \"required\": [\"task\", \"action\"]\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return available_tools\n",
    "    def _fs_read(self, operations: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced fs_read with intelligent chunking and advanced features\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for op in operations:\n",
    "            mode = op.get(\"mode\")\n",
    "            path = op.get(\"path\")\n",
    "            \n",
    "            try:\n",
    "                if mode == \"Directory\":\n",
    "                    depth = op.get(\"depth\", 1)\n",
    "                    show_hidden = op.get(\"show_hidden\", False)\n",
    "                    items = self._list_directory_tree(path, depth, show_hidden)\n",
    "                    results.append({\"path\": path, \"items\": items, \"depth\": depth})\n",
    "                \n",
    "                elif mode == \"Line\":\n",
    "                    start_line = op.get(\"start_line\", 1)\n",
    "                    end_line = op.get(\"end_line\", -1)\n",
    "                    smart_chunk = op.get(\"smart_chunk\", True)\n",
    "                    content = self._read_file_intelligent(path, start_line, end_line, smart_chunk)\n",
    "                    results.append({\"path\": path, \"content\": content, \"start_line\": start_line, \"end_line\": end_line})\n",
    "                \n",
    "                elif mode == \"Search\":\n",
    "                    pattern = op.get(\"pattern\", \"\")\n",
    "                    context_lines = op.get(\"context_lines\", 2)\n",
    "                    case_sensitive = op.get(\"case_sensitive\", False)\n",
    "                    matches = self._search_in_file(path, pattern, context_lines, case_sensitive)\n",
    "                    results.append({\"path\": path, \"pattern\": pattern, \"matches\": matches})\n",
    "                \n",
    "                elif mode == \"Find\":\n",
    "                    name_pattern = op.get(\"name_pattern\", \"*\")\n",
    "                    file_type = op.get(\"file_type\", \"all\")\n",
    "                    max_depth = op.get(\"max_depth\", 10)\n",
    "                    found_items = self._find_files(path, name_pattern, file_type, max_depth)\n",
    "                    results.append({\"path\": path, \"name_pattern\": name_pattern, \"found\": found_items})\n",
    "                \n",
    "                elif mode == \"Grep\":\n",
    "                    pattern = op.get(\"pattern\", \"\")\n",
    "                    file_pattern = op.get(\"file_pattern\", \"*\")\n",
    "                    recursive = op.get(\"recursive\", True)\n",
    "                    context_lines = op.get(\"context_lines\", 2)\n",
    "                    matches = self._grep_files(path, pattern, file_pattern, recursive, context_lines)\n",
    "                    results.append({\"path\": path, \"pattern\": pattern, \"matches\": matches})\n",
    "                \n",
    "                elif mode == \"Tree\":\n",
    "                    max_depth = op.get(\"max_depth\", 3)\n",
    "                    show_hidden = op.get(\"show_hidden\", False)\n",
    "                    tree_structure = self._generate_tree(path, max_depth, show_hidden)\n",
    "                    results.append({\"path\": path, \"tree\": tree_structure})\n",
    "                \n",
    "            except Exception as e:\n",
    "                results.append({\"path\": path, \"error\": str(e)})\n",
    "        \n",
    "        return {\"results\": results}\n",
    "\n",
    "    def _get_file_info(self, path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive file information for intelligent reading decisions\"\"\"\n",
    "        path_obj = Path(path)\n",
    "        if not path_obj.exists():\n",
    "            return {\"error\": \"File not found\"}\n",
    "        \n",
    "        stat = path_obj.stat()\n",
    "        file_size = stat.st_size\n",
    "        \n",
    "        # Determine file type and characteristics\n",
    "        file_info = {\n",
    "            \"size\": file_size,\n",
    "            \"lines\": 0,\n",
    "            \"is_binary\": False,\n",
    "            \"encoding\": \"utf-8\",\n",
    "            \"file_type\": path_obj.suffix.lower(),\n",
    "            \"is_large\": file_size > 1024 * 1024,  # > 1MB\n",
    "            \"estimated_lines\": file_size // 50 if file_size > 0 else 0  # rough estimate\n",
    "        }\n",
    "        \n",
    "        # Quick binary check\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                chunk = f.read(1024)\n",
    "                if b'\\x00' in chunk:\n",
    "                    file_info[\"is_binary\"] = True\n",
    "                    return file_info\n",
    "        except:\n",
    "            file_info[\"is_binary\"] = True\n",
    "            return file_info\n",
    "        \n",
    "        # Count actual lines for text files (sample for large files)\n",
    "        try:\n",
    "            if file_size < 10 * 1024 * 1024:  # < 10MB, count all lines\n",
    "                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    file_info[\"lines\"] = sum(1 for _ in f)\n",
    "            else:\n",
    "                # Sample lines for very large files\n",
    "                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    sample_lines = sum(1 for _, line in zip(range(10000), f))\n",
    "                    file_info[\"lines\"] = int((file_size / (f.tell() or 1)) * sample_lines)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return file_info\n",
    "\n",
    "    def _decide_read_strategy(self, path: str, start_line: int, end_line: int, file_info: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Decide how to read the file based on size, type, and request\"\"\"\n",
    "        strategy = {\n",
    "            \"method\": \"full\",  # full, chunked, sampled, head_tail\n",
    "            \"chunk_size\": 1000,\n",
    "            \"sample_ratio\": 0.1,\n",
    "            \"reason\": \"\"\n",
    "        }\n",
    "        \n",
    "        total_lines = file_info.get(\"lines\", 0)\n",
    "        file_size = file_info.get(\"size\", 0)\n",
    "        is_large = file_info.get(\"is_large\", False)\n",
    "        \n",
    "        # Determine read range\n",
    "        if end_line == -1:\n",
    "            end_line = total_lines\n",
    "        \n",
    "        requested_lines = end_line - start_line + 1 if total_lines > 0 else 0\n",
    "        \n",
    "        # Decision logic similar to Amazon Q\n",
    "        if file_info.get(\"is_binary\"):\n",
    "            strategy.update({\"method\": \"binary_info\", \"reason\": \"Binary file detected\"})\n",
    "        elif file_size > 50 * 1024 * 1024:  # > 50MB\n",
    "            strategy.update({\"method\": \"head_tail\", \"chunk_size\": 500, \"reason\": \"Very large file, showing head and tail\"})\n",
    "        elif total_lines > 10000 and requested_lines > 5000:\n",
    "            strategy.update({\"method\": \"sampled\", \"sample_ratio\": 0.2, \"reason\": \"Large file with many requested lines\"})\n",
    "        elif total_lines > 5000 and (start_line == 1 and end_line == total_lines):\n",
    "            strategy.update({\"method\": \"chunked\", \"chunk_size\": 2000, \"reason\": \"Large file, full read requested\"})\n",
    "        elif requested_lines > 2000:\n",
    "            strategy.update({\"method\": \"chunked\", \"chunk_size\": 1000, \"reason\": \"Many lines requested\"})\n",
    "        else:\n",
    "            strategy.update({\"method\": \"full\", \"reason\": \"Standard read\"})\n",
    "        \n",
    "        return strategy\n",
    "\n",
    "    def _read_file_intelligent(self, path: str, start_line: int = 1, end_line: int = -1, smart_chunk: bool = True) -> str:\n",
    "        \"\"\"Intelligently read file based on size and content type\"\"\"\n",
    "        if not smart_chunk:\n",
    "            return self._read_file_lines(path, start_line, end_line)\n",
    "        \n",
    "        file_info = self._get_file_info(path)\n",
    "        if \"error\" in file_info:\n",
    "            return file_info[\"error\"]\n",
    "        \n",
    "        if file_info.get(\"is_binary\"):\n",
    "            return f\"[Binary file: {file_info['size']} bytes, type: {file_info.get('file_type', 'unknown')}]\"\n",
    "        \n",
    "        strategy = self._decide_read_strategy(path, start_line, end_line, file_info)\n",
    "        \n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            total_lines = len(lines)\n",
    "            if end_line == -1:\n",
    "                end_line = total_lines\n",
    "            \n",
    "            start_idx = max(0, start_line - 1)\n",
    "            end_idx = min(total_lines, end_line)\n",
    "            \n",
    "            if strategy[\"method\"] == \"full\":\n",
    "                content = ''.join(lines[start_idx:end_idx])\n",
    "            \n",
    "            elif strategy[\"method\"] == \"chunked\":\n",
    "                chunk_size = strategy[\"chunk_size\"]\n",
    "                chunks = []\n",
    "                current_idx = start_idx\n",
    "                \n",
    "                while current_idx < end_idx:\n",
    "                    chunk_end = min(current_idx + chunk_size, end_idx)\n",
    "                    chunk = ''.join(lines[current_idx:chunk_end])\n",
    "                    chunks.append(f\"--- Lines {current_idx + 1}-{chunk_end} ---\\n{chunk}\")\n",
    "                    current_idx = chunk_end\n",
    "                    \n",
    "                    if len(chunks) >= 5:  # Limit chunks\n",
    "                        remaining = end_idx - current_idx\n",
    "                        if remaining > 0:\n",
    "                            chunks.append(f\"... [{remaining} more lines omitted] ...\")\n",
    "                        break\n",
    "                \n",
    "                content = '\\n'.join(chunks)\n",
    "            \n",
    "            elif strategy[\"method\"] == \"head_tail\":\n",
    "                chunk_size = strategy[\"chunk_size\"]\n",
    "                head = ''.join(lines[start_idx:start_idx + chunk_size])\n",
    "                tail = ''.join(lines[max(start_idx + chunk_size, end_idx - chunk_size):end_idx])\n",
    "                \n",
    "                omitted = end_idx - start_idx - (2 * chunk_size)\n",
    "                content = f\"--- Head (lines {start_idx + 1}-{start_idx + chunk_size}) ---\\n{head}\\n\"\n",
    "                if omitted > 0:\n",
    "                    content += f\"... [{omitted} lines omitted] ...\\n\"\n",
    "                content += f\"--- Tail (lines {end_idx - chunk_size + 1}-{end_idx}) ---\\n{tail}\"\n",
    "            \n",
    "            elif strategy[\"method\"] == \"sampled\":\n",
    "                sample_ratio = strategy[\"sample_ratio\"]\n",
    "                total_requested = end_idx - start_idx\n",
    "                sample_size = int(total_requested * sample_ratio)\n",
    "                \n",
    "                if sample_size < 100:\n",
    "                    sample_size = min(100, total_requested)\n",
    "                \n",
    "                # Sample lines evenly\n",
    "                step = max(1, total_requested // sample_size)\n",
    "                sampled_lines = []\n",
    "                \n",
    "                for i in range(start_idx, end_idx, step):\n",
    "                    if len(sampled_lines) >= sample_size:\n",
    "                        break\n",
    "                    sampled_lines.append(f\"{i + 1:6d}: {lines[i]}\")\n",
    "                \n",
    "                content = f\"--- Sampled {len(sampled_lines)} lines from {total_requested} total ---\\n\"\n",
    "                content += ''.join(sampled_lines)\n",
    "            \n",
    "            # Add metadata\n",
    "            metadata = f\"\\n--- File Info: {file_info['size']} bytes, {file_info['lines']} lines, Strategy: {strategy['method']} ({strategy['reason']}) ---\"\n",
    "            return content + metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "    def _read_file_lines(self, path: str, start_line: int, end_line: int) -> str:\n",
    "        \"\"\"Simple line-based file reading\"\"\"\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        if end_line == -1:\n",
    "            end_line = len(lines)\n",
    "        \n",
    "        start_idx = max(0, start_line - 1)\n",
    "        end_idx = min(len(lines), end_line)\n",
    "        \n",
    "        return ''.join(lines[start_idx:end_idx])\n",
    "\n",
    "    def _list_directory_tree(self, path: str, depth: int, show_hidden: bool) -> List[Dict]:\n",
    "        \"\"\"List directory with depth and metadata\"\"\"\n",
    "        items = []\n",
    "        path_obj = Path(path)\n",
    "        \n",
    "        if not path_obj.exists() or not path_obj.is_dir():\n",
    "            return items\n",
    "            \n",
    "        def _scan_dir(dir_path: Path, current_depth: int) -> List[Dict]:\n",
    "            dir_items = []\n",
    "            if current_depth > depth:\n",
    "                return dir_items\n",
    "                \n",
    "            try:\n",
    "                for item in sorted(dir_path.iterdir()):\n",
    "                    if not show_hidden and item.name.startswith('.'):\n",
    "                        continue\n",
    "                        \n",
    "                    stat = item.stat()\n",
    "                    item_info = {\n",
    "                        \"name\": item.name,\n",
    "                        \"type\": \"directory\" if item.is_dir() else \"file\",\n",
    "                        \"size\": stat.st_size if item.is_file() else None,\n",
    "                        \"modified\": stat.st_mtime,\n",
    "                        \"permissions\": oct(stat.st_mode)[-3:],\n",
    "                        \"path\": str(item)\n",
    "                    }\n",
    "                    \n",
    "                    if item.is_dir() and current_depth < depth:\n",
    "                        item_info[\"children\"] = _scan_dir(item, current_depth + 1)\n",
    "                    \n",
    "                    dir_items.append(item_info)\n",
    "            except PermissionError:\n",
    "                pass\n",
    "                \n",
    "            return dir_items\n",
    "        \n",
    "        return _scan_dir(path_obj, 0)\n",
    "\n",
    "    def _search_in_file(self, path: str, pattern: str, context_lines: int, case_sensitive: bool) -> List[Dict]:\n",
    "        \"\"\"Search pattern in file with context\"\"\"\n",
    "        matches = []\n",
    "        \n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                lines = f.readlines()\n",
    "        except:\n",
    "            return matches\n",
    "        \n",
    "        search_pattern = pattern if case_sensitive else pattern.lower()\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            search_line = line if case_sensitive else line.lower()\n",
    "            \n",
    "            if search_pattern in search_line:\n",
    "                start_ctx = max(0, i - context_lines)\n",
    "                end_ctx = min(len(lines), i + context_lines + 1)\n",
    "                \n",
    "                context = []\n",
    "                for j in range(start_ctx, end_ctx):\n",
    "                    context.append({\n",
    "                        \"line_number\": j + 1,\n",
    "                        \"content\": lines[j].rstrip(),\n",
    "                        \"is_match\": j == i\n",
    "                    })\n",
    "                \n",
    "                matches.append({\n",
    "                    \"line_number\": i + 1,\n",
    "                    \"content\": line.rstrip(),\n",
    "                    \"context\": context\n",
    "                })\n",
    "        \n",
    "        return matches\n",
    "\n",
    "    def _find_files(self, path: str, name_pattern: str, file_type: str, max_depth: int) -> List[Dict]:\n",
    "        \"\"\"Find files by name pattern\"\"\"\n",
    "        found = []\n",
    "        path_obj = Path(path)\n",
    "        \n",
    "        def _search_recursive(dir_path: Path, current_depth: int):\n",
    "            if current_depth > max_depth:\n",
    "                return\n",
    "                \n",
    "            try:\n",
    "                for item in dir_path.iterdir():\n",
    "                    if fnmatch.fnmatch(item.name, name_pattern):\n",
    "                        if (file_type == \"all\" or \n",
    "                            (file_type == \"file\" and item.is_file()) or\n",
    "                            (file_type == \"dir\" and item.is_dir())):\n",
    "                            \n",
    "                            stat = item.stat()\n",
    "                            found.append({\n",
    "                                \"name\": item.name,\n",
    "                                \"path\": str(item),\n",
    "                                \"type\": \"directory\" if item.is_dir() else \"file\",\n",
    "                                \"size\": stat.st_size if item.is_file() else None,\n",
    "                                \"modified\": stat.st_mtime\n",
    "                            })\n",
    "                    \n",
    "                    if item.is_dir() and current_depth < max_depth:\n",
    "                        _search_recursive(item, current_depth + 1)\n",
    "            except PermissionError:\n",
    "                pass\n",
    "        \n",
    "        if path_obj.is_dir():\n",
    "            _search_recursive(path_obj, 0)\n",
    "        \n",
    "        return found\n",
    "\n",
    "    def _grep_files(self, path: str, pattern: str, file_pattern: str, recursive: bool, context_lines: int) -> List[Dict]:\n",
    "        \"\"\"Grep pattern across multiple files\"\"\"\n",
    "        matches = []\n",
    "        path_obj = Path(path)\n",
    "        \n",
    "        def _grep_file(file_path: Path):\n",
    "            try:\n",
    "                file_matches = self._search_in_file(str(file_path), pattern, context_lines, False)\n",
    "                if file_matches:\n",
    "                    matches.append({\n",
    "                        \"file\": str(file_path),\n",
    "                        \"matches\": file_matches\n",
    "                    })\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        def _scan_directory(dir_path: Path):\n",
    "            try:\n",
    "                for item in dir_path.iterdir():\n",
    "                    if item.is_file() and fnmatch.fnmatch(item.name, file_pattern):\n",
    "                        _grep_file(item)\n",
    "                    elif item.is_dir() and recursive:\n",
    "                        _scan_directory(item)\n",
    "            except PermissionError:\n",
    "                pass\n",
    "        \n",
    "        if path_obj.is_file():\n",
    "            _grep_file(path_obj)\n",
    "        elif path_obj.is_dir():\n",
    "            _scan_directory(path_obj)\n",
    "        \n",
    "        return matches\n",
    "\n",
    "    def _generate_tree(self, path: str, max_depth: int, show_hidden: bool) -> str:\n",
    "        \"\"\"Generate tree-like directory structure\"\"\"\n",
    "        tree_lines = []\n",
    "        path_obj = Path(path)\n",
    "        \n",
    "        def _build_tree(dir_path: Path, prefix: str, current_depth: int):\n",
    "            if current_depth > max_depth:\n",
    "                return\n",
    "                \n",
    "            try:\n",
    "                items = sorted(dir_path.iterdir(), key=lambda x: (x.is_file(), x.name.lower()))\n",
    "                if not show_hidden:\n",
    "                    items = [item for item in items if not item.name.startswith('.')]\n",
    "                \n",
    "                for i, item in enumerate(items):\n",
    "                    is_last = i == len(items) - 1\n",
    "                    current_prefix = \"└── \" if is_last else \"├── \"\n",
    "                    tree_lines.append(f\"{prefix}{current_prefix}{item.name}\")\n",
    "                    \n",
    "                    if item.is_dir() and current_depth < max_depth:\n",
    "                        next_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "                        _build_tree(item, next_prefix, current_depth + 1)\n",
    "            except PermissionError:\n",
    "                tree_lines.append(f\"{prefix}└── [Permission Denied]\")\n",
    "        \n",
    "        tree_lines.append(str(path_obj))\n",
    "        if path_obj.is_dir():\n",
    "            _build_tree(path_obj, \"\", 0)\n",
    "        \n",
    "        return \"\\n\".join(tree_lines)\n",
    "\n",
    "    def _fs_write(self, command: str, path: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced file writing with diff preview and advanced operations\"\"\"\n",
    "        import difflib\n",
    "        import shutil\n",
    "        from datetime import datetime\n",
    "        \n",
    "        try:\n",
    "            path_obj = Path(path)\n",
    "            show_diff = kwargs.get(\"show_diff\", True)\n",
    "            create_backup = kwargs.get(\"create_backup\", False)\n",
    "            summary = kwargs.get(\"summary\", \"\")\n",
    "            \n",
    "            # Read original content if file exists\n",
    "            original_content = \"\"\n",
    "            if path_obj.exists():\n",
    "                try:\n",
    "                    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        original_content = f.read()\n",
    "                except:\n",
    "                    original_content = \"[Binary or unreadable file]\"\n",
    "            \n",
    "            # Create backup if requested\n",
    "            if create_backup and path_obj.exists():\n",
    "                backup_path = f\"{path}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "                shutil.copy2(path, backup_path)\n",
    "            \n",
    "            new_content = original_content\n",
    "            \n",
    "            if command == \"create\":\n",
    "                file_text = kwargs.get(\"file_text\", \"\")\n",
    "                path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "                new_content = file_text\n",
    "                \n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(file_text)\n",
    "                \n",
    "                result = {\n",
    "                    \"success\": True, \n",
    "                    \"message\": f\"Created file: {path}\",\n",
    "                    \"lines_added\": len(file_text.splitlines()),\n",
    "                    \"size_bytes\": len(file_text.encode('utf-8'))\n",
    "                }\n",
    "                \n",
    "                if show_diff and file_text:\n",
    "                    result[\"diff\"] = self._generate_diff(\"\", file_text, path, \"created\")\n",
    "                \n",
    "                return result\n",
    "            \n",
    "            elif command == \"append\":\n",
    "                new_str = kwargs.get(\"new_str\", \"\")\n",
    "                new_content = original_content + ('\\n' if original_content and not original_content.endswith('\\n') else '') + new_str\n",
    "                \n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(new_content)\n",
    "                \n",
    "                result = {\n",
    "                    \"success\": True, \n",
    "                    \"message\": f\"Appended to file: {path}\",\n",
    "                    \"lines_added\": len(new_str.splitlines())\n",
    "                }\n",
    "                \n",
    "                if show_diff:\n",
    "                    result[\"diff\"] = self._generate_diff(original_content, new_content, path, \"appended\")\n",
    "                \n",
    "                return result\n",
    "            \n",
    "            elif command == \"prepend\":\n",
    "                new_str = kwargs.get(\"new_str\", \"\")\n",
    "                new_content = new_str + ('\\n' if new_str and not new_str.endswith('\\n') else '') + original_content\n",
    "                \n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(new_content)\n",
    "                \n",
    "                result = {\n",
    "                    \"success\": True, \n",
    "                    \"message\": f\"Prepended to file: {path}\",\n",
    "                    \"lines_added\": len(new_str.splitlines())\n",
    "                }\n",
    "                \n",
    "                if show_diff:\n",
    "                    result[\"diff\"] = self._generate_diff(original_content, new_content, path, \"prepended\")\n",
    "                \n",
    "                return result\n",
    "            \n",
    "            elif command == \"str_replace\":\n",
    "                old_str = kwargs.get(\"old_str\", \"\")\n",
    "                new_str = kwargs.get(\"new_str\", \"\")\n",
    "                \n",
    "                if old_str not in original_content:\n",
    "                    return {\n",
    "                        \"error\": f\"String not found in file: '{old_str[:100]}{'...' if len(old_str) > 100 else ''}'\",\n",
    "                        \"suggestion\": \"Use fs_read to check file content first\"\n",
    "                    }\n",
    "                \n",
    "                # Count occurrences\n",
    "                occurrences = original_content.count(old_str)\n",
    "                if occurrences > 1:\n",
    "                    return {\n",
    "                        \"error\": f\"Multiple occurrences found ({occurrences}). Please be more specific with old_str to ensure unique match.\",\n",
    "                        \"occurrences\": occurrences\n",
    "                    }\n",
    "                \n",
    "                new_content = original_content.replace(old_str, new_str)\n",
    "                \n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(new_content)\n",
    "                \n",
    "                result = {\n",
    "                    \"success\": True, \n",
    "                    \"message\": f\"Replaced text in: {path}\",\n",
    "                    \"replacements\": 1,\n",
    "                    \"old_length\": len(old_str),\n",
    "                    \"new_length\": len(new_str)\n",
    "                }\n",
    "                \n",
    "                if show_diff:\n",
    "                    result[\"diff\"] = self._generate_diff(original_content, new_content, path, \"modified\")\n",
    "                \n",
    "                return result\n",
    "            \n",
    "            elif command == \"insert\":\n",
    "                insert_line = kwargs.get(\"insert_line\", 1)\n",
    "                new_str = kwargs.get(\"new_str\", \"\")\n",
    "                \n",
    "                lines = original_content.splitlines(keepends=True)\n",
    "                insert_idx = min(max(0, insert_line - 1), len(lines))\n",
    "                \n",
    "                # Insert new content\n",
    "                new_lines = new_str.splitlines(keepends=True)\n",
    "                if new_lines and not new_lines[-1].endswith('\\n'):\n",
    "                    new_lines[-1] += '\\n'\n",
    "                \n",
    "                lines[insert_idx:insert_idx] = new_lines\n",
    "                new_content = ''.join(lines)\n",
    "                \n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(new_content)\n",
    "                \n",
    "                result = {\n",
    "                    \"success\": True, \n",
    "                    \"message\": f\"Inserted at line {insert_line} in: {path}\",\n",
    "                    \"lines_added\": len(new_lines)\n",
    "                }\n",
    "                \n",
    "                if show_diff:\n",
    "                    result[\"diff\"] = self._generate_diff(original_content, new_content, path, \"inserted\")\n",
    "                \n",
    "                return result\n",
    "            \n",
    "            elif command == \"delete_lines\":\n",
    "                start_line = kwargs.get(\"start_line\", 1)\n",
    "                end_line = kwargs.get(\"end_line\", start_line)\n",
    "                \n",
    "                lines = original_content.splitlines(keepends=True)\n",
    "                start_idx = max(0, start_line - 1)\n",
    "                end_idx = min(len(lines), end_line)\n",
    "                \n",
    "                deleted_lines = lines[start_idx:end_idx]\n",
    "                del lines[start_idx:end_idx]\n",
    "                new_content = ''.join(lines)\n",
    "                \n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(new_content)\n",
    "                \n",
    "                result = {\n",
    "                    \"success\": True, \n",
    "                    \"message\": f\"Deleted lines {start_line}-{end_line} from: {path}\",\n",
    "                    \"lines_deleted\": len(deleted_lines)\n",
    "                }\n",
    "                \n",
    "                if show_diff:\n",
    "                    result[\"diff\"] = self._generate_diff(original_content, new_content, path, \"deleted\")\n",
    "                \n",
    "                return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"File operation failed: {str(e)}\"}\n",
    "\n",
    "    def _generate_diff(self, original: str, new: str, filepath: str, operation: str) -> str:\n",
    "        \"\"\"Generate git-style diff output\"\"\"\n",
    "        if original == new:\n",
    "            return \"No changes\"\n",
    "        \n",
    "        original_lines = original.splitlines(keepends=True)\n",
    "        new_lines = new.splitlines(keepends=True)\n",
    "        \n",
    "        diff_lines = list(difflib.unified_diff(\n",
    "            original_lines, \n",
    "            new_lines,\n",
    "            fromfile=f\"a/{filepath}\",\n",
    "            tofile=f\"b/{filepath}\",\n",
    "            lineterm=\"\"\n",
    "        ))\n",
    "        \n",
    "        if not diff_lines:\n",
    "            return \"No changes\"\n",
    "        \n",
    "        # Format like git diff\n",
    "        diff_output = []\n",
    "        diff_output.append(f\"--- a/{filepath}\")\n",
    "        diff_output.append(f\"+++ b/{filepath}\")\n",
    "        \n",
    "        # Skip the first two lines (file headers) from difflib output\n",
    "        for line in diff_lines[2:]:\n",
    "            diff_output.append(line.rstrip())\n",
    "        \n",
    "        # Add summary\n",
    "        added_lines = sum(1 for line in diff_lines if line.startswith('+') and not line.startswith('+++'))\n",
    "        removed_lines = sum(1 for line in diff_lines if line.startswith('-') and not line.startswith('---'))\n",
    "        \n",
    "        summary = f\"\\n📝 Changes: +{added_lines} -{removed_lines} lines ({operation})\"\n",
    "        \n",
    "        return '\\n'.join(diff_output) + summary\n",
    "\n",
    "    def _execute_bash(self, command: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced bash execution with advanced features\"\"\"\n",
    "        import shlex\n",
    "        import time\n",
    "        from datetime import datetime\n",
    "        \n",
    "        # Extract parameters\n",
    "        working_dir = kwargs.get(\"working_dir\", os.getcwd())\n",
    "        timeout = kwargs.get(\"timeout\", 30)\n",
    "        env_vars = kwargs.get(\"env_vars\", {})\n",
    "        capture_output = kwargs.get(\"capture_output\", True)\n",
    "        shell_path = kwargs.get(\"shell\", \"/bin/bash\")\n",
    "        summary = kwargs.get(\"summary\", \"\")\n",
    "        \n",
    "        # Prepare environment\n",
    "        env = os.environ.copy()\n",
    "        env.update(env_vars)\n",
    "        \n",
    "        # Security check for dangerous commands\n",
    "        dangerous_patterns = [\n",
    "            \"rm -rf /\", \":(){ :|:& };:\", \"dd if=/dev/zero\", \"mkfs\", \"fdisk\",\n",
    "            \"shutdown\", \"reboot\", \"halt\", \"init 0\", \"init 6\"\n",
    "        ]\n",
    "        \n",
    "        if any(pattern in command.lower() for pattern in dangerous_patterns):\n",
    "            return {\n",
    "                \"error\": \"Potentially dangerous command blocked for safety\",\n",
    "                \"command\": command,\n",
    "                \"blocked_patterns\": [p for p in dangerous_patterns if p in command.lower()]\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Change to working directory if specified\n",
    "            original_cwd = os.getcwd()\n",
    "            if working_dir and working_dir != original_cwd:\n",
    "                if not os.path.exists(working_dir):\n",
    "                    return {\"error\": f\"Working directory does not exist: {working_dir}\"}\n",
    "                os.chdir(working_dir)\n",
    "            \n",
    "            # Execute command\n",
    "            if capture_output:\n",
    "                result = subprocess.run(\n",
    "                    command,\n",
    "                    shell=True,\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    timeout=timeout,\n",
    "                    env=env,\n",
    "                    executable=shell_path if os.path.exists(shell_path) else None\n",
    "                )\n",
    "                \n",
    "                execution_time = time.time() - start_time\n",
    "                \n",
    "                # Format output\n",
    "                output_info = {\n",
    "                    \"command\": command,\n",
    "                    \"exit_status\": result.returncode,\n",
    "                    \"execution_time\": round(execution_time, 3),\n",
    "                    \"working_directory\": working_dir,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                if result.stdout:\n",
    "                    output_info[\"stdout\"] = result.stdout\n",
    "                    output_info[\"stdout_lines\"] = len(result.stdout.splitlines())\n",
    "                \n",
    "                if result.stderr:\n",
    "                    output_info[\"stderr\"] = result.stderr\n",
    "                    output_info[\"stderr_lines\"] = len(result.stderr.splitlines())\n",
    "                \n",
    "                # Add success/failure indicator\n",
    "                if result.returncode == 0:\n",
    "                    output_info[\"status\"] = \"success\"\n",
    "                    if summary:\n",
    "                        output_info[\"summary\"] = f\"✅ {summary}\"\n",
    "                else:\n",
    "                    output_info[\"status\"] = \"failed\"\n",
    "                    if summary:\n",
    "                        output_info[\"summary\"] = f\"❌ {summary}\"\n",
    "                \n",
    "                # Format output like Amazon Q\n",
    "                formatted_output = self._format_command_output(output_info)\n",
    "                output_info[\"formatted_output\"] = formatted_output\n",
    "                \n",
    "                return output_info\n",
    "            \n",
    "            else:\n",
    "                # Non-capturing execution for interactive commands\n",
    "                result = subprocess.run(\n",
    "                    command,\n",
    "                    shell=True,\n",
    "                    timeout=timeout,\n",
    "                    env=env,\n",
    "                    executable=shell_path if os.path.exists(shell_path) else None\n",
    "                )\n",
    "                \n",
    "                return {\n",
    "                    \"command\": command,\n",
    "                    \"exit_status\": result.returncode,\n",
    "                    \"status\": \"success\" if result.returncode == 0 else \"failed\",\n",
    "                    \"execution_time\": round(time.time() - start_time, 3),\n",
    "                    \"note\": \"Output not captured (interactive mode)\"\n",
    "                }\n",
    "        \n",
    "        except subprocess.TimeoutExpired:\n",
    "            return {\n",
    "                \"error\": f\"Command timed out after {timeout} seconds\",\n",
    "                \"command\": command,\n",
    "                \"timeout\": timeout\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": f\"Command execution failed: {str(e)}\",\n",
    "                \"command\": command\n",
    "            }\n",
    "        finally:\n",
    "            # Restore original working directory\n",
    "            if working_dir and working_dir != original_cwd:\n",
    "                os.chdir(original_cwd)\n",
    "\n",
    "    def _format_command_output(self, output_info: Dict[str, Any]) -> str:\n",
    "        \"\"\"Format command output in Amazon Q style\"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        # Header\n",
    "        status_icon = \"✅\" if output_info[\"status\"] == \"success\" else \"❌\"\n",
    "        lines.append(f\"{status_icon} Command: {output_info['command']}\")\n",
    "        lines.append(f\"📁 Working Directory: {output_info['working_directory']}\")\n",
    "        lines.append(f\"⏱️  Execution Time: {output_info['execution_time']}s\")\n",
    "        lines.append(f\"🔢 Exit Code: {output_info['exit_status']}\")\n",
    "        \n",
    "        # Stdout\n",
    "        if output_info.get(\"stdout\"):\n",
    "            lines.append(f\"\\n📤 Output ({output_info.get('stdout_lines', 0)} lines):\")\n",
    "            lines.append(\"```\")\n",
    "            lines.append(output_info[\"stdout\"].rstrip())\n",
    "            lines.append(\"```\")\n",
    "        \n",
    "        # Stderr\n",
    "        if output_info.get(\"stderr\"):\n",
    "            lines.append(f\"\\n⚠️  Error Output ({output_info.get('stderr_lines', 0)} lines):\")\n",
    "            lines.append(\"```\")\n",
    "            lines.append(output_info[\"stderr\"].rstrip())\n",
    "            lines.append(\"```\")\n",
    "        \n",
    "        # Summary\n",
    "        if output_info.get(\"summary\"):\n",
    "            lines.append(f\"\\n{output_info['summary']}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def _get_command_suggestions(self, command: str, error: str) -> List[str]:\n",
    "        \"\"\"Provide helpful suggestions for failed commands\"\"\"\n",
    "        suggestions = []\n",
    "        \n",
    "        if \"command not found\" in error.lower():\n",
    "            cmd_name = command.split()[0]\n",
    "            suggestions.append(f\"Install {cmd_name}: sudo apt install {cmd_name} (Ubuntu/Debian)\")\n",
    "            suggestions.append(f\"Check if {cmd_name} is in PATH: which {cmd_name}\")\n",
    "        \n",
    "        elif \"permission denied\" in error.lower():\n",
    "            suggestions.append(f\"Try with sudo: sudo {command}\")\n",
    "            suggestions.append(\"Check file permissions: ls -la\")\n",
    "        \n",
    "        elif \"no such file or directory\" in error.lower():\n",
    "            suggestions.append(\"Check if the file/directory exists\")\n",
    "            suggestions.append(\"Use absolute path instead of relative path\")\n",
    "        \n",
    "        elif \"connection refused\" in error.lower():\n",
    "            suggestions.append(\"Check if the service is running\")\n",
    "            suggestions.append(\"Verify network connectivity\")\n",
    "        \n",
    "        return suggestions\n",
    "\n",
    "    def _memory_manager(self, action: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Manage conversation memory and token budget\"\"\"\n",
    "        messages = kwargs.get(\"messages\", [])\n",
    "        \n",
    "        if action == \"status\":\n",
    "            token_info = self.get_token_info(messages)\n",
    "            return {\n",
    "                \"token_usage\": token_info,\n",
    "                \"message_count\": len(messages),\n",
    "                \"status\": \"healthy\" if not token_info[\"needs_compression\"] else \"needs_compression\",\n",
    "                \"recommendation\": \"Continue normally\" if not token_info[\"needs_compression\"] else \"Compression recommended\"\n",
    "            }\n",
    "        \n",
    "        elif action == \"compress\":\n",
    "            if len(messages) <= 2:\n",
    "                return {\"error\": \"Not enough messages to compress\"}\n",
    "            \n",
    "            compressed, was_compressed = self.check_and_compress_history(messages)\n",
    "            if was_compressed:\n",
    "                original_tokens = self.token_manager.count_conversation_tokens(messages)\n",
    "                new_tokens = self.token_manager.count_conversation_tokens(compressed)\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"compressed\": True,\n",
    "                    \"original_messages\": len(messages),\n",
    "                    \"compressed_messages\": len(compressed),\n",
    "                    \"original_tokens\": original_tokens,\n",
    "                    \"compressed_tokens\": new_tokens,\n",
    "                    \"tokens_saved\": original_tokens - new_tokens,\n",
    "                    \"compression_ratio\": round((1 - new_tokens/original_tokens) * 100, 1),\n",
    "                    \"compressed_history\": compressed\n",
    "                }\n",
    "            else:\n",
    "                return {\"success\": True, \"compressed\": False, \"reason\": \"No compression needed\"}\n",
    "        \n",
    "        elif action == \"optimize\":\n",
    "            token_info = self.get_token_info(messages)\n",
    "            if token_info[\"needs_compression\"]:\n",
    "                return self._memory_manager(\"compress\", messages=messages)\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"action_taken\": \"none\",\n",
    "                    \"reason\": \"Memory usage within acceptable limits\",\n",
    "                    \"token_info\": token_info\n",
    "                }\n",
    "        \n",
    "        return {\"error\": \"Invalid action\"}\n",
    "\n",
    "    def _code_interpreter(self, code: str, **kwargs) -> Dict[str, Any]:\n",
    "        \n",
    "        if action == \"critique\":\n",
    "            return self._critique_action(context, focus_areas)\n",
    "        elif action == \"validate\":\n",
    "            return self._validate_decision(context, focus_areas)\n",
    "        elif action == \"reflect\":\n",
    "            return self._self_reflect(context)\n",
    "        elif action == \"improve\":\n",
    "            return self._suggest_improvements(context, focus_areas)\n",
    "        else:\n",
    "            return {\"error\": \"Invalid introspection action\"}\n",
    "\n",
    "    def _critique_action(self, context: Dict, focus_areas: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Provide constructive criticism of an action\"\"\"\n",
    "        tool_used = context.get(\"tool_used\", \"\")\n",
    "        command = context.get(\"command\", \"\")\n",
    "        result = context.get(\"result\", {})\n",
    "        \n",
    "        critique = {\n",
    "            \"overall_assessment\": \"analyzing...\",\n",
    "            \"strengths\": [],\n",
    "            \"weaknesses\": [],\n",
    "            \"risks\": [],\n",
    "            \"suggestions\": [],\n",
    "            \"score\": 0  # 1-10 scale\n",
    "        }\n",
    "        \n",
    "        # Analyze based on tool used\n",
    "        if tool_used == \"fs_write\":\n",
    "            critique.update(self._critique_fs_write(context, focus_areas))\n",
    "        elif tool_used == \"execute_bash\":\n",
    "            critique.update(self._critique_bash_execution(context, focus_areas))\n",
    "        elif tool_used == \"fs_read\":\n",
    "            critique.update(self._critique_fs_read(context, focus_areas))\n",
    "        else:\n",
    "            critique.update(self._generic_critique(context, focus_areas))\n",
    "        \n",
    "        return {\"critique\": critique, \"action_recommended\": self._get_recommendation(critique)}\n",
    "\n",
    "    def _critique_fs_write(self, context: Dict, focus_areas: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Critique file writing operations\"\"\"\n",
    "        command = context.get(\"command\", \"\")\n",
    "        result = context.get(\"result\", {})\n",
    "        \n",
    "        strengths = []\n",
    "        weaknesses = []\n",
    "        risks = []\n",
    "        suggestions = []\n",
    "        score = 7  # default\n",
    "        \n",
    "        if \"safety\" in focus_areas:\n",
    "            if command == \"str_replace\" and result.get(\"replacements\", 0) > 1:\n",
    "                risks.append(\"Multiple replacements could affect unintended code\")\n",
    "                suggestions.append(\"Use more specific old_str to ensure single replacement\")\n",
    "                score -= 1\n",
    "            elif command == \"create\" and not result.get(\"show_diff\"):\n",
    "                weaknesses.append(\"No diff preview for file creation\")\n",
    "                suggestions.append(\"Enable show_diff to review changes\")\n",
    "            else:\n",
    "                strengths.append(\"Safe file operation with proper validation\")\n",
    "        \n",
    "        if \"efficiency\" in focus_areas:\n",
    "            if result.get(\"size_bytes\", 0) > 1024 * 1024:  # > 1MB\n",
    "                weaknesses.append(\"Large file operation may impact performance\")\n",
    "                suggestions.append(\"Consider chunked operations for large files\")\n",
    "                score -= 1\n",
    "            else:\n",
    "                strengths.append(\"Efficient file size handling\")\n",
    "        \n",
    "        if \"completeness\" in focus_areas:\n",
    "            if not result.get(\"success\"):\n",
    "                weaknesses.append(\"Operation failed - incomplete action\")\n",
    "                suggestions.append(\"Check file permissions and path validity\")\n",
    "                score -= 2\n",
    "            elif result.get(\"diff\"):\n",
    "                strengths.append(\"Complete with change visualization\")\n",
    "                score += 1\n",
    "        \n",
    "        return {\n",
    "            \"overall_assessment\": f\"File operation scored {score}/10\",\n",
    "            \"strengths\": strengths,\n",
    "            \"weaknesses\": weaknesses,\n",
    "            \"risks\": risks,\n",
    "            \"suggestions\": suggestions,\n",
    "            \"score\": score\n",
    "        }\n",
    "\n",
    "    def _critique_bash_execution(self, context: Dict, focus_areas: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Critique bash command execution\"\"\"\n",
    "        command = context.get(\"command\", \"\")\n",
    "        result = context.get(\"result\", {})\n",
    "        \n",
    "        strengths = []\n",
    "        weaknesses = []\n",
    "        risks = []\n",
    "        suggestions = []\n",
    "        score = 7\n",
    "        \n",
    "        if \"safety\" in focus_areas:\n",
    "            dangerous_commands = [\"rm -rf\", \"sudo rm\", \"dd if=\", \"mkfs\", \"fdisk\"]\n",
    "            if any(cmd in command.lower() for cmd in dangerous_commands):\n",
    "                risks.append(\"Potentially destructive command detected\")\n",
    "                suggestions.append(\"Use safer alternatives or add confirmation\")\n",
    "                score -= 3\n",
    "            elif result.get(\"exit_status\") != 0:\n",
    "                weaknesses.append(\"Command failed - potential system impact\")\n",
    "                suggestions.append(\"Check error output and validate command syntax\")\n",
    "                score -= 1\n",
    "            else:\n",
    "                strengths.append(\"Safe command execution\")\n",
    "        \n",
    "        if \"efficiency\" in focus_areas:\n",
    "            exec_time = result.get(\"execution_time\", 0)\n",
    "            if exec_time > 10:\n",
    "                weaknesses.append(f\"Long execution time: {exec_time}s\")\n",
    "                suggestions.append(\"Consider background execution or optimization\")\n",
    "                score -= 1\n",
    "            elif exec_time < 0.1:\n",
    "                strengths.append(\"Fast execution\")\n",
    "                score += 1\n",
    "        \n",
    "        return {\n",
    "            \"overall_assessment\": f\"Command execution scored {score}/10\",\n",
    "            \"strengths\": strengths,\n",
    "            \"weaknesses\": weaknesses,\n",
    "            \"risks\": risks,\n",
    "            \"suggestions\": suggestions,\n",
    "            \"score\": score\n",
    "        }\n",
    "\n",
    "    def _critique_fs_read(self, context: Dict, focus_areas: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Critique file reading operations\"\"\"\n",
    "        result = context.get(\"result\", {})\n",
    "        \n",
    "        strengths = []\n",
    "        weaknesses = []\n",
    "        suggestions = []\n",
    "        score = 8\n",
    "        \n",
    "        if \"efficiency\" in focus_areas:\n",
    "            results = result.get(\"results\", [])\n",
    "            for res in results:\n",
    "                if res.get(\"error\"):\n",
    "                    weaknesses.append(f\"Failed to read: {res.get('path')}\")\n",
    "                    score -= 1\n",
    "                elif \"tree\" in res and len(res[\"tree\"].split('\\n')) > 100:\n",
    "                    weaknesses.append(\"Large tree output may be overwhelming\")\n",
    "                    suggestions.append(\"Use max_depth parameter to limit output\")\n",
    "                    score -= 1\n",
    "                else:\n",
    "                    strengths.append(\"Efficient file reading strategy\")\n",
    "        \n",
    "        return {\n",
    "            \"overall_assessment\": f\"File reading scored {score}/10\",\n",
    "            \"strengths\": strengths,\n",
    "            \"weaknesses\": weaknesses,\n",
    "            \"risks\": [],\n",
    "            \"suggestions\": suggestions,\n",
    "            \"score\": score\n",
    "        }\n",
    "\n",
    "    def _generic_critique(self, context: Dict, focus_areas: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Generic critique for unknown actions\"\"\"\n",
    "        return {\n",
    "            \"overall_assessment\": \"Generic action - limited analysis available\",\n",
    "            \"strengths\": [\"Action completed\"],\n",
    "            \"weaknesses\": [\"No specific analysis available\"],\n",
    "            \"risks\": [],\n",
    "            \"suggestions\": [\"Provide more context for detailed critique\"],\n",
    "            \"score\": 6\n",
    "        }\n",
    "\n",
    "    def _validate_decision(self, context: Dict, focus_areas: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Validate a decision or approach\"\"\"\n",
    "        decision_type = context.get(\"decision_type\", \"unknown\")\n",
    "        \n",
    "        validation = {\n",
    "            \"is_valid\": True,\n",
    "            \"confidence\": 0.8,\n",
    "            \"concerns\": [],\n",
    "            \"alternatives\": [],\n",
    "            \"recommendation\": \"proceed\"\n",
    "        }\n",
    "        \n",
    "        # Add decision-specific validation logic\n",
    "        if \"architecture\" in decision_type.lower():\n",
    "            validation[\"alternatives\"].append(\"Consider microservices vs monolith trade-offs\")\n",
    "            validation[\"concerns\"].append(\"Scalability implications\")\n",
    "        elif \"security\" in decision_type.lower():\n",
    "            validation[\"concerns\"].append(\"Ensure compliance with security standards\")\n",
    "            validation[\"alternatives\"].append(\"Multi-factor authentication options\")\n",
    "        \n",
    "        return {\"validation\": validation}\n",
    "\n",
    "    def _self_reflect(self, context: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Self-reflection on overall performance\"\"\"\n",
    "        return {\n",
    "            \"reflection\": {\n",
    "                \"performance_areas\": [\"tool_usage\", \"decision_making\", \"communication\"],\n",
    "                \"strengths\": [\"Systematic approach\", \"Tool integration\"],\n",
    "                \"improvement_areas\": [\"Context awareness\", \"Error handling\"],\n",
    "                \"learning_points\": [\"User feedback incorporation\", \"Efficiency optimization\"],\n",
    "                \"confidence_level\": 0.75\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _suggest_improvements(self, context: Dict, focus_areas: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Suggest specific improvements\"\"\"\n",
    "        improvements = []\n",
    "        \n",
    "        for area in focus_areas:\n",
    "            if area == \"efficiency\":\n",
    "                improvements.append(\"Use batch operations for multiple file operations\")\n",
    "                improvements.append(\"Implement caching for repeated operations\")\n",
    "            elif area == \"safety\":\n",
    "                improvements.append(\"Add confirmation prompts for destructive operations\")\n",
    "                improvements.append(\"Implement rollback mechanisms\")\n",
    "            elif area == \"completeness\":\n",
    "                improvements.append(\"Add comprehensive error handling\")\n",
    "                improvements.append(\"Provide detailed operation summaries\")\n",
    "        \n",
    "        return {\"improvements\": improvements, \"priority\": \"high\"}\n",
    "\n",
    "    def _get_recommendation(self, critique: Dict) -> str:\n",
    "        \"\"\"Get action recommendation based on critique\"\"\"\n",
    "        score = critique.get(\"score\", 5)\n",
    "        \n",
    "        if score >= 8:\n",
    "            return \"proceed_confidently\"\n",
    "        elif score >= 6:\n",
    "            return \"proceed_with_caution\"\n",
    "        elif score >= 4:\n",
    "            return \"revise_approach\"\n",
    "        else:\n",
    "            return \"stop_and_reconsider\"\n",
    "\n",
    "    def _debate_agent(self, decision: str, context: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Multi-perspective debate agent for design decisions\"\"\"\n",
    "        perspectives = kwargs.get(\"perspectives\", [\"security\", \"performance\", \"maintainability\"])\n",
    "        debate_style = kwargs.get(\"debate_style\", \"pros_cons\")\n",
    "        \n",
    "        if debate_style == \"pros_cons\":\n",
    "            return self._pros_cons_analysis(decision, context, perspectives)\n",
    "        elif debate_style == \"alternatives\":\n",
    "            return self._alternatives_analysis(decision, context, perspectives)\n",
    "        elif debate_style == \"devil_advocate\":\n",
    "            return self._devil_advocate_analysis(decision, context, perspectives)\n",
    "        elif debate_style == \"stakeholder_views\":\n",
    "            return self._stakeholder_analysis(decision, context, perspectives)\n",
    "        \n",
    "        return {\"error\": \"Invalid debate style\"}\n",
    "\n",
    "    def _pros_cons_analysis(self, decision: str, context: str, perspectives: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze pros and cons from multiple perspectives\"\"\"\n",
    "        analysis = {\"decision\": decision, \"perspectives\": {}}\n",
    "        \n",
    "        for perspective in perspectives:\n",
    "            pros, cons = self._get_perspective_pros_cons(decision, perspective, context)\n",
    "            analysis[\"perspectives\"][perspective] = {\"pros\": pros, \"cons\": cons}\n",
    "        \n",
    "        # Overall recommendation\n",
    "        total_pros = sum(len(p[\"pros\"]) for p in analysis[\"perspectives\"].values())\n",
    "        total_cons = sum(len(p[\"cons\"]) for p in analysis[\"perspectives\"].values())\n",
    "        \n",
    "        analysis[\"recommendation\"] = {\n",
    "            \"overall_score\": total_pros - total_cons,\n",
    "            \"confidence\": min(0.9, max(0.3, abs(total_pros - total_cons) / max(total_pros + total_cons, 1))),\n",
    "            \"action\": \"approve\" if total_pros > total_cons else \"reconsider\"\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def _get_perspective_pros_cons(self, decision: str, perspective: str, context: str) -> tuple:\n",
    "        \"\"\"Get pros and cons for a specific perspective\"\"\"\n",
    "        pros = []\n",
    "        cons = []\n",
    "        \n",
    "        if perspective == \"security\":\n",
    "            if \"authentication\" in decision.lower():\n",
    "                pros.append(\"Enhanced user verification\")\n",
    "                cons.append(\"Additional complexity for users\")\n",
    "            if \"encryption\" in decision.lower():\n",
    "                pros.append(\"Data protection in transit/rest\")\n",
    "                cons.append(\"Performance overhead\")\n",
    "        \n",
    "        elif perspective == \"performance\":\n",
    "            if \"cache\" in decision.lower():\n",
    "                pros.append(\"Faster response times\")\n",
    "                cons.append(\"Memory usage increase\")\n",
    "            if \"database\" in decision.lower():\n",
    "                pros.append(\"Optimized data access\")\n",
    "                cons.append(\"Potential bottleneck\")\n",
    "        \n",
    "        elif perspective == \"maintainability\":\n",
    "            if \"microservice\" in decision.lower():\n",
    "                pros.append(\"Independent deployment\")\n",
    "                cons.append(\"Increased operational complexity\")\n",
    "            if \"monolith\" in decision.lower():\n",
    "                pros.append(\"Simpler deployment\")\n",
    "                cons.append(\"Harder to scale specific components\")\n",
    "        \n",
    "        # Generic fallbacks\n",
    "        if not pros:\n",
    "            pros.append(f\"Addresses {perspective} requirements\")\n",
    "        if not cons:\n",
    "            cons.append(f\"May impact {perspective} considerations\")\n",
    "        \n",
    "        return pros, cons\n",
    "\n",
    "    def _alternatives_analysis(self, decision: str, context: str, perspectives: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate alternative approaches\"\"\"\n",
    "        alternatives = []\n",
    "        \n",
    "        # Generate alternatives based on decision type\n",
    "        if \"database\" in decision.lower():\n",
    "            alternatives.extend([\"SQL database\", \"NoSQL database\", \"In-memory cache\", \"File-based storage\"])\n",
    "        elif \"architecture\" in decision.lower():\n",
    "            alternatives.extend([\"Microservices\", \"Monolith\", \"Serverless\", \"Event-driven\"])\n",
    "        elif \"deployment\" in decision.lower():\n",
    "            alternatives.extend([\"Docker containers\", \"Virtual machines\", \"Kubernetes\", \"Serverless functions\"])\n",
    "        else:\n",
    "            alternatives.extend([\"Current approach\", \"Simplified version\", \"Enhanced version\", \"Alternative technology\"])\n",
    "        \n",
    "        # Score each alternative\n",
    "        scored_alternatives = []\n",
    "        for alt in alternatives[:4]:  # Limit to 4 alternatives\n",
    "            score = self._score_alternative(alt, perspectives, context)\n",
    "            scored_alternatives.append({\"alternative\": alt, \"score\": score, \"reasoning\": f\"Evaluated against {', '.join(perspectives)}\"})\n",
    "        \n",
    "        return {\n",
    "            \"original_decision\": decision,\n",
    "            \"alternatives\": scored_alternatives,\n",
    "            \"recommendation\": max(scored_alternatives, key=lambda x: x[\"score\"])\n",
    "        }\n",
    "\n",
    "    def _score_alternative(self, alternative: str, perspectives: List[str], context: str) -> float:\n",
    "        \"\"\"Score an alternative based on perspectives\"\"\"\n",
    "        base_score = 0.5\n",
    "        \n",
    "        # Simple scoring logic\n",
    "        for perspective in perspectives:\n",
    "            if perspective == \"security\" and \"secure\" in alternative.lower():\n",
    "                base_score += 0.2\n",
    "            elif perspective == \"performance\" and any(word in alternative.lower() for word in [\"fast\", \"cache\", \"optimized\"]):\n",
    "                base_score += 0.2\n",
    "            elif perspective == \"maintainability\" and any(word in alternative.lower() for word in [\"simple\", \"clean\", \"modular\"]):\n",
    "                base_score += 0.2\n",
    "        \n",
    "        return min(1.0, base_score)\n",
    "\n",
    "    def _devil_advocate_analysis(self, decision: str, context: str, perspectives: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Play devil's advocate against the decision\"\"\"\n",
    "        challenges = []\n",
    "        \n",
    "        challenges.append(f\"What if {decision} fails under high load?\")\n",
    "        challenges.append(f\"How does {decision} handle edge cases?\")\n",
    "        challenges.append(f\"What are the long-term maintenance costs of {decision}?\")\n",
    "        challenges.append(f\"Are there simpler alternatives to {decision}?\")\n",
    "        \n",
    "        return {\n",
    "            \"decision\": decision,\n",
    "            \"challenges\": challenges,\n",
    "            \"risk_assessment\": \"medium\",\n",
    "            \"mitigation_strategies\": [\n",
    "                \"Implement comprehensive testing\",\n",
    "                \"Plan rollback procedures\",\n",
    "                \"Monitor key metrics\",\n",
    "                \"Document decision rationale\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def _stakeholder_analysis(self, decision: str, context: str, perspectives: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze from different stakeholder viewpoints\"\"\"\n",
    "        stakeholders = {\n",
    "            \"developers\": {\"concerns\": [\"Complexity\", \"Learning curve\"], \"benefits\": [\"Better tools\", \"Cleaner code\"]},\n",
    "            \"operations\": {\"concerns\": [\"Deployment complexity\", \"Monitoring\"], \"benefits\": [\"Automation\", \"Reliability\"]},\n",
    "            \"users\": {\"concerns\": [\"Performance impact\", \"Downtime\"], \"benefits\": [\"Better experience\", \"New features\"]},\n",
    "            \"management\": {\"concerns\": [\"Cost\", \"Timeline\"], \"benefits\": [\"ROI\", \"Competitive advantage\"]}\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"decision\": decision,\n",
    "            \"stakeholder_views\": stakeholders,\n",
    "            \"consensus_level\": \"moderate\",\n",
    "            \"key_negotiations\": [\"Timeline vs Quality\", \"Cost vs Features\"]\n",
    "        }\n",
    "        \n",
    "    def _todo(self, task: str, action: str) -> Dict[str, Any]:\n",
    "        \"\"\"Break down big tasks into smaller chunks\"\"\"\n",
    "        task_hash = str(hash(task))\n",
    "        \n",
    "        if action == \"plan\":\n",
    "            steps = [\n",
    "                {\"description\": \"Analyze requirements\"},\n",
    "                {\"description\": \"Create project structure\"},\n",
    "                {\"description\": \"Implement core functionality\"},\n",
    "                {\"description\": \"Add tests and documentation\"}\n",
    "            ]\n",
    "            self._task_plans[task_hash] = {\"task\": task, \"steps\": steps, \"completed\": []}\n",
    "            return {\"task\": task, \"steps\": steps, \"total_steps\": len(steps)}\n",
    "        \n",
    "        elif action == \"execute\":\n",
    "            if task_hash in self._task_plans:\n",
    "                plan = self._task_plans[task_hash]\n",
    "                next_step = len(plan[\"completed\"])\n",
    "                if next_step < len(plan[\"steps\"]):\n",
    "                    step = plan[\"steps\"][next_step]\n",
    "                    plan[\"completed\"].append(next_step)\n",
    "                    return {\"step\": step, \"progress\": f\"{len(plan['completed'])}/{len(plan['steps'])}\"}\n",
    "            return {\"message\": \"Task not found or completed\"}\n",
    "        \n",
    "        elif action == \"status\":\n",
    "            if task_hash in self._task_plans:\n",
    "                plan = self._task_plans[task_hash]\n",
    "                return {\"progress\": f\"{len(plan['completed'])}/{len(plan['steps'])}\"}\n",
    "            return {\"error\": \"Task not found\"}\n",
    "        \n",
    "        return {\"error\": \"Invalid action\"}\n",
    "\n",
    "    def _break_down_task(self, task: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Break down a complex task into smaller steps\"\"\"\n",
    "        # Simple heuristic-based task breakdown\n",
    "        task_lower = task.lower()\n",
    "        steps = []\n",
    "        \n",
    "        # Common patterns for different types of tasks\n",
    "        if \"create\" in task_lower and (\"app\" in task_lower or \"application\" in task_lower):\n",
    "            steps = [\n",
    "                {\"type\": \"fs_write\", \"description\": \"Create project directory structure\", \"action\": \"setup_project\"},\n",
    "                {\"type\": \"fs_write\", \"description\": \"Create main application file\", \"action\": \"create_main\"},\n",
    "                {\"type\": \"fs_write\", \"description\": \"Create configuration files\", \"action\": \"create_config\"},\n",
    "                {\"type\": \"fs_write\", \"description\": \"Create requirements/dependencies file\", \"action\": \"create_deps\"},\n",
    "                {\"type\": \"execute_bash\", \"description\": \"Install dependencies\", \"action\": \"install_deps\"},\n",
    "                {\"type\": \"fs_write\", \"description\": \"Create basic tests\", \"action\": \"create_tests\"},\n",
    "                {\"type\": \"execute_bash\", \"description\": \"Run tests to verify setup\", \"action\": \"run_tests\"}\n",
    "            ]\n",
    "        elif \"deploy\" in task_lower:\n",
    "            steps = [\n",
    "                {\"type\": \"fs_write\", \"description\": \"Create Dockerfile\", \"action\": \"create_dockerfile\"},\n",
    "                {\"type\": \"fs_write\", \"description\": \"Create docker-compose.yml\", \"action\": \"create_compose\"},\n",
    "                {\"type\": \"execute_bash\", \"description\": \"Build Docker image\", \"action\": \"build_image\"},\n",
    "                {\"type\": \"execute_bash\", \"description\": \"Test container locally\", \"action\": \"test_container\"},\n",
    "                {\"type\": \"fs_write\", \"description\": \"Create deployment manifests\", \"action\": \"create_manifests\"},\n",
    "                {\"type\": \"execute_bash\", \"description\": \"Deploy to environment\", \"action\": \"deploy\"}\n",
    "            ]\n",
    "        elif \"api\" in task_lower:\n",
    "            steps = [\n",
    "                {\"type\": \"fs_write\", \"description\": \"Create API structure\", \"action\": \"create_api_structure\"},\n",
    "                {\"type\": \"fs_write\", \"description\": \"Define data models\", \"action\": \"create_models\"},\n",
    "                {\"type\": \"fs_write\", \"description\": \"Create API endpoints\", \"action\": \"create_endpoints\"},\n",
    "                {\"type\": \"fs_write\", \"description\": \"Add authentication\", \"action\": \"add_auth\"},\n",
    "                {\"type\": \"fs_write\", \"description\": \"Create API documentation\", \"action\": \"create_docs\"},\n",
    "                {\"type\": \"execute_bash\", \"description\": \"Test API endpoints\", \"action\": \"test_api\"}\n",
    "            ]\n",
    "        else:\n",
    "            # Generic breakdown\n",
    "            steps = [\n",
    "                {\"type\": \"fs_read\", \"description\": \"Analyze current state\", \"action\": \"analyze\"},\n",
    "                {\"type\": \"fs_write\", \"description\": \"Create necessary files\", \"action\": \"create_files\"},\n",
    "                {\"type\": \"execute_bash\", \"description\": \"Execute required commands\", \"action\": \"execute_commands\"},\n",
    "                {\"type\": \"fs_read\", \"description\": \"Verify results\", \"action\": \"verify\"}\n",
    "            ]\n",
    "        \n",
    "        return steps\n",
    "\n",
    "    def _execute_step(self, step: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Execute a single step of a task\"\"\"\n",
    "        step_type = step.get(\"type\")\n",
    "        description = step.get(\"description\")\n",
    "        action = step.get(\"action\")\n",
    "        \n",
    "        try:\n",
    "            if step_type == \"fs_write\":\n",
    "                # Create a placeholder file or directory based on the action\n",
    "                if action == \"setup_project\":\n",
    "                    self._fs_write(\"create\", \"./project/README.md\", file_text=\"# Project\\n\\nGenerated by todo tool\")\n",
    "                    return {\"success\": True, \"message\": \"Project structure created\"}\n",
    "                elif action == \"create_main\":\n",
    "                    self._fs_write(\"create\", \"./project/main.py\", file_text=\"#!/usr/bin/env python3\\n# Main application file\\n\\nif __name__ == '__main__':\\n    print('Hello World')\")\n",
    "                    return {\"success\": True, \"message\": \"Main file created\"}\n",
    "                else:\n",
    "                    return {\"success\": True, \"message\": f\"Step '{description}' planned for execution\"}\n",
    "            \n",
    "            elif step_type == \"execute_bash\":\n",
    "                if action == \"install_deps\":\n",
    "                    result = self._execute_bash(\"echo 'Dependencies would be installed here'\")\n",
    "                    return result\n",
    "                elif action == \"run_tests\":\n",
    "                    result = self._execute_bash(\"echo 'Tests would run here'\")\n",
    "                    return result\n",
    "                else:\n",
    "                    return {\"success\": True, \"message\": f\"Command '{description}' planned for execution\"}\n",
    "            \n",
    "            elif step_type == \"fs_read\":\n",
    "                return {\"success\": True, \"message\": f\"Analysis step '{description}' completed\"}\n",
    "            \n",
    "            else:\n",
    "                return {\"success\": True, \"message\": f\"Step '{description}' completed\"}\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Failed to execute step: {str(e)}\"}\n",
    "\n",
    "    def _code_interpreter(self, code: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Execute Python code with visualization and result capture\"\"\"\n",
    "        language = kwargs.get(\"language\", \"python\")\n",
    "        save_plots = kwargs.get(\"save_plots\", True)\n",
    "        timeout = kwargs.get(\"timeout\", 30)\n",
    "        working_dir = kwargs.get(\"working_dir\", os.getcwd())\n",
    "        \n",
    "        if language != \"python\":\n",
    "            return {\"error\": \"Only Python is currently supported\"}\n",
    "        \n",
    "        original_cwd = os.getcwd()\n",
    "        plot_files = []\n",
    "        \n",
    "        try:\n",
    "            os.chdir(working_dir)\n",
    "            stdout_capture = io.StringIO()\n",
    "            stderr_capture = io.StringIO()\n",
    "            \n",
    "            # Prepare code with plot saving\n",
    "            if save_plots:\n",
    "                plot_setup = '''\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "_plot_counter = 0\n",
    "\n",
    "def _save_plot():\n",
    "    global _plot_counter\n",
    "    if plt.get_fignums():\n",
    "        filename = f'plot_{_plot_counter}_{uuid.uuid4().hex[:8]}.png'\n",
    "        plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "        _plot_counter += 1\n",
    "        return filename\n",
    "    return None\n",
    "\n",
    "_original_show = plt.show\n",
    "def _custom_show(*args, **kwargs):\n",
    "    filename = _save_plot()\n",
    "    if filename:\n",
    "        print(f'Plot saved: {filename}')\n",
    "    plt.close('all')\n",
    "plt.show = _custom_show\n",
    "'''\n",
    "                code = plot_setup + \"\\n\" + code\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            with contextlib.redirect_stdout(stdout_capture), contextlib.redirect_stderr(stderr_capture):\n",
    "                exec_globals = {'__name__': '__main__', '__builtins__': __builtins__}\n",
    "                exec(code, exec_globals)\n",
    "            \n",
    "            execution_time = time.time() - start_time\n",
    "            stdout_content = stdout_capture.getvalue()\n",
    "            stderr_content = stderr_capture.getvalue()\n",
    "            \n",
    "            # Find plot files\n",
    "            current_plots = [f for f in os.listdir('.') if f.startswith('plot_') and f.endswith('.png')]\n",
    "            plot_files.extend(current_plots)\n",
    "            \n",
    "            # Encode plots\n",
    "            plot_data = []\n",
    "            for plot_file in plot_files:\n",
    "                if os.path.exists(plot_file):\n",
    "                    with open(plot_file, 'rb') as f:\n",
    "                        plot_data.append({\n",
    "                            \"filename\": plot_file,\n",
    "                            \"data\": base64.b64encode(f.read()).decode('utf-8'),\n",
    "                            \"size\": os.path.getsize(plot_file)\n",
    "                        })\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"execution_time\": round(execution_time, 3),\n",
    "                \"stdout\": stdout_content,\n",
    "                \"stderr\": stderr_content,\n",
    "                \"plots\": plot_data,\n",
    "                \"plot_count\": len(plot_files)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Execution failed: {str(e)}\"}\n",
    "        finally:\n",
    "            os.chdir(original_cwd)\n",
    "\n",
    "    def _repo_quality_analyzer(self, repo_path: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive repository code quality analysis\"\"\"\n",
    "        analysis_type = kwargs.get(\"analysis_type\", \"full\")\n",
    "        \n",
    "        repo_path = Path(repo_path)\n",
    "        if not repo_path.exists():\n",
    "            return {\"error\": f\"Repository path does not exist: {repo_path}\"}\n",
    "        \n",
    "        # Discover Python files\n",
    "        python_files = list(repo_path.rglob(\"*.py\"))\n",
    "        \n",
    "        analysis = {\n",
    "            \"repository\": str(repo_path),\n",
    "            \"files_analyzed\": len(python_files),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "        \n",
    "        # Basic metrics\n",
    "        total_lines = 0\n",
    "        total_functions = 0\n",
    "        security_issues = []\n",
    "        \n",
    "        for file_path in python_files:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    lines = content.splitlines()\n",
    "                    total_lines += len(lines)\n",
    "                \n",
    "                # Security check\n",
    "                for line_num, line in enumerate(lines, 1):\n",
    "                    if any(pattern in line.lower() for pattern in ['password =', 'api_key =', 'secret =']):\n",
    "                        security_issues.append({\n",
    "                            \"file\": str(file_path),\n",
    "                            \"line\": line_num,\n",
    "                            \"issue\": \"Potential hardcoded secret\"\n",
    "                        })\n",
    "                \n",
    "                # AST analysis\n",
    "                try:\n",
    "                    tree = ast.parse(content)\n",
    "                    for node in ast.walk(tree):\n",
    "                        if isinstance(node, ast.FunctionDef):\n",
    "                            total_functions += 1\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        # Generate recommendations\n",
    "        if len(security_issues) > 0:\n",
    "            analysis[\"recommendations\"].append({\n",
    "                \"category\": \"security\",\n",
    "                \"severity\": \"high\",\n",
    "                \"title\": f\"Found {len(security_issues)} potential security issues\",\n",
    "                \"action\": \"Review hardcoded secrets and use environment variables\"\n",
    "            })\n",
    "        \n",
    "        if total_lines > 10000:\n",
    "            analysis[\"recommendations\"].append({\n",
    "                \"category\": \"maintainability\",\n",
    "                \"severity\": \"medium\", \n",
    "                \"title\": \"Large codebase detected\",\n",
    "                \"action\": \"Consider modularization and documentation\"\n",
    "            })\n",
    "        \n",
    "        analysis[\"summary\"] = {\n",
    "            \"total_lines\": total_lines,\n",
    "            \"total_functions\": total_functions,\n",
    "            \"security_issues\": len(security_issues),\n",
    "            \"quality_score\": max(0, 100 - len(security_issues) * 10)\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def _documentation_generator(self, repo_path: str, doc_type: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Generate enhanced documentation using nbdev_preview and HTML enhancement\"\"\"\n",
    "        output_format = kwargs.get(\"output_format\", \"html\")\n",
    "        output_path = kwargs.get(\"output_path\", \"docs\")\n",
    "        include_examples = kwargs.get(\"include_examples\", True)\n",
    "        target_audience = kwargs.get(\"target_audience\", \"developers\")\n",
    "        \n",
    "        repo_path = Path(repo_path)\n",
    "        if not repo_path.exists():\n",
    "            return {\"error\": f\"Repository path does not exist: {repo_path}\"}\n",
    "        \n",
    "        try:\n",
    "            # Check if it's an nbdev project\n",
    "            if not (repo_path / \"settings.ini\").exists() and not (repo_path / \"nbs\").exists():\n",
    "                return {\"error\": \"Not an nbdev project. settings.ini or nbs folder not found.\"}\n",
    "            \n",
    "            original_cwd = os.getcwd()\n",
    "            os.chdir(repo_path)\n",
    "            \n",
    "            # Run nbdev_preview to generate base documentation\n",
    "            result = subprocess.run(\n",
    "                [\"nbdev_preview\", \"--port\", \"0\", \"--no_browser\"], \n",
    "                capture_output=True, text=True, timeout=60\n",
    "            )\n",
    "            \n",
    "            if result.returncode != 0:\n",
    "                return {\"error\": f\"nbdev_preview failed: {result.stderr}\"}\n",
    "            \n",
    "            # Find generated _docs directory\n",
    "            docs_dir = repo_path / \"_docs\"\n",
    "            if not docs_dir.exists():\n",
    "                return {\"error\": \"_docs directory not found after nbdev_preview\"}\n",
    "            \n",
    "            # Analyze repository for enhancements\n",
    "            repo_analysis = self._analyze_nbdev_repo(repo_path)\n",
    "            \n",
    "            # Enhance HTML files\n",
    "            enhanced_files = self._enhance_html_files(docs_dir, repo_analysis, include_examples, target_audience)\n",
    "            \n",
    "            # Copy enhanced docs to output path if different\n",
    "            if output_path != \"_docs\":\n",
    "                output_dir = Path(output_path)\n",
    "                if output_dir.exists():\n",
    "                    shutil.rmtree(output_dir)\n",
    "                shutil.copytree(docs_dir, output_dir)\n",
    "                final_docs_dir = output_dir\n",
    "            else:\n",
    "                final_docs_dir = docs_dir\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"doc_type\": doc_type,\n",
    "                \"output_directory\": str(final_docs_dir),\n",
    "                \"enhanced_files\": enhanced_files,\n",
    "                \"base_generator\": \"nbdev_preview\",\n",
    "                \"enhancements_applied\": len(enhanced_files),\n",
    "                \"target_audience\": target_audience\n",
    "            }\n",
    "            \n",
    "        except subprocess.TimeoutExpired:\n",
    "            return {\"error\": \"nbdev_preview timed out\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Documentation generation failed: {str(e)}\"}\n",
    "        finally:\n",
    "            os.chdir(original_cwd)\n",
    "\n",
    "    def _analyze_nbdev_repo(self, repo_path: Path) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze nbdev repository structure\"\"\"\n",
    "        analysis = {\n",
    "            \"name\": repo_path.name,\n",
    "            \"notebooks\": [],\n",
    "            \"modules\": [],\n",
    "            \"functions\": [],\n",
    "            \"classes\": [],\n",
    "            \"examples\": [],\n",
    "            \"settings\": {}\n",
    "        }\n",
    "        \n",
    "        # Read settings.ini\n",
    "        settings_file = repo_path / \"settings.ini\"\n",
    "        if settings_file.exists():\n",
    "            try:\n",
    "                with open(settings_file, 'r') as f:\n",
    "                    for line in f:\n",
    "                        if '=' in line and not line.strip().startswith('#'):\n",
    "                            key, value = line.strip().split('=', 1)\n",
    "                            analysis[\"settings\"][key.strip()] = value.strip()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Analyze notebooks\n",
    "        nbs_dir = repo_path / \"nbs\"\n",
    "        if nbs_dir.exists():\n",
    "            for nb_file in nbs_dir.rglob(\"*.ipynb\"):\n",
    "                try:\n",
    "                    with open(nb_file, 'r', encoding='utf-8') as f:\n",
    "                        nb_content = json.load(f)\n",
    "                    \n",
    "                    nb_info = {\n",
    "                        \"name\": nb_file.name,\n",
    "                        \"path\": str(nb_file.relative_to(repo_path)),\n",
    "                        \"cells\": len(nb_content.get(\"cells\", [])),\n",
    "                        \"exports\": [],\n",
    "                        \"examples\": []\n",
    "                    }\n",
    "                    \n",
    "                    # Extract exports and examples\n",
    "                    for cell in nb_content.get(\"cells\", []):\n",
    "                        if cell.get(\"cell_type\") == \"code\":\n",
    "                            source = \"\".join(cell.get(\"source\", []))\n",
    "                            if \"# | export\" in source:\n",
    "                                nb_info[\"exports\"].append(source)\n",
    "                            if any(keyword in source.lower() for keyword in [\"example\", \"demo\", \"usage\"]):\n",
    "                                nb_info[\"examples\"].append(source)\n",
    "                    \n",
    "                    analysis[\"notebooks\"].append(nb_info)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def _enhance_html_files(self, docs_dir: Path, analysis: Dict[str, Any], include_examples: bool, target_audience: str) -> List[str]:\n",
    "        \"\"\"Enhance generated HTML files with additional content\"\"\"\n",
    "        enhanced_files = []\n",
    "        \n",
    "        for html_file in docs_dir.rglob(\"*.html\"):\n",
    "            try:\n",
    "                with open(html_file, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # Parse HTML\n",
    "                from html.parser import HTMLParser\n",
    "                \n",
    "                # Add enhancements\n",
    "                enhanced_content = self._add_html_enhancements(content, analysis, include_examples, target_audience, html_file.name)\n",
    "                \n",
    "                # Write enhanced content\n",
    "                with open(html_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(enhanced_content)\n",
    "                \n",
    "                enhanced_files.append(str(html_file))\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        return enhanced_files\n",
    "\n",
    "    def _add_html_enhancements(self, content: str, analysis: Dict[str, Any], include_examples: bool, target_audience: str, filename: str) -> str:\n",
    "        \"\"\"Add enhancements to HTML content\"\"\"\n",
    "        \n",
    "        # Add custom CSS for better styling\n",
    "        custom_css = \"\"\"\n",
    "<style>\n",
    ".enhancement-box {\n",
    "    background: #f8f9fa;\n",
    "    border-left: 4px solid #007bff;\n",
    "    padding: 15px;\n",
    "    margin: 20px 0;\n",
    "    border-radius: 4px;\n",
    "}\n",
    ".example-code {\n",
    "    background: #f4f4f4;\n",
    "    border: 1px solid #ddd;\n",
    "    border-radius: 4px;\n",
    "    padding: 10px;\n",
    "    margin: 10px 0;\n",
    "    font-family: 'Courier New', monospace;\n",
    "}\n",
    ".audience-note {\n",
    "    background: #e7f3ff;\n",
    "    border: 1px solid #b3d9ff;\n",
    "    padding: 10px;\n",
    "    margin: 15px 0;\n",
    "    border-radius: 4px;\n",
    "}\n",
    ".repo-stats {\n",
    "    display: flex;\n",
    "    gap: 20px;\n",
    "    margin: 20px 0;\n",
    "}\n",
    ".stat-item {\n",
    "    background: #fff;\n",
    "    border: 1px solid #ddd;\n",
    "    padding: 10px;\n",
    "    border-radius: 4px;\n",
    "    text-align: center;\n",
    "    min-width: 100px;\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "        \n",
    "        # Insert CSS before closing head tag\n",
    "        if \"</head>\" in content:\n",
    "            content = content.replace(\"</head>\", custom_css + \"\\n</head>\")\n",
    "        \n",
    "        # Add repository statistics for index page\n",
    "        if filename == \"index.html\":\n",
    "            repo_stats = f\"\"\"\n",
    "<div class=\"enhancement-box\">\n",
    "    <h3>📊 Repository Statistics</h3>\n",
    "    <div class=\"repo-stats\">\n",
    "        <div class=\"stat-item\">\n",
    "            <strong>{len(analysis['notebooks'])}</strong><br>\n",
    "            <small>Notebooks</small>\n",
    "        </div>\n",
    "        <div class=\"stat-item\">\n",
    "            <strong>{sum(len(nb['exports']) for nb in analysis['notebooks'])}</strong><br>\n",
    "            <small>Exports</small>\n",
    "        </div>\n",
    "        <div class=\"stat-item\">\n",
    "            <strong>{sum(nb['cells'] for nb in analysis['notebooks'])}</strong><br>\n",
    "            <small>Total Cells</small>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "            \n",
    "            # Insert after first h1 or at beginning of body\n",
    "            if \"<h1\" in content:\n",
    "                h1_end = content.find(\"</h1>\") + 5\n",
    "                content = content[:h1_end] + repo_stats + content[h1_end:]\n",
    "            elif \"<body\" in content:\n",
    "                body_start = content.find(\">\", content.find(\"<body\")) + 1\n",
    "                content = content[:body_start] + repo_stats + content[body_start:]\n",
    "        \n",
    "        # Add audience-specific notes\n",
    "        audience_note = f\"\"\"\n",
    "<div class=\"audience-note\">\n",
    "    <strong>👥 Target Audience:</strong> This documentation is optimized for <em>{target_audience}</em>.\n",
    "    {\"Includes practical examples and usage patterns.\" if include_examples else \"Focuses on API reference and technical details.\"}\n",
    "</div>\n",
    "\"\"\"\n",
    "        \n",
    "        # Insert audience note after main content starts\n",
    "        if '<div class=\"cell border-box-sizing text_cell rendered\">' in content:\n",
    "            first_cell = content.find('<div class=\"cell border-box-sizing text_cell rendered\">')\n",
    "            content = content[:first_cell] + audience_note + content[first_cell:]\n",
    "        \n",
    "        # Add quick navigation for notebooks\n",
    "        if len(analysis['notebooks']) > 1:\n",
    "            nav_links = ['<div class=\"enhancement-box\">', '<h4>📚 Quick Navigation</h4>', '<ul>']\n",
    "            \n",
    "            for nb in analysis['notebooks']:\n",
    "                nb_name = nb['name'].replace('.ipynb', '')\n",
    "                nav_links.append(f'<li><a href=\"{nb_name}.html\">{nb_name}</a> ({nb[\"cells\"]} cells)</li>')\n",
    "            \n",
    "            nav_links.extend(['</ul>', '</div>'])\n",
    "            navigation = '\\n'.join(nav_links)\n",
    "            \n",
    "            # Insert navigation\n",
    "            if \"<body\" in content:\n",
    "                body_start = content.find(\">\", content.find(\"<body\")) + 1\n",
    "                content = content[:body_start] + navigation + content[body_start:]\n",
    "        \n",
    "        # Add examples section if requested\n",
    "        if include_examples:\n",
    "            examples_found = []\n",
    "            for nb in analysis['notebooks']:\n",
    "                examples_found.extend(nb['examples'])\n",
    "            \n",
    "            if examples_found:\n",
    "                examples_section = ['<div class=\"enhancement-box\">', '<h4>💡 Usage Examples</h4>']\n",
    "                \n",
    "                for i, example in enumerate(examples_found[:3]):  # Limit to 3 examples\n",
    "                    clean_example = example.replace('# | export\\n', '').strip()\n",
    "                    if clean_example:\n",
    "                        examples_section.append(f'<div class=\"example-code\"><pre><code>{clean_example}</code></pre></div>')\n",
    "                \n",
    "                examples_section.append('</div>')\n",
    "                examples_html = '\\n'.join(examples_section)\n",
    "                \n",
    "                # Insert before closing body\n",
    "                if \"</body>\" in content:\n",
    "                    content = content.replace(\"</body>\", examples_html + \"\\n</body>\")\n",
    "        \n",
    "        # Add footer with generation info\n",
    "        footer = f\"\"\"\n",
    "<div style=\"margin-top: 40px; padding: 20px; background: #f8f9fa; border-top: 1px solid #ddd; font-size: 0.9em; color: #666;\">\n",
    "    <p>📝 Documentation generated by <strong>nbdev_preview</strong> and enhanced by <strong>Buddy AI Agent</strong></p>\n",
    "    <p>🕒 Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "    <p>🎯 Optimized for: {target_audience}</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "        \n",
    "        if \"</body>\" in content:\n",
    "            content = content.replace(\"</body>\", footer + \"\\n</body>\")\n",
    "        \n",
    "        return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bae08d-c5f5-4a31-898c-539c11cba0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974e507-82e9-40e2-bb35-b80c9b806314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
