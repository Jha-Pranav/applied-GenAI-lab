{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991e13d3-9bfa-40d6-8c2f-542a2ed9ddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp agent.planner.validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca54c1d-3f35-4939-8c02-ef008ef3cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import re\n",
    "from typing import List\n",
    "from rich.console import Console\n",
    "\n",
    "from agentic.tools.introspect import IntrospectTool\n",
    "from agentic.tools.base import ToolMetadata, ToolCategory\n",
    "\n",
    "from agentic.agent.planner.models import Task, IntrospectionResult, ProjectBreakdown, ProjectContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75772852-1d77-4870-9828-8803b5c3ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class TaskValidator:\n",
    "    \"\"\"Handles task validation and consistency checks\"\"\"\n",
    "    \n",
    "    def __init__(self, console: Console):\n",
    "        self.console = console\n",
    "        self.introspect = IntrospectTool(\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"introspect\",\n",
    "                description=\"Task validation and feedback tool\",\n",
    "                category=ToolCategory.INTELLIGENCE\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def validate_task_consistency(self, task: Task) -> IntrospectionResult:\n",
    "        \"\"\"Validate internal consistency of task fields\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        user_prompts = \" \".join([action.user_prompt for action in task.actions])\n",
    "        system_prompts = \" \".join([action.system_prompt for action in task.actions])\n",
    "        success_criteria = task.success_criteria\n",
    "        expected_outputs = task.expected_outputs\n",
    "        \n",
    "        if not isinstance(expected_outputs, list):\n",
    "            expected_outputs = []\n",
    "        \n",
    "        files_in_criteria = re.findall(r'\\b\\w+\\.\\w+\\b', success_criteria)\n",
    "        \n",
    "        for file in files_in_criteria:\n",
    "            covered = any(\n",
    "                file in expected_outputs or\n",
    "                any(output.endswith('/') and file in success_criteria for output in expected_outputs)\n",
    "            )\n",
    "            if not covered:\n",
    "                issues.append(f\"Success criteria mentions '{file}' but it's not covered in expected_outputs\")\n",
    "        \n",
    "        for output in expected_outputs:\n",
    "            if output.endswith('/'):\n",
    "                if output.rstrip('/') not in user_prompts and output.rstrip('/') not in system_prompts:\n",
    "                    issues.append(f\"Expected directory '{output}' not mentioned in action prompts\")\n",
    "            else:\n",
    "                if output not in user_prompts and output not in system_prompts:\n",
    "                    issues.append(f\"Expected output '{output}' not mentioned in action prompts\")\n",
    "        \n",
    "        if not any(keyword in task.description.lower() for keyword in ['create', 'generate', 'build', 'implement', 'setup']):\n",
    "            if any('write' in action.purpose.lower() or 'create' in action.purpose.lower() for action in task.actions):\n",
    "                issues.append(\"Task uses creation actions but description doesn't indicate creation/setup\")\n",
    "        \n",
    "        success = len(issues) == 0\n",
    "        feedback = f\"Task consistency issues: {'; '.join(issues)}\" if issues else \"Task fields are consistent\"\n",
    "        \n",
    "        return IntrospectionResult(\n",
    "            success=success,\n",
    "            score=10.0 if success else 5.0,\n",
    "            feedback=feedback,\n",
    "            next_action=\"regenerate\" if not success else \"proceed\",\n",
    "            recommendations=[\"Ensure file names match across all task fields\", \"Include specific files in expected_outputs if mentioned in success_criteria\"] if issues else []\n",
    "        )\n",
    "    \n",
    "    def introspect_task_planning(self, task: Task, breakdown: ProjectBreakdown, context: ProjectContext) -> IntrospectionResult:\n",
    "        \"\"\"Pre-execution validation of task planning\"\"\"\n",
    "        \n",
    "        consistency_result = self.validate_task_consistency(task)\n",
    "        if not consistency_result.success:\n",
    "            return consistency_result\n",
    "        \n",
    "        try:\n",
    "            introspect_result = self.introspect.execute(\n",
    "                action=\"validate\",\n",
    "                validation_context={\n",
    "                    \"proposed_task\": task.name,\n",
    "                    \"task_description\": task.description,\n",
    "                    \"project_breakdown\": breakdown.project_summary,\n",
    "                    \"execution_history\": [t.task_name for t in context.execution_history],\n",
    "                    \"current_artifacts\": context.current_artifacts,\n",
    "                    \"validation_type\": \"task_planning\"\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            if introspect_result and 'performance_score' in introspect_result:\n",
    "                score = introspect_result['performance_score']\n",
    "                success = score >= 7\n",
    "                feedback = introspect_result.get('feedback_for_retry', 'Task planning looks good')\n",
    "                next_action = \"regenerate\" if not success else \"proceed\"\n",
    "                \n",
    "                return IntrospectionResult(\n",
    "                    success=success,\n",
    "                    score=score,\n",
    "                    feedback=feedback,\n",
    "                    next_action=next_action,\n",
    "                    recommendations=introspect_result.get('recommendations', [])\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"⚠️ Pre-execution introspection error: {e}\")\n",
    "        \n",
    "        return IntrospectionResult(\n",
    "            success=True,\n",
    "            score=7.0,\n",
    "            feedback=\"Pre-execution validation unavailable\",\n",
    "            next_action=\"proceed\",\n",
    "            recommendations=[]\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
