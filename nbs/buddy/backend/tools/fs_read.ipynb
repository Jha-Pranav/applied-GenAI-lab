{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c01dc636-3b46-48f2-9d95-5e29198f50e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp backend.tools.fs_read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2cb2d1-8b7a-4bc1-aae5-b369afcc7f20",
   "metadata": {},
   "source": [
    "https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/git-aware-selection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60257385-ef59-4b8e-8298-4e070c6a7599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional, Literal\n",
    "from fuzzywuzzy import fuzz\n",
    "import fnmatch\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pydantic import BaseModel, field_validator, Field, ValidationInfo\n",
    "from enum import Enum\n",
    "\n",
    "try:\n",
    "    import git\n",
    "except ImportError:\n",
    "    git = None\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ToolCallMode(str, Enum):\n",
    "    DISCOVER = \"discover\"\n",
    "    EXTRACT = \"extract\"\n",
    "\n",
    "class FsReadOperation(BaseModel):\n",
    "    mode: ToolCallMode\n",
    "    path: str = Field(..., description=\"Specific file or directory path to operate on, e.g., '/project/src' or 'app.py'. Mandatory for precision; scans from here. Defaults to current working directory (project root) if empty, but always specify to avoid broad scans.\")\n",
    "    query: Optional[str] = Field(None, description=\"Search query: For DISCOVER, fuzzy-matches filenames/content for relevance; for EXTRACT, regex or substring to find matches (regex default). Required for EXTRACT; optional for DISCOVER (glob-only if absent).\")\n",
    "    file_pattern: str = Field(\"*.py|*.go|*.rs|*.java|*.js|*.ts|*.cpp|*.c|*.h|*.cs\", description=\"Glob filter for files, e.g., '*.py' or '*.go|*.rs'. Defaults to common source file extensions (*.py|*.go|*.rs|*.java|*.js|*.ts|*.cpp|*.c|*.h|*.cs) in EXTRACT mode to process code files; '*' in DISCOVER mode for broader searches. Specify to optimize scans.\")\n",
    "    max_depth: Optional[int] = Field(10, description=\"Max recursion depth for DISCOVER in directories (default 10); set lower (e.g., 3) for flat projects to skip deep dirs like node_modules.\")\n",
    "    context_lines: Optional[int] = Field(3, description=\"Lines to include before/after each match in EXTRACT (default 3); e.g., 5 for full function context in code.\")\n",
    "    regex_mode: Optional[bool] = Field(True, description=\"True (default) treats query as regex in EXTRACT, e.g., r'def\\\\s+login'; false for simple text search.\")\n",
    "    start_line: Optional[int] = Field(1, description=\"Fallback start line for EXTRACT if no matches (default 1); for partial file reads.\")\n",
    "    end_line: Optional[int] = Field(-1, description=\"Fallback end line for EXTRACT (-1 = file end); combines with start_line for targeted reads.\")\n",
    "    max_files: Optional[int] = Field(100, description=\"Max files to scan in DISCOVER (default 100); caps computation in large repos for <5s response.\")\n",
    "    limit_results: Optional[int] = Field(5, description=\"Max results from DISCOVER (top N by score, default 5); keeps outputs lean for tools like code_interpreter.\")\n",
    "\n",
    "    @field_validator(\"path\")\n",
    "    @classmethod\n",
    "    def validate_path(cls, value: str) -> str:\n",
    "        if not value:\n",
    "            value = os.getcwd()\n",
    "            logger.info(f\"Path empty; defaulting to project root: {value}\")\n",
    "        path_obj = Path(value)\n",
    "        if not path_obj.exists():\n",
    "            logger.warning(f\"Path {value} does not exist; returning empty results.\")\n",
    "        return str(path_obj)\n",
    "\n",
    "    @field_validator(\"query\")\n",
    "    @classmethod\n",
    "    def validate_query(cls, value: Any, info: ValidationInfo) -> Any:\n",
    "        mode = info.data.get(\"mode\")\n",
    "        if mode == ToolCallMode.EXTRACT and value is None:\n",
    "            raise ValueError(\"Query is required for extract mode to perform regex or substring matching.\")\n",
    "        if mode == ToolCallMode.DISCOVER and value is None:\n",
    "            return \"\"\n",
    "        return value\n",
    "\n",
    "    @field_validator(\"file_pattern\")\n",
    "    @classmethod\n",
    "    def validate_file_pattern(cls, value: Any, info: ValidationInfo) -> Any:\n",
    "        mode = info.data.get(\"mode\")\n",
    "        if value is None:\n",
    "            return \"*.py|*.txt|*.md|*.yaml|*.json\" if mode == ToolCallMode.EXTRACT else \"*\"\n",
    "        if not isinstance(value, str) or ('*' not in value and '?' not in value and '|' not in value):\n",
    "            raise ValueError(\"file_pattern must be a valid glob, e.g., '*.py' or '*.py|*.txt'\")\n",
    "        return value\n",
    "\n",
    "class FsReadParams(BaseModel):\n",
    "    operations: List[FsReadOperation]\n",
    "\n",
    "class FsReadTool:\n",
    "    def __init__(self):\n",
    "        self.repo = None\n",
    "        self.gitignore_patterns = []\n",
    "        self.git_status_cache = {}\n",
    "        self.query_cache = {}\n",
    "        self.common_exclusions = [\n",
    "            \".*\", \"*.pyc\", \"*.o\", \"*.obj\", \"*.class\", \"*.exe\", \"*.dll\", \"*.so\",\n",
    "            \"*.lock\", \"node_modules/*\", \"dist/*\", \"build/*\", \"__pycache__/*\",\n",
    "            \"*.bin\", \"*.zip\", \"*.tar.gz\", \"*.log\"\n",
    "        ]\n",
    "        if git:\n",
    "            try:\n",
    "                self.repo = git.Repo(os.getcwd(), search_parent_directories=True)\n",
    "                self.gitignore_patterns = self._load_gitignore()\n",
    "                self._batch_load_git_status()\n",
    "            except (git.InvalidGitRepositoryError, git.NoSuchPathError):\n",
    "                pass\n",
    "\n",
    "    def _load_gitignore(self) -> List[str]:\n",
    "        \"\"\"Load .gitignore patterns recursively from repo root and subdirs.\"\"\"\n",
    "        patterns = self.common_exclusions.copy()\n",
    "        try:\n",
    "            repo_root = Path(self.repo.working_dir)\n",
    "            for gitignore in repo_root.rglob(\".gitignore\"):\n",
    "                try:\n",
    "                    with open(gitignore, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        for line in f:\n",
    "                            line = line.strip()\n",
    "                            if line and not line.startswith('#'):\n",
    "                                rel_path = str(gitignore.parent.relative_to(repo_root)) + '/' + line\n",
    "                                if rel_path.startswith('./'):\n",
    "                                    rel_path = rel_path[2:]\n",
    "                                patterns.append(rel_path)\n",
    "                except (OSError, UnicodeDecodeError):\n",
    "                    logger.debug(f\"Failed to read {gitignore}\")\n",
    "        except (OSError, Exception):\n",
    "            logger.warning(\"Failed to load .gitignore patterns; using common exclusions only.\")\n",
    "        return patterns\n",
    "\n",
    "    def _batch_load_git_status(self):\n",
    "        \"\"\"Batch load Git status for all files.\"\"\"\n",
    "        try:\n",
    "            status_output = self.repo.git.status('--porcelain', '-u', 'all')\n",
    "            for line in status_output.split('\\n'):\n",
    "                if line.strip():\n",
    "                    parts = line.split(maxsplit=1)\n",
    "                    if len(parts) >= 2:\n",
    "                        status_code = parts[0][0]\n",
    "                        rel_path = parts[1].strip()\n",
    "                        self.git_status_cache[rel_path] = status_code if status_code != '?' else 'untracked'\n",
    "        except (OSError, Exception):\n",
    "            logger.warning(\"Failed to batch-load Git status; falling back to per-file.\")\n",
    "\n",
    "    def _get_file_info(self, path: str, lightweight: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Optimized file info with exclusions.\"\"\"\n",
    "        path_obj = Path(path)\n",
    "        if not path_obj.exists():\n",
    "            return {\"error\": \"File not found\"}\n",
    "\n",
    "        try:\n",
    "            stat = path_obj.stat()\n",
    "        except OSError:\n",
    "            return {\"error\": \"Stat failed\"}\n",
    "\n",
    "        rel_path = path_obj.name\n",
    "        if self.repo:\n",
    "            try:\n",
    "                rel_path = str(path_obj.relative_to(self.repo.working_dir))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if any(fnmatch.fnmatch(rel_path, p) for p in self.gitignore_patterns):\n",
    "            return {\"error\": \"Excluded by .gitignore or common patterns\"}\n",
    "\n",
    "        file_size = stat.st_size\n",
    "        mtime = stat.st_mtime\n",
    "        recency_boost = 1.0 if time.time() - mtime < 86400 else 0.5\n",
    "\n",
    "        file_info = {\n",
    "            \"size\": file_size,\n",
    "            \"is_binary\": False,\n",
    "            \"file_type\": path_obj.suffix.lower(),\n",
    "            \"is_large\": file_size > 1024 * 1024,\n",
    "            \"mtime\": mtime,\n",
    "            \"recency_boost\": recency_boost,\n",
    "            \"git_status\": None,\n",
    "        }\n",
    "\n",
    "        if self.repo:\n",
    "            rel_path = os.path.relpath(path, self.repo.working_dir)\n",
    "            file_info[\"git_status\"] = self.git_status_cache.get(rel_path, \"clean\")\n",
    "            if file_info[\"git_status\"] in [\"M\", \"A\", \"?\"]:\n",
    "                file_info[\"recency_boost\"] = 1.0\n",
    "\n",
    "        if lightweight:\n",
    "            return file_info\n",
    "\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                chunk = f.read(512)\n",
    "                if b'\\x00' in chunk:\n",
    "                    file_info[\"is_binary\"] = True\n",
    "                    return file_info\n",
    "        except (OSError, UnicodeDecodeError):\n",
    "            file_info[\"is_binary\"] = True\n",
    "            return file_info\n",
    "\n",
    "        try:\n",
    "            if file_size < 10 * 1024 * 1024:\n",
    "                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    file_info[\"lines\"] = sum(1 for _ in f)\n",
    "            else:\n",
    "                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    sample_lines = sum(1 for _, _ in zip(range(10000), f))\n",
    "                    file_info[\"lines\"] = int((file_size / (f.tell() or 1)) * sample_lines)\n",
    "        except (OSError, UnicodeDecodeError):\n",
    "            pass\n",
    "\n",
    "        return file_info\n",
    "\n",
    "    def _score_file_content(self, file_path: str, query_lower: str) -> float:\n",
    "        \"\"\"Parallel-friendly content scorer.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read(1024).lower()\n",
    "            return fuzz.partial_ratio(query_lower, content)\n",
    "        except (OSError, UnicodeDecodeError):\n",
    "            return 0.0\n",
    "    \n",
    "    def _discover_files(self, path: str, file_pattern: str, query: str, max_depth: int, max_files: int = 100, limit_results: int = 5, timeout: float = 10.0) -> List[Dict]:\n",
    "        \"\"\"Blazing-fast discovery with scandir, parallel scoring, and exclusions.\"\"\"\n",
    "        start_time = time.time()\n",
    "        path_obj = Path(path)\n",
    "        if not path_obj.is_dir():\n",
    "            return [{\"error\": f\"Path {path} is not a directory for discovery\"}]\n",
    "    \n",
    "        query_lower = self.query_cache.get(query, query.lower() if query else \"\")\n",
    "        if query and query not in self.query_cache:\n",
    "            self.query_cache[query] = query_lower\n",
    "    \n",
    "        candidates = []\n",
    "        # Support multiple patterns (e.g., *.py|*.txt)\n",
    "        patterns = file_pattern.split('|')\n",
    "    \n",
    "        def _collect_with_scandir(dir_path: str, current_depth: int, file_count: List[int]) -> None:\n",
    "            if current_depth > max_depth or file_count[0] >= max_files or time.time() - start_time > timeout:\n",
    "                return\n",
    "            try:\n",
    "                with os.scandir(dir_path) as entries:\n",
    "                    for entry in entries:\n",
    "                        if time.time() - start_time > timeout:\n",
    "                            logger.warning(f\"Discovery timeout after {timeout}s\")\n",
    "                            return\n",
    "                        rel_path = entry.name\n",
    "                        if self.repo:\n",
    "                            try:\n",
    "                                rel_path = str(Path(entry.path).relative_to(self.repo.working_dir))\n",
    "                            except ValueError:\n",
    "                                pass\n",
    "                        if any(fnmatch.fnmatch(rel_path, p) for p in self.gitignore_patterns):\n",
    "                            continue\n",
    "                        if any(fnmatch.fnmatch(entry.name, p) for p in patterns) and entry.is_file() and Path(entry.path).exists():\n",
    "                            file_info = self._get_file_info(entry.path, lightweight=True)\n",
    "                            if \"error\" in file_info or file_info.get(\"is_binary\"):\n",
    "                                continue\n",
    "                            base_score = 0\n",
    "                            if query:\n",
    "                                name_score = fuzz.partial_ratio(query_lower, entry.name.lower())\n",
    "                                base_score = name_score\n",
    "                                if name_score < 50 and not file_info.get(\"is_binary\"):\n",
    "                                    base_score = max(base_score, self._score_file_content(entry.path, query_lower))\n",
    "                            git_boost = 20 if file_info.get(\"git_status\") in [\"M\", \"A\"] else 10 if file_info.get(\"git_status\") == \"untracked\" else 0\n",
    "                            mtime_boost = 10 if file_info[\"recency_boost\"] > 0.8 else 0\n",
    "                            candidates.append({\n",
    "                                \"path\": entry.path,\n",
    "                                \"name\": entry.name,\n",
    "                                \"type\": \"file\",\n",
    "                                \"score\": base_score + git_boost + mtime_boost,\n",
    "                                \"file_info\": file_info\n",
    "                            })\n",
    "                            file_count[0] += 1\n",
    "                            if file_count[0] >= max_files:\n",
    "                                logger.info(f\"Reached max files: {max_files}\")\n",
    "                                return\n",
    "                        if entry.is_dir():\n",
    "                            _collect_with_scandir(entry.path, current_depth + 1, file_count)\n",
    "            except (OSError, PermissionError) as e:\n",
    "                logger.warning(f\"Scan error in {dir_path}: {e}\")\n",
    "    \n",
    "        file_count = [0]\n",
    "        _collect_with_scandir(str(path_obj), 0, file_count)\n",
    "    \n",
    "        if not candidates:\n",
    "            return [{\"error\": \"No files matched the criteria\"}]\n",
    "    \n",
    "        if query and len(candidates) > 10:\n",
    "            low_scorers = [c for c in candidates if c[\"score\"] < 50 and not c[\"file_info\"].get(\"is_binary\")]\n",
    "            if low_scorers:\n",
    "                with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "                    futures = {executor.submit(self._score_file_content, c[\"path\"], query_lower): c for c in low_scorers[:20]}\n",
    "                    for future in as_completed(futures):\n",
    "                        content_score = future.result()\n",
    "                        if content_score > 0:\n",
    "                            idx = low_scorers.index(futures[future])\n",
    "                            low_scorers[idx][\"score\"] = max(low_scorers[idx][\"score\"], content_score)\n",
    "    \n",
    "        candidates.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        found = []\n",
    "        for item in candidates[:limit_results]:\n",
    "            found.append({\n",
    "                \"path\": item[\"path\"],\n",
    "                \"name\": item[\"name\"],\n",
    "                \"type\": item[\"type\"],\n",
    "                \"score\": item[\"score\"],\n",
    "                \"git_status\": item[\"file_info\"].get(\"git_status\", \"N/A\"),\n",
    "                \"recency\": item[\"file_info\"][\"mtime\"]\n",
    "            })\n",
    "    \n",
    "        logger.info(f\"Discovered {len(found)} files in {time.time() - start_time:.2f}s (processed {file_count[0]})\")\n",
    "        return found\n",
    "    \n",
    "    def _extract_content(self, path: str, query: str, file_pattern: str, context_lines: int, regex_mode: bool, start_line: int, end_line: int) -> str:\n",
    "        \"\"\"Context-aware extraction with byte cap.\"\"\"\n",
    "        path_obj = Path(path)\n",
    "        if not path_obj.exists():\n",
    "            return json.dumps({\"error\": f\"Path {path} does not exist\"})\n",
    "    \n",
    "        if path_obj.is_dir():\n",
    "            snippets = []\n",
    "            discovered = self._discover_files(path, file_pattern, query, 1, max_files=10, limit_results=10)\n",
    "            for file_item in discovered:\n",
    "                if \"error\" not in file_item and isinstance(file_item.get(\"path\"), str) and Path(file_item[\"path\"]).is_file():\n",
    "                    file_info = self._get_file_info(file_item[\"path\"], lightweight=False)\n",
    "                    if \"error\" in file_info or file_info.get(\"is_binary\"):\n",
    "                        continue\n",
    "                    file_snip = self._extract_from_file(file_item[\"path\"], query, context_lines, regex_mode, start_line, end_line)\n",
    "                    if file_snip and not file_snip.startswith(\"[Binary or invalid file\") and not file_snip.startswith(\"Error reading file\"):\n",
    "                        snippets.append({\"file\": file_item[\"path\"], \"snippet\": file_snip})\n",
    "            if not snippets:\n",
    "                return json.dumps({\"error\": f\"No valid text files matched the criteria in {path}\"})\n",
    "            content = json.dumps(snippets)\n",
    "        else:\n",
    "            content = self._extract_from_file(str(path_obj), query, context_lines, regex_mode, start_line, end_line)\n",
    "    \n",
    "        if len(content) > 16 * 1024:\n",
    "            content = content[:16 * 1024] + \"\\n... [truncated]\"\n",
    "        return content\n",
    "    \n",
    "    def _extract_from_file(self, file_path: str, query: str, context_lines: int, regex_mode: bool, start_line: int, end_line: int) -> str:\n",
    "        \"\"\"Line-by-line extraction with byte cap.\"\"\"\n",
    "        if not isinstance(file_path, str) or not Path(file_path).is_file():\n",
    "            return f\"[Invalid file path: {file_path}]\"\n",
    "    \n",
    "        file_info = self._get_file_info(file_path, lightweight=False)\n",
    "        if \"error\" in file_info or file_info.get(\"is_binary\"):\n",
    "            return f\"[Binary or invalid file: {file_info.get('size', 0)} bytes]\"\n",
    "    \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                matches = []\n",
    "                pattern = None\n",
    "                if query and regex_mode:\n",
    "                    pattern = re.compile(query, re.IGNORECASE | re.MULTILINE)\n",
    "                elif query:\n",
    "                    query_lower = query.lower()\n",
    "    \n",
    "                current_line = start_line\n",
    "                byte_count = 0\n",
    "                line_buffer = []\n",
    "                for line_num, line in enumerate(f, start_line):\n",
    "                    if current_line > end_line and end_line != -1:\n",
    "                        break\n",
    "                    line_stripped = line.rstrip()\n",
    "                    byte_count += len(line_stripped.encode('utf-8'))\n",
    "                    if byte_count > 16 * 1024:\n",
    "                        break\n",
    "                    line_buffer.append((current_line, line_stripped))\n",
    "                    if len(line_buffer) > 2 * context_lines + 1:\n",
    "                        line_buffer.pop(0)\n",
    "    \n",
    "                    matched = False\n",
    "                    if pattern:\n",
    "                        if pattern.search(line):\n",
    "                            matched = True\n",
    "                    elif query:\n",
    "                        if query_lower in line.lower():\n",
    "                            matched = True\n",
    "    \n",
    "                    if matched:\n",
    "                        context_start = max(1, current_line - context_lines)\n",
    "                        context_end = current_line + context_lines + 1\n",
    "                        context_lines_content = [f\"Line {ln}: {text}\" for ln, text in line_buffer if context_start <= ln < context_end]\n",
    "                        matches.append({\n",
    "                            \"line_number\": current_line,\n",
    "                            \"content\": line_stripped,\n",
    "                            \"context\": \"\\n\".join(context_lines_content)\n",
    "                        })\n",
    "                    current_line += 1\n",
    "    \n",
    "                snippets = []\n",
    "                for match in matches:\n",
    "                    snippet = f\"Line {match['line_number']}: {match['content']}\\n{match['context']}\\n\"\n",
    "                    snippets.append(snippet)\n",
    "                content = '\\n--- Match ---\\n'.join(snippets) if matches else \"No matches found; fallback content truncated.\"\n",
    "        except (OSError, UnicodeDecodeError) as e:\n",
    "            return f\"Error reading file: {str(e)}\"\n",
    "    \n",
    "        git_info = file_info['git_status']\n",
    "        if not git_info:\n",
    "            git_info = f\"N/A (non-Git: mtime {file_info['mtime']})\"\n",
    "        \n",
    "        metadata = (\n",
    "            f\"\\n--- File Info: {file_info['size']} bytes, \"\n",
    "            f\"{file_info.get('lines', 0)} lines, Git: {git_info} ---\"\n",
    "        )\n",
    "        return content + metadata\n",
    "\n",
    "    def get_tool_schema(self) -> Dict[str, Any]:\n",
    "        \"\"\"Detailed schema for LLMs.\"\"\"\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"fs_read\",\n",
    "                \"description\": \"Blazing-fast tool for discovering relevant files (fuzzy/glob, respects .gitignore and excludes binaries/lock files) and extracting context-aware snippets (regex with surrounding lines). Optimized for <5s response in large repos; uses Git or mtime for prioritization. Ideal for chaining with fs_write, code_interpreter, or task_executor.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"operations\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"mode\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"enum\": [mode.value for mode in ToolCallMode],\n",
    "                                        \"description\": \"Select 'discover' to find/list top relevant files (fuzzy if query given, else glob-based); 'extract' to pull content snippets with context (regex or substring matches).\"\n",
    "                                    },\n",
    "                                    \"path\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"Mandatory file or directory path, e.g., '/project/src' or 'app.py'. Scans from here; defaults to project root if empty (avoid this for precision).\"\n",
    "                                    },\n",
    "                                    \"query\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"Search term: In DISCOVER, fuzzy-matches filenames/content; in EXTRACT, regex (e.g., r'def\\\\s+login') or substring. Required for EXTRACT; optional for DISCOVER.\"\n",
    "                                    },\n",
    "                                    \"file_pattern\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"Glob filter, e.g., '*.py' or 'config*.yaml'. Defaults to '*' (all files); use to focus scans.\"\n",
    "                                    },\n",
    "                                    \"max_depth\": {\n",
    "                                        \"type\": \"integer\",\n",
    "                                        \"description\": \"Max dir recursion for DISCOVER (default 10); lower (e.g., 3) for flat projects to skip node_modules.\"\n",
    "                                    },\n",
    "                                    \"context_lines\": {\n",
    "                                        \"type\": \"integer\",\n",
    "                                        \"description\": \"Lines around each match in EXTRACT (default 3); e.g., 5 for full code blocks.\"\n",
    "                                    },\n",
    "                                    \"regex_mode\": {\n",
    "                                        \"type\": \"boolean\",\n",
    "                                        \"description\": \"True (default) treats query as regex in EXTRACT; false for simple text search.\"\n",
    "                                    },\n",
    "                                    \"start_line\": {\n",
    "                                        \"type\": \"integer\",\n",
    "                                        \"description\": \"Fallback start line for EXTRACT if no matches (default 1).\"\n",
    "                                    },\n",
    "                                    \"end_line\": {\n",
    "                                        \"type\": \"integer\",\n",
    "                                        \"description\": \"Fallback end line for EXTRACT (-1 = file end).\"\n",
    "                                    },\n",
    "                                    \"max_files\": {\n",
    "                                        \"type\": \"integer\",\n",
    "                                        \"description\": \"Max files to scan in DISCOVER (default 100); ensures <5s response.\"\n",
    "                                    },\n",
    "                                    \"limit_results\": {\n",
    "                                        \"type\": \"integer\",\n",
    "                                        \"description\": \"Max results from DISCOVER (default 5); keeps outputs lean.\"\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\"mode\", \"path\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"operations\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def execute(self, params: FsReadParams) -> Dict[str, Any]:\n",
    "        \"\"\"Execute with chainable JSON output.\"\"\"\n",
    "        results = []\n",
    "        git_detected = self.repo is not None\n",
    "        total_processed = 0\n",
    "\n",
    "        for op in params.operations:\n",
    "            mode = op.mode.value\n",
    "            path = op.path\n",
    "            output_data = None\n",
    "            error = None\n",
    "\n",
    "            try:\n",
    "                if mode == \"discover\":\n",
    "                    output_data = self._discover_files(path, op.file_pattern, op.query or \"\", op.max_depth, op.max_files, op.limit_results)\n",
    "                    total_processed += len([x for x in output_data if \"error\" not in x])\n",
    "                elif mode == \"extract\":\n",
    "                    output_data = self._extract_content(path, op.query, op.file_pattern, op.context_lines, op.regex_mode, op.start_line, op.end_line)\n",
    "                    total_processed += 1\n",
    "            except (OSError, UnicodeDecodeError, Exception) as e:\n",
    "                error = {\"type\": \"ExecutionError\", \"message\": str(e)}\n",
    "                logger.error(f\"Error in {mode} on {path}: {str(e)}\")\n",
    "\n",
    "            results.append({\n",
    "                \"mode\": mode,\n",
    "                \"path\": path,\n",
    "                \"data\": output_data,\n",
    "                \"error\": error\n",
    "            })\n",
    "\n",
    "        return {\n",
    "            \"success\": all(r.get(\"error\") is None for r in results),\n",
    "            \"data\": results,\n",
    "            \"error\": None if all(r.get(\"error\") is None for r in results) else {\"type\": \"BatchError\", \"message\": \"Some operations failed—check individual errors.\"},\n",
    "            \"metadata\": {\n",
    "                \"git_detected\": git_detected,\n",
    "                \"processed_files\": total_processed,\n",
    "                \"description\": \"Results ready for chaining: Use the returned 'data[].path' and 'data[].snippet' fields as inputs for the next processing step.\"\n",
    "                \n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89717580-4156-4a28-97c8-69ca18d01e19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FsReadTool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# | hide\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m tool = \u001b[43mFsReadTool\u001b[49m()\n\u001b[32m      3\u001b[39m params = FsReadParams(operations=[\n\u001b[32m      4\u001b[39m     FsReadOperation(mode=ToolCallMode.DISCOVER, path=\u001b[33m\"\u001b[39m\u001b[33m/home/pranav-pc/projects/applied-GenAI-lab\u001b[39m\u001b[33m\"\u001b[39m, query=\u001b[33m\"\u001b[39m\u001b[33msystem prompt\u001b[39m\u001b[33m\"\u001b[39m, file_pattern=\u001b[33m\"\u001b[39m\u001b[33m*.py\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      5\u001b[39m     FsReadOperation(mode=ToolCallMode.EXTRACT, path=\u001b[33m\"\u001b[39m\u001b[33m/home/pranav-pc/projects/applied-GenAI-lab/\u001b[39m\u001b[33m\"\u001b[39m, query=\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdef\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+get_system_prompt\u001b[39m\u001b[33m\"\u001b[39m, regex_mode=\u001b[38;5;28;01mTrue\u001b[39;00m, context_lines=\u001b[32m5\u001b[39m)\n\u001b[32m      6\u001b[39m ])\n\u001b[32m      7\u001b[39m result = tool.execute(params)\n",
      "\u001b[31mNameError\u001b[39m: name 'FsReadTool' is not defined"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "tool = FsReadTool()\n",
    "params = FsReadParams(operations=[\n",
    "    FsReadOperation(mode=ToolCallMode.DISCOVER, path=\"/home/pranav-pc/projects/applied-GenAI-lab\", query=\"system prompt\", file_pattern=\"*.py\"),\n",
    "    FsReadOperation(mode=ToolCallMode.EXTRACT, path=\"/home/pranav-pc/projects/applied-GenAI-lab/\", query=r\"def\\s+get_system_prompt\", regex_mode=True, context_lines=5)\n",
    "])\n",
    "result = tool.execute(params)\n",
    "print(json.dumps(result, indent=2))  # Chainable JSON for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928eec06-be43-4cb2-834e-2d53c8937ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
