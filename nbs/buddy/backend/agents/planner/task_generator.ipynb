{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f66251-d324-4e26-a1c3-68698805a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp agent.planner.task_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75def6dc-5631-43cc-aa5b-5a2838e567e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import json\n",
    "import re\n",
    "from typing import Optional\n",
    "from rich.console import Console\n",
    "from datetime import datetime\n",
    "from agentic.agent.planner.models import Task, ProjectBreakdown, ProjectContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108479c4-03b7-4648-bba1-bb86bcd6787e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Console' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# | export\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mTaskGenerator\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;250;43m    \u001b[39;49m\u001b[33;43;03m\"\"\"Handles task generation and validation\"\"\"\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsole\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mConsole\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mTaskGenerator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTaskGenerator\u001b[39;00m:\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Handles task generation and validation\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent, console: \u001b[43mConsole\u001b[49m):\n\u001b[32m      6\u001b[39m         \u001b[38;5;28mself\u001b[39m.agent = agent\n\u001b[32m      7\u001b[39m         \u001b[38;5;28mself\u001b[39m.console = console\n",
      "\u001b[31mNameError\u001b[39m: name 'Console' is not defined"
     ]
    }
   ],
   "source": [
    "# | export \n",
    "class TaskGenerator:\n",
    "    \"\"\"Handles task generation and validation\"\"\"\n",
    "    \n",
    "    def __init__(self, agent, console: Console):\n",
    "        self.agent = agent\n",
    "        self.console = console\n",
    "    \n",
    "    def _determine_project_phase(self, completed: int, total: int) -> str:\n",
    "        \"\"\"Determine current project phase based on completion\"\"\"\n",
    "        progress = completed / max(total, 1)\n",
    "        if progress < 0.3:\n",
    "            return \"SETUP & FOUNDATION\"\n",
    "        elif progress < 0.6:\n",
    "            return \"CORE DEVELOPMENT\"\n",
    "        elif progress < 0.8:\n",
    "            return \"INTEGRATION & FEATURES\"\n",
    "        else:\n",
    "            return \"TESTING & FINALIZATION\"\n",
    "    \n",
    "    def _get_phase_guidance(self, completed: int, total: int) -> str:\n",
    "        \"\"\"Get phase-specific guidance for task generation\"\"\"\n",
    "        phase = self._determine_project_phase(completed, total)\n",
    "        \n",
    "        guidance = {\n",
    "            \"SETUP & FOUNDATION\": \"\"\"\n",
    "• Focus: Project structure, requirements analysis, environment setup\n",
    "• Priority: Create solid foundation for development\n",
    "• Tasks: Directory structure, dependency management, configuration files\n",
    "• Quality: Establish coding standards and project conventions\"\"\",\n",
    "            \n",
    "            \"CORE DEVELOPMENT\": \"\"\"\n",
    "• Focus: Main functionality implementation, core components\n",
    "• Priority: Build primary features and business logic\n",
    "• Tasks: Core modules, main algorithms, data structures\n",
    "• Quality: Robust error handling and comprehensive logging\"\"\",\n",
    "            \n",
    "            \"INTEGRATION & FEATURES\": \"\"\"\n",
    "• Focus: Component integration, advanced features, optimization\n",
    "• Priority: Connect systems and add sophisticated functionality\n",
    "• Tasks: Module integration, advanced features, performance tuning\n",
    "• Quality: Code optimization and maintainability\"\"\",\n",
    "            \n",
    "            \"TESTING & FINALIZATION\": \"\"\"\n",
    "• Focus: Quality assurance, documentation, final polish\n",
    "• Priority: Ensure reliability and completeness\n",
    "• Tasks: Testing (if required), documentation, final validation\n",
    "• Quality: Code review and production readiness\"\"\"\n",
    "        }\n",
    "        \n",
    "        return guidance.get(phase, \"Continue with next logical development step\")\n",
    "    \n",
    "    \n",
    "    def generate_next_task(self, context: ProjectContext, breakdown: ProjectBreakdown, estimated_total: int) -> Optional[Task]:\n",
    "        \"\"\"Generate single next task using project breakdown context\"\"\"\n",
    "        \n",
    "        context_prompt = self._build_context_prompt(context)\n",
    "        execution_context = self._get_execution_context(context)\n",
    "        \n",
    "        completed_count = len(context.execution_history)\n",
    "        next_task_id = f\"T{completed_count + 1:03d}\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "MISSION: Generate the next critical task in the software development pipeline.\n",
    "\n",
    "EXECUTION CONTEXT:\n",
    "• Progress: {completed_count}/{estimated_total} tasks completed\n",
    "• Project Phase: {self._determine_project_phase(completed_count, estimated_total)}\n",
    "• Quality Standard: Production-grade implementation required\n",
    "\n",
    "{execution_context}\n",
    "\n",
    "PROJECT SPECIFICATION:\n",
    "• Objective: {breakdown.project_summary}\n",
    "• Development Phases: {' → '.join(breakdown.phases)}\n",
    "• Target Deliverables: {', '.join(breakdown.key_deliverables)}\n",
    "• Technical Stack: {breakdown.technical_approach}\n",
    "\n",
    "ORIGINAL REQUIREMENT: {context.original_request}\n",
    "\n",
    "{context_prompt}\n",
    "\n",
    "TASK GENERATION STRATEGY:\n",
    "{self._get_phase_guidance(completed_count, estimated_total)}\n",
    "\n",
    "RESPONSE FORMAT - SINGLE TASK OBJECT:\n",
    "{{\n",
    "  \"id\": \"{next_task_id}\",\n",
    "  \"name\": \"[Descriptive Task Name - Action Oriented]\",\n",
    "  \"description\": \"[Detailed description aligned with current project phase and deliverables]\",\n",
    "  \"dependencies\": [/* Previous task IDs if applicable */],\n",
    "  \"actions\": [\n",
    "    {{\n",
    "      \"step\": 1,\n",
    "      \"purpose\": \"[Clear, actionable purpose statement]\",\n",
    "      \"sub_steps\": [\n",
    "        \"[Specific action 1 - mention exact files/outputs]\",\n",
    "        \"[Specific action 2 - mention exact files/outputs]\",\n",
    "        \"[Specific action 3 - mention exact files/outputs]\"\n",
    "      ],\n",
    "      \"introspect_after\": true,\n",
    "      \"system_prompt\": \"You are a senior software engineer working on: {breakdown.project_summary}. Deliver high-quality code with proper error handling, documentation, and best practices.\",\n",
    "      \"user_prompt\": \"Implement the required functionality focusing on: [LIST ALL expected_outputs HERE]. Use appropriate tools (fs_write, execute_bash) to create well-structured, documented code.\",\n",
    "      \"introspect_prompt\": \"Validate implementation quality: check file existence, code structure, documentation completeness, and adherence to requirements.\",\n",
    "      \"execution_mode\": \"sequential\"\n",
    "    }}\n",
    "  ],\n",
    "  \"success_criteria\": \"[Specific, measurable success criteria with file/component verification]\",\n",
    "  \"expected_outputs\": [\"[specific_file1.py]\", \"[specific_file2.json]\", \"[directory/]\"],\n",
    "  \"potential_options\": [],\n",
    "  \"needs_debate\": false\n",
    "}}\n",
    "\n",
    "DEBATE USAGE GUIDELINES:\n",
    "• Set \"needs_debate\": true for tasks involving:\n",
    "  - Architecture decisions (database choice, framework selection)\n",
    "  - Technology stack decisions (library comparisons, tool selection)\n",
    "  - Design pattern choices (authentication methods, API design)\n",
    "  - Performance optimization approaches\n",
    "• Set \"potential_options\": [\"option1\", \"option2\", \"option3\"] when debate is needed\n",
    "• Use debate sparingly - only for significant decisions, not implementation details\n",
    "\n",
    "QUALITY REQUIREMENTS:\n",
    "✓ All expected_outputs must be explicitly mentioned in sub_steps and user_prompt\n",
    "✓ Task must advance toward project objectives\n",
    "✓ Actions must be specific and executable\n",
    "✓ Success criteria must be measurable\n",
    "✓ High-quality standards apply\n",
    "\n",
    "Generate the next logical task or return null if project is complete.\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.agent.run(prompt).get(\"content\", \"\")\n",
    "            \n",
    "            # Save raw response for debugging in JSONL format\n",
    "            import json\n",
    "            from datetime import datetime\n",
    "            log_entry = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"task_id\": next_task_id,\n",
    "                \"title\": breakdown.title if hasattr(breakdown, 'title') else \"Unknown Project\",\n",
    "                \"response\": response\n",
    "            }\n",
    "            with open(\"task_generation_raw.jsonl\", 'a') as f:\n",
    "                f.write(json.dumps(log_entry) + '\\n')\n",
    "            \n",
    "            # Parse and validate task\n",
    "            response = response.strip()\n",
    "            \n",
    "            # Only return None if explicitly null AND we have enough tasks\n",
    "            if response.lower() == \"null\":\n",
    "                if context.total_tasks_completed >= 5:\n",
    "                    return None\n",
    "                else:\n",
    "                    self.console.print(\"⚠️ LLM returned null but project needs more tasks. Regenerating...\")\n",
    "                    return self._force_generate_next_task(context, breakdown, estimated_total)\n",
    "            \n",
    "            # Try to extract JSON from response\n",
    "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group()\n",
    "                task_data = json.loads(json_str)\n",
    "            else:\n",
    "                task_data = json.loads(response)\n",
    "                \n",
    "        except json.JSONDecodeError as e:\n",
    "            self.console.print(f\"❌ JSON parsing failed: {e}\")\n",
    "            task_data = self._extract_task_from_text(response, context)\n",
    "            if not task_data:\n",
    "                self.console.print(\"❌ Failed to extract task from response\")\n",
    "                return None\n",
    "        \n",
    "        # If LLM returns array, take only the first task\n",
    "        if isinstance(task_data, list):\n",
    "            if len(task_data) > 0:\n",
    "                task_data = task_data[0]\n",
    "                self.console.print(\"⚠️ LLM returned multiple tasks, using only the first one\")\n",
    "            else:\n",
    "                self.console.print(\"⚠️ LLM returned empty task array\")\n",
    "                return None\n",
    "        \n",
    "        # Validate required fields\n",
    "        if not isinstance(task_data, dict) or 'name' not in task_data:\n",
    "            self.console.print(\"⚠️ Invalid task format returned by LLM\")\n",
    "            return None\n",
    "        \n",
    "        task = Task(**task_data)\n",
    "        task.id = f\"T{len(context.execution_history) + 1:03d}\"\n",
    "        \n",
    "        # Check for task repetition\n",
    "        for prev_task in context.execution_history:\n",
    "            if task.name.lower().strip() == prev_task.task_name.lower().strip():\n",
    "                self.console.print(f\"⚠️ Task '{task.name}' appears to be a repeat. Skipping.\")\n",
    "                return None\n",
    "        \n",
    "        return task\n",
    "    \n",
    "    def regenerate_task_with_feedback(self, context: ProjectContext, breakdown: ProjectBreakdown, feedback: str) -> Optional[Task]:\n",
    "        \"\"\"Regenerate task incorporating introspection feedback\"\"\"\n",
    "        context_prompt = self._build_context_prompt(context)\n",
    "        execution_context = self._get_execution_context(context)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "CRITICAL: The previously generated task was rejected. Generate a BETTER task incorporating the feedback.\n",
    "\n",
    "FEEDBACK FROM VALIDATION: {feedback}\n",
    "\n",
    "{execution_context}\n",
    "\n",
    "PROJECT BREAKDOWN CONTEXT:\n",
    "Summary: {breakdown.project_summary}\n",
    "Phases: {', '.join(breakdown.phases)}\n",
    "Key Deliverables: {', '.join(breakdown.key_deliverables)}\n",
    "\n",
    "{context_prompt}\n",
    "\n",
    "Generate ONE improved task that addresses the validation feedback. Ensure it's unique and not a repeat.\n",
    "\n",
    "RESPOND WITH SINGLE TASK OBJECT OR NULL:\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            agent_response = self.agent.run(prompt)\n",
    "            response = agent_response.get(\"content\", \"\") if isinstance(agent_response, dict) else str(agent_response)\n",
    "            \n",
    "            if response.strip().lower() == \"null\":\n",
    "                return None\n",
    "                \n",
    "            task_data = json.loads(response.strip())\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            task_data = self._extract_task_from_text(response.strip(), context)\n",
    "            if not task_data:\n",
    "                raise ValueError(\"Failed to parse JSON and extract task fields. Please provide valid JSON format with required fields: id, name, description, actions.\")\n",
    "            \n",
    "            if isinstance(task_data, list) and len(task_data) > 0:\n",
    "                task_data = task_data[0]\n",
    "            elif isinstance(task_data, list):\n",
    "                return None\n",
    "                \n",
    "            if not isinstance(task_data, dict) or 'name' not in task_data:\n",
    "                return None\n",
    "                \n",
    "            task = Task(**task_data)\n",
    "            task.id = f\"T{len(context.execution_history) + 1:03d}\"\n",
    "            \n",
    "            return task\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"❌ Task regeneration failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _force_generate_next_task(self, context: ProjectContext, breakdown: ProjectBreakdown, estimated_total: int) -> Optional[Task]:\n",
    "        \"\"\"Force generation of next task when LLM returns null prematurely\"\"\"\n",
    "        \n",
    "        completed_count = len(context.execution_history)\n",
    "        next_task_id = f\"T{completed_count + 1:03d}\"\n",
    "        \n",
    "        # Determine what type of task is needed based on progress\n",
    "        if completed_count < 3:\n",
    "            task_type = \"setup and structure\"\n",
    "            example_tasks = [\"Create project structure\", \"Setup dependencies\", \"Configure environment\"]\n",
    "        elif completed_count < 6:\n",
    "            task_type = \"core implementation\"\n",
    "            example_tasks = [\"Implement main scraper\", \"Add rate limiting\", \"Create data models\"]\n",
    "        else:\n",
    "            task_type = \"integration and testing\"\n",
    "            example_tasks = [\"Add error handling\", \"Create tests\", \"Setup deployment\"]\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "CRITICAL: The project needs more tasks to be complete. You MUST generate a task.\n",
    "\n",
    "CURRENT STATUS: Only {completed_count} tasks completed - this is insufficient for a complete web scraping framework.\n",
    "\n",
    "REQUIRED TASK TYPE: {task_type}\n",
    "EXAMPLES: {', '.join(example_tasks)}\n",
    "\n",
    "PROJECT CONTEXT:\n",
    "- Original Request: {context.original_request}\n",
    "- Project Summary: {breakdown.project_summary}\n",
    "- Completed Tasks: {[t.task_name for t in context.execution_history]}\n",
    "\n",
    "Generate a {task_type} task that is essential for completing the web scraping framework.\n",
    "\n",
    "RESPOND WITH TASK OBJECT:\n",
    "{{\n",
    "  \"id\": \"{next_task_id}\",\n",
    "  \"name\": \"Essential Task Name\",\n",
    "  \"description\": \"Description of essential task\",\n",
    "  \"dependencies\": [],\n",
    "  \"actions\": [\n",
    "    {{\n",
    "      \"step\": 1,\n",
    "      \"purpose\": \"Main purpose\",\n",
    "      \"sub_steps\": [\"Step 1\", \"Step 2\"],\n",
    "      \"introspect_after\": true,\n",
    "      \"system_prompt\": \"You are a developer working on a web scraping framework. Complete this essential task.\",\n",
    "      \"user_prompt\": \"Implement the required functionality for the web scraping framework.\",\n",
    "      \"introspect_prompt\": \"Validate that the implementation is correct and functional.\",\n",
    "      \"execution_mode\": \"sequential\"\n",
    "    }}\n",
    "  ],\n",
    "  \"success_criteria\": \"Task completion criteria\",\n",
    "  \"expected_outputs\": [\"output.py\"],\n",
    "  \"potential_options\": [],\n",
    "  \"needs_debate\": false\n",
    "}}\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.agent.run(prompt).get(\"content\", \"\")\n",
    "            \n",
    "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                task_data = json.loads(json_match.group())\n",
    "            else:\n",
    "                task_data = json.loads(response.strip())\n",
    "            \n",
    "            if isinstance(task_data, dict) and 'name' in task_data:\n",
    "                task = Task(**task_data)\n",
    "                task.id = next_task_id\n",
    "                return task\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"❌ Force task generation failed: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _build_context_prompt(self, context: ProjectContext) -> str:\n",
    "        \"\"\"Build comprehensive context from execution history\"\"\"\n",
    "        if not context.execution_history:\n",
    "            return \"PREVIOUS CONTEXT: This is the first task.\"\n",
    "        \n",
    "        context_parts = [\"EXECUTION HISTORY:\"]\n",
    "        \n",
    "        for i, task_result in enumerate(context.execution_history, 1):\n",
    "            status_emoji = \"✅\" if task_result.status == \"success\" else \"❌\"\n",
    "            context_parts.append(f\"{i}. {task_result.task_name} (ID: {task_result.task_id}) - {status_emoji} {task_result.status}\")\n",
    "            context_parts.append(f\"   Description: {task_result.task_name}\")\n",
    "            if task_result.artifacts_created:\n",
    "                if len(task_result.artifacts_created) <= 3:\n",
    "                    context_parts.append(f\"   Artifacts: {', '.join(task_result.artifacts_created)}\")\n",
    "                else:\n",
    "                    shown = ', '.join(task_result.artifacts_created[:3])\n",
    "                    remaining = len(task_result.artifacts_created) - 3\n",
    "                    context_parts.append(f\"   Artifacts: {shown} ... (+{remaining} more)\")\n",
    "        \n",
    "        context_parts.append(f\"\\nTOTAL ARTIFACTS: {len(context.current_artifacts)}\")\n",
    "        context_parts.append(f\"PROJECT STATUS: {context.project_status}\")\n",
    "        context_parts.append(f\"PROGRESS: {context.total_tasks_completed}/{len(context.execution_history)} tasks completed\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def _get_execution_context(self, context: ProjectContext) -> str:\n",
    "        \"\"\"Generate minimal context for next task generation\"\"\"\n",
    "        if not context.execution_history:\n",
    "            return \"CONTEXT: This is the first task.\"\n",
    "        \n",
    "        last_task = context.execution_history[-1].task_name\n",
    "        artifacts_count = len(context.current_artifacts)\n",
    "        \n",
    "        return f\"CONTEXT: Last completed task: '{last_task}'. Total artifacts created: {artifacts_count}.\"\n",
    "    \n",
    "    def _extract_task_from_text(self, text: str, context: ProjectContext) -> Optional[dict]:\n",
    "        \"\"\"Extract task fields from malformed JSON/text as fallback\"\"\"\n",
    "        \n",
    "        json_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                return json.loads(json_match.group())\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        task_data = {}\n",
    "        \n",
    "        if match := re.search(r'\"?id\"?\\s*:\\s*\"?([^\",\\n]+)\"?', text, re.IGNORECASE):\n",
    "            task_data[\"id\"] = match.group(1).strip('\"')\n",
    "        \n",
    "        if match := re.search(r'\"?name\"?\\s*:\\s*\"([^\"]+)\"', text, re.IGNORECASE):\n",
    "            task_data[\"name\"] = match.group(1)\n",
    "        \n",
    "        if match := re.search(r'\"?description\"?\\s*:\\s*\"([^\"]+)\"', text, re.IGNORECASE):\n",
    "            task_data[\"description\"] = match.group(1)\n",
    "        \n",
    "        if \"name\" in task_data:\n",
    "            task_data.setdefault(\"id\", f\"T{len(context.execution_history) + 1:03d}\")\n",
    "            task_data.setdefault(\"description\", task_data[\"name\"])\n",
    "            task_data.setdefault(\"dependencies\", [])\n",
    "            task_data.setdefault(\"success_criteria\", f\"Complete {task_data['name']}\")\n",
    "            task_data.setdefault(\"expected_outputs\", [])\n",
    "            task_data.setdefault(\"potential_options\", [])\n",
    "            task_data.setdefault(\"needs_debate\", False)\n",
    "            \n",
    "            task_data.setdefault(\"actions\", [{\n",
    "                \"step\": 1,\n",
    "                \"purpose\": task_data[\"name\"],\n",
    "                \"sub_steps\": [f\"Create {output}\" for output in task_data.get(\"expected_outputs\", [])],\n",
    "                \"introspect_after\": True,\n",
    "                \"system_prompt\": f\"You are an AI assistant. Complete the task: {task_data['name']}. Create all expected outputs.\",\n",
    "                \"user_prompt\": f\"{task_data['description']}. Create all expected outputs using fs_write tool: {', '.join(task_data.get('expected_outputs', []))}\",\n",
    "                \"introspect_prompt\": f\"Validate that {task_data['name']} was completed successfully and all expected outputs exist\",\n",
    "                \"execution_mode\": \"sequential\"\n",
    "            }])\n",
    "            \n",
    "            return task_data\n",
    "        \n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a00db-431a-4aa3-9295-44d02204d3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
