# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/buddy/backend/tools.ipynb.

# %% auto 0
__all__ = ['TokenManager', 'ToolManager']

# %% ../../nbs/buddy/backend/tools.ipynb 1
import os
import json
import subprocess
from typing import Dict, List, Any, Optional
from pathlib import Path

# %% ../../nbs/buddy/backend/tools.ipynb 2
class TokenManager:
    def __init__(self, model_name: str = "gpt-4", max_tokens: int = 128000):
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.compression_threshold = 0.8  # 80%
        try:
            self.encoding = tiktoken.encoding_for_model(model_name)
        except:
            self.encoding = tiktoken.get_encoding("cl100k_base")  # fallback
    
    def count_tokens(self, text: str) -> int:
        """Count tokens in text"""
        return len(self.encoding.encode(text))
    
    def count_conversation_tokens(self, messages: List[Dict]) -> int:
        """Count total tokens in conversation history"""
        total = 0
        for msg in messages:
            content = msg.get("content", "")
            if isinstance(content, str):
                total += self.count_tokens(content)
            # Add overhead for message structure
            total += 10
        return total
    
    def should_compress(self, messages: List[Dict]) -> bool:
        """Check if compression is needed"""
        current_tokens = self.count_conversation_tokens(messages)
        return current_tokens >= (self.max_tokens * self.compression_threshold)
    
    def compress_history(self, messages: List[Dict]) -> List[Dict]:
        """Compress conversation history while preserving context"""
        if len(messages) <= 2:  # Keep system + at least 1 message
            return messages
        
        system_msg = messages[0] if messages[0]["role"] == "system" else None
        recent_messages = messages[-4:]  # Keep last 4 messages
        
        # Compress middle messages into summary
        middle_messages = messages[1:-4] if len(messages) > 5 else []
        
        compressed = []
        if system_msg:
            compressed.append(system_msg)
        
        if middle_messages:
            summary = self._create_summary(middle_messages)
            compressed.append({
                "role": "assistant",
                "content": f"[COMPRESSED HISTORY - {len(middle_messages)} messages summarized]\n{summary}"
            })
        
        compressed.extend(recent_messages)
        return compressed
    
    def _create_summary(self, messages: List[Dict]) -> str:
        """Create concise summary of conversation history"""
        topics = []
        actions = []
        
        for msg in messages:
            content = msg.get("content", "")
            role = msg.get("role", "")
            
            if role == "user":
                # Extract key topics/requests
                if any(word in content.lower() for word in ["create", "build", "make"]):
                    topics.append("Creation tasks")
                if any(word in content.lower() for word in ["fix", "debug", "error"]):
                    topics.append("Debugging")
                if any(word in content.lower() for word in ["analyze", "review", "check"]):
                    topics.append("Analysis")
                if any(word in content.lower() for word in ["deploy", "install", "setup"]):
                    topics.append("Deployment")
            
            elif role == "assistant":
                # Extract actions taken
                if "fs_read" in content:
                    actions.append("File reading")
                if "fs_write" in content:
                    actions.append("File editing")
                if "execute_bash" in content:
                    actions.append("Command execution")
        
        summary_parts = []
        if topics:
            summary_parts.append(f"Topics: {', '.join(set(topics))}")
        if actions:
            summary_parts.append(f"Actions: {', '.join(set(actions))}")
        
        return "; ".join(summary_parts) if summary_parts else "General conversation"


# %% ../../nbs/buddy/backend/tools.ipynb 3
class ToolManager:
    def __init__(self):
        self.tools = {
            "fs_read": self._fs_read,
            "fs_write": self._fs_write,
            "execute_bash": self._execute_bash,
            "introspect": self._introspect,
            "todo": self._todo,
            "memory_manager": self._memory_manager,
            "debate_agent": self._debate_agent,
            "code_interpreter": self._code_interpreter,
            "repo_quality_analyzer": self._repo_quality_analyzer,
            "documentation_generator": self._documentation_generator
        }
        self._task_plans = {}
        self.token_manager = TokenManager(model_name, max_tokens)

    def check_and_compress_history(self, messages: List[Dict]) -> tuple[List[Dict], bool]:
        """Check token budget and compress if needed"""
        if self.token_manager.should_compress(messages):
            compressed = self.token_manager.compress_history(messages)
            return compressed, True
        return messages, False
    
    def get_token_info(self, messages: List[Dict]) -> Dict[str, Any]:
        """Get current token usage information"""
        current_tokens = self.token_manager.count_conversation_tokens(messages)
        return {
            "current_tokens": current_tokens,
            "max_tokens": self.token_manager.max_tokens,
            "usage_percent": round((current_tokens / self.token_manager.max_tokens) * 100, 1),
            "compression_threshold": round(self.token_manager.compression_threshold * 100),
            "tokens_until_compression": max(0, int(self.token_manager.max_tokens * self.token_manager.compression_threshold) - current_tokens),
            "needs_compression": self.token_manager.should_compress(messages)
        }

    def get_tools(self, requested_tools: List[str]) -> List[Dict]:
        """Convert tools to function calling format"""
        available_tools = []
        
        if "fs_read" in requested_tools:
            available_tools.append({
                "type": "function",
                "function": {
                    "name": "fs_read",
                    "description": "file system operations: read files with intelligent chunking, list directories with depth, search patterns, find files, grep across files, and generate tree structures. Supports batch operations.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "operations": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "mode": {
                                            "type": "string", 
                                            "enum": ["Line", "Directory", "Search", "Find", "Grep", "Tree"],
                                            "description": "Operation mode: Line (read file), Directory (list with depth), Search (pattern in file), Find (find files by name), Grep (search across files), Tree (directory tree)"
                                        },
                                        "path": {"type": "string", "description": "File or directory path"},
                                        "pattern": {"type": "string", "description": "Search pattern for Search/Grep modes"},
                                        "name_pattern": {"type": "string", "description": "File name pattern for Find mode (supports wildcards)"},
                                        "file_pattern": {"type": "string", "description": "File pattern for Grep mode (e.g., '*.py')"},
                                        "depth": {"type": "integer", "description": "Directory depth for Directory mode"},
                                        "max_depth": {"type": "integer", "description": "Maximum depth for Find/Tree modes"},
                                        "context_lines": {"type": "integer", "description": "Context lines around matches"},
                                        "start_line": {"type": "integer", "description": "Start line for Line mode"},
                                        "end_line": {"type": "integer", "description": "End line for Line mode (-1 for end of file)"},
                                        "show_hidden": {"type": "boolean", "description": "Show hidden files/directories"},
                                        "case_sensitive": {"type": "boolean", "description": "Case sensitive search"},
                                        "recursive": {"type": "boolean", "description": "Recursive search for Grep mode"},
                                        "file_type": {"type": "string", "enum": ["file", "dir", "all"], "description": "File type filter for Find mode"},
                                        "smart_chunk": {"type": "boolean", "description": "Use intelligent chunking for large files"}
                                    },
                                    "required": ["mode", "path"]
                                }
                            }
                        },
                        "required": ["operations"]
                    }
                }
            })




        if "fs_write" in requested_tools:
            available_tools.append({
                "type": "function",
                "function": {
                    "name": "fs_write",
                    "description": "file operations: create, edit, modify files with diff preview, insert at specific lines, and batch operations. Shows changes in git diff format.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "command": {
                                "type": "string", 
                                "enum": ["create", "str_replace", "append", "insert", "prepend", "delete_lines"],
                                "description": "Operation: create (new file), str_replace (replace text), append (add to end), insert (add at line), prepend (add to start), delete_lines (remove lines)"
                            },
                            "path": {"type": "string", "description": "File path"},
                            "file_text": {"type": "string", "description": "Content for create command"},
                            "old_str": {"type": "string", "description": "Text to replace (exact match required)"},
                            "new_str": {"type": "string", "description": "Replacement text"},
                            "insert_line": {"type": "integer", "description": "Line number to insert at (1-based)"},
                            "start_line": {"type": "integer", "description": "Start line for delete_lines"},
                            "end_line": {"type": "integer", "description": "End line for delete_lines"},
                            "create_backup": {"type": "boolean", "description": "Create backup before editing"},
                            "show_diff": {"type": "boolean", "description": "Show diff of changes"},
                            "summary": {"type": "string", "description": "Brief description of the change"}
                        },
                        "required": ["command", "path"]
                    }
                }
            })


        if "execute_bash" in requested_tools:
            available_tools.append({
                "type": "function",
                "function": {
                    "name": "execute_bash",
                    "description": "bash execution with working directory control, environment variables, timeout management, and detailed output formatting.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "command": {"type": "string", "description": "Bash command to execute"},
                            "working_dir": {"type": "string", "description": "Working directory for command execution"},
                            "timeout": {"type": "integer", "description": "Timeout in seconds (default: 30)"},
                            "env_vars": {"type": "object", "description": "Additional environment variables"},
                            "capture_output": {"type": "boolean", "description": "Capture stdout/stderr (default: true)"},
                            "shell": {"type": "string", "description": "Shell to use (default: /bin/bash)"},
                            "summary": {"type": "string", "description": "Brief description of what the command does"}
                        },
                        "required": ["command"]
                    }
                }
            })

        if "introspect" in requested_tools:
            available_tools.append({
                "type": "function",
                "function": {
                    "name": "introspect",
                    "description": "self-reflection and criticism tool. Analyzes actions, provides feedback, and validates decisions with constructive criticism.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "action": {
                                "type": "string",
                                "enum": ["critique", "validate", "reflect", "improve"],
                                "description": "Type of introspection: critique (analyze action), validate (check decision), reflect (self-assessment), improve (suggest enhancements)"
                            },
                            "context": {
                                "type": "object",
                                "description": "Context about the action/decision being analyzed",
                                "properties": {
                                    "tool_used": {"type": "string"},
                                    "command": {"type": "string"},
                                    "result": {"type": "object"},
                                    "user_request": {"type": "string"},
                                    "decision_type": {"type": "string"}
                                }
                            },
                            "focus_areas": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Specific areas to focus criticism on: efficiency, safety, completeness, best_practices, alternatives"
                            }
                        },
                        "required": ["action"]
                    }
                }
            })

        if "debate_agent" in requested_tools:
            available_tools.append({
                "type": "function",
                "function": {
                    "name": "debate_agent",
                    "description": "Multi-perspective debate agent for design decisions. Presents alternative viewpoints and challenges assumptions.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "decision": {"type": "string", "description": "The design decision or approach being considered"},
                            "context": {"type": "string", "description": "Background context and requirements"},
                            "perspectives": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Perspectives to consider: security, performance, maintainability, scalability, cost, user_experience"
                            },
                            "debate_style": {
                                "type": "string",
                                "enum": ["pros_cons", "alternatives", "devil_advocate", "stakeholder_views"],
                                "description": "Style of debate analysis"
                            }
                        },
                        "required": ["decision", "context"]
                    }
                }
            })
        if "code_interpreter" in requested_tools:
            available_tools.append({
                "type": "function",
                "function": {
                    "name": "code_interpreter",
                    "description": "Execute Python code with visualization support. Handles plotting, data analysis, calculations, and complex computations with result capture.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "code": {"type": "string", "description": "Python code to execute"},
                            "language": {"type": "string", "enum": ["python"], "description": "Programming language"},
                            "capture_output": {"type": "boolean", "description": "Capture stdout/stderr"},
                            "save_plots": {"type": "boolean", "description": "Save matplotlib plots as images"},
                            "timeout": {"type": "integer", "description": "Execution timeout in seconds"},
                            "working_dir": {"type": "string", "description": "Working directory for execution"},
                            "requirements": {"type": "array", "items": {"type": "string"}, "description": "Required packages to install"}
                        },
                        "required": ["code"]
                    }
                }
            })


        if "repo_quality_analyzer" in requested_tools:
            available_tools.append({
                "type": "function",
                "function": {
                    "name": "repo_quality_analyzer",
                    "description": "Comprehensive repository code quality analysis with actionable recommendations for improvements.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "repo_path": {"type": "string", "description": "Repository root path"},
                            "analysis_type": {
                                "type": "string",
                                "enum": ["full", "security", "performance", "maintainability", "documentation"],
                                "description": "Type of analysis to perform"
                            },
                            "file_patterns": {"type": "array", "items": {"type": "string"}, "description": "File patterns to analyze (e.g., '*.py', '*.js')"},
                            "exclude_patterns": {"type": "array", "items": {"type": "string"}, "description": "Patterns to exclude"},
                            "severity_threshold": {"type": "string", "enum": ["low", "medium", "high", "critical"], "description": "Minimum severity for recommendations"}
                        },
                        "required": ["repo_path"]
                    }
                }
            })


        if "documentation_generator" in requested_tools:
            available_tools.append({
                "type": "function",
                "function": {
                    "name": "documentation_generator",
                    "description": "Intelligent documentation generator that analyzes entire repositories and creates comprehensive usage documentation.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "repo_path": {"type": "string", "description": "Repository root path"},
                            "doc_type": {
                                "type": "string",
                                "enum": ["api", "usage", "architecture", "setup", "comprehensive"],
                                "description": "Type of documentation to generate"
                            },
                            "output_format": {"type": "string", "enum": ["markdown", "html", "rst"], "description": "Output format"},
                            "include_examples": {"type": "boolean", "description": "Include code examples"},
                            "include_diagrams": {"type": "boolean", "description": "Generate architecture diagrams"},
                            "target_audience": {"type": "string", "enum": ["developers", "users", "contributors"], "description": "Target audience"},
                            "output_path": {"type": "string", "description": "Output directory for documentation"}
                        },
                        "required": ["repo_path", "doc_type"]
                    }
                }
            })

        if "memory_manager" in requested_tools:
            available_tools.append({
                "type": "function",
                "function": {
                    "name": "memory_manager",
                    "description": "Manage conversation memory and token budget. Check usage, compress history, and optimize memory.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "action": {
                                "type": "string",
                                "enum": ["status", "compress", "optimize"],
                                "description": "Action: status (check usage), compress (force compression), optimize (auto-manage)"
                            },
                            "messages": {
                                "type": "array",
                                "description": "Conversation history for analysis"
                            }
                        },
                        "required": ["action"]
                    }
                }
            })
        
        if "todo" in requested_tools:
            available_tools.append({
                "type": "function",
                "function": {
                    "name": "todo",
                    "description": "Break down tasks into steps",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "task": {"type": "string"},
                            "action": {"type": "string", "enum": ["plan", "execute", "status"]}
                        },
                        "required": ["task", "action"]
                    }
                }
            })
        
        return available_tools
    def _fs_read(self, operations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """fs_read with intelligent chunking and advanced features"""
        results = []
        
        for op in operations:
            mode = op.get("mode")
            path = op.get("path")
            
            try:
                if mode == "Directory":
                    depth = op.get("depth", 1)
                    show_hidden = op.get("show_hidden", False)
                    items = self._list_directory_tree(path, depth, show_hidden)
                    results.append({"path": path, "items": items, "depth": depth})
                
                elif mode == "Line":
                    start_line = op.get("start_line", 1)
                    end_line = op.get("end_line", -1)
                    smart_chunk = op.get("smart_chunk", True)
                    content = self._read_file_intelligent(path, start_line, end_line, smart_chunk)
                    results.append({"path": path, "content": content, "start_line": start_line, "end_line": end_line})
                
                elif mode == "Search":
                    pattern = op.get("pattern", "")
                    context_lines = op.get("context_lines", 2)
                    case_sensitive = op.get("case_sensitive", False)
                    matches = self._search_in_file(path, pattern, context_lines, case_sensitive)
                    results.append({"path": path, "pattern": pattern, "matches": matches})
                
                elif mode == "Find":
                    name_pattern = op.get("name_pattern", "*")
                    file_type = op.get("file_type", "all")
                    max_depth = op.get("max_depth", 10)
                    found_items = self._find_files(path, name_pattern, file_type, max_depth)
                    results.append({"path": path, "name_pattern": name_pattern, "found": found_items})
                
                elif mode == "Grep":
                    pattern = op.get("pattern", "")
                    file_pattern = op.get("file_pattern", "*")
                    recursive = op.get("recursive", True)
                    context_lines = op.get("context_lines", 2)
                    matches = self._grep_files(path, pattern, file_pattern, recursive, context_lines)
                    results.append({"path": path, "pattern": pattern, "matches": matches})
                
                elif mode == "Tree":
                    max_depth = op.get("max_depth", 3)
                    show_hidden = op.get("show_hidden", False)
                    tree_structure = self._generate_tree(path, max_depth, show_hidden)
                    results.append({"path": path, "tree": tree_structure})
                
            except Exception as e:
                results.append({"path": path, "error": str(e)})
        
        return {"results": results}

    def _get_file_info(self, path: str) -> Dict[str, Any]:
        """Get comprehensive file information for intelligent reading decisions"""
        path_obj = Path(path)
        if not path_obj.exists():
            return {"error": "File not found"}
        
        stat = path_obj.stat()
        file_size = stat.st_size
        
        # Determine file type and characteristics
        file_info = {
            "size": file_size,
            "lines": 0,
            "is_binary": False,
            "encoding": "utf-8",
            "file_type": path_obj.suffix.lower(),
            "is_large": file_size > 1024 * 1024,  # > 1MB
            "estimated_lines": file_size // 50 if file_size > 0 else 0  # rough estimate
        }
        
        # Quick binary check
        try:
            with open(path, 'rb') as f:
                chunk = f.read(1024)
                if b'\x00' in chunk:
                    file_info["is_binary"] = True
                    return file_info
        except:
            file_info["is_binary"] = True
            return file_info
        
        # Count actual lines for text files (sample for large files)
        try:
            if file_size < 10 * 1024 * 1024:  # < 10MB, count all lines
                with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                    file_info["lines"] = sum(1 for _ in f)
            else:
                # Sample lines for very large files
                with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                    sample_lines = sum(1 for _, line in zip(range(10000), f))
                    file_info["lines"] = int((file_size / (f.tell() or 1)) * sample_lines)
        except:
            pass
        
        return file_info

    def _decide_read_strategy(self, path: str, start_line: int, end_line: int, file_info: Dict) -> Dict[str, Any]:
        """Decide how to read the file based on size, type, and request"""
        strategy = {
            "method": "full",  # full, chunked, sampled, head_tail
            "chunk_size": 1000,
            "sample_ratio": 0.1,
            "reason": ""
        }
        
        total_lines = file_info.get("lines", 0)
        file_size = file_info.get("size", 0)
        is_large = file_info.get("is_large", False)
        
        # Determine read range
        if end_line == -1:
            end_line = total_lines
        
        requested_lines = end_line - start_line + 1 if total_lines > 0 else 0
        
        # Decision logic similar to Amazon Q
        if file_info.get("is_binary"):
            strategy.update({"method": "binary_info", "reason": "Binary file detected"})
        elif file_size > 50 * 1024 * 1024:  # > 50MB
            strategy.update({"method": "head_tail", "chunk_size": 500, "reason": "Very large file, showing head and tail"})
        elif total_lines > 10000 and requested_lines > 5000:
            strategy.update({"method": "sampled", "sample_ratio": 0.2, "reason": "Large file with many requested lines"})
        elif total_lines > 5000 and (start_line == 1 and end_line == total_lines):
            strategy.update({"method": "chunked", "chunk_size": 2000, "reason": "Large file, full read requested"})
        elif requested_lines > 2000:
            strategy.update({"method": "chunked", "chunk_size": 1000, "reason": "Many lines requested"})
        else:
            strategy.update({"method": "full", "reason": "Standard read"})
        
        return strategy

    def _read_file_intelligent(self, path: str, start_line: int = 1, end_line: int = -1, smart_chunk: bool = True) -> str:
        """Intelligently read file based on size and content type"""
        if not smart_chunk:
            return self._read_file_lines(path, start_line, end_line)
        
        file_info = self._get_file_info(path)
        if "error" in file_info:
            return file_info["error"]
        
        if file_info.get("is_binary"):
            return f"[Binary file: {file_info['size']} bytes, type: {file_info.get('file_type', 'unknown')}]"
        
        strategy = self._decide_read_strategy(path, start_line, end_line, file_info)
        
        try:
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
            
            total_lines = len(lines)
            if end_line == -1:
                end_line = total_lines
            
            start_idx = max(0, start_line - 1)
            end_idx = min(total_lines, end_line)
            
            if strategy["method"] == "full":
                content = ''.join(lines[start_idx:end_idx])
            
            elif strategy["method"] == "chunked":
                chunk_size = strategy["chunk_size"]
                chunks = []
                current_idx = start_idx
                
                while current_idx < end_idx:
                    chunk_end = min(current_idx + chunk_size, end_idx)
                    chunk = ''.join(lines[current_idx:chunk_end])
                    chunks.append(f"--- Lines {current_idx + 1}-{chunk_end} ---\n{chunk}")
                    current_idx = chunk_end
                    
                    if len(chunks) >= 5:  # Limit chunks
                        remaining = end_idx - current_idx
                        if remaining > 0:
                            chunks.append(f"... [{remaining} more lines omitted] ...")
                        break
                
                content = '\n'.join(chunks)
            
            elif strategy["method"] == "head_tail":
                chunk_size = strategy["chunk_size"]
                head = ''.join(lines[start_idx:start_idx + chunk_size])
                tail = ''.join(lines[max(start_idx + chunk_size, end_idx - chunk_size):end_idx])
                
                omitted = end_idx - start_idx - (2 * chunk_size)
                content = f"--- Head (lines {start_idx + 1}-{start_idx + chunk_size}) ---\n{head}\n"
                if omitted > 0:
                    content += f"... [{omitted} lines omitted] ...\n"
                content += f"--- Tail (lines {end_idx - chunk_size + 1}-{end_idx}) ---\n{tail}"
            
            elif strategy["method"] == "sampled":
                sample_ratio = strategy["sample_ratio"]
                total_requested = end_idx - start_idx
                sample_size = int(total_requested * sample_ratio)
                
                if sample_size < 100:
                    sample_size = min(100, total_requested)
                
                # Sample lines evenly
                step = max(1, total_requested // sample_size)
                sampled_lines = []
                
                for i in range(start_idx, end_idx, step):
                    if len(sampled_lines) >= sample_size:
                        break
                    sampled_lines.append(f"{i + 1:6d}: {lines[i]}")
                
                content = f"--- Sampled {len(sampled_lines)} lines from {total_requested} total ---\n"
                content += ''.join(sampled_lines)
            
            # Add metadata
            metadata = f"\n--- File Info: {file_info['size']} bytes, {file_info['lines']} lines, Strategy: {strategy['method']} ({strategy['reason']}) ---"
            return content + metadata
            
        except Exception as e:
            return f"Error reading file: {str(e)}"

    def _read_file_lines(self, path: str, start_line: int, end_line: int) -> str:
        """Simple line-based file reading"""
        with open(path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
        
        if end_line == -1:
            end_line = len(lines)
        
        start_idx = max(0, start_line - 1)
        end_idx = min(len(lines), end_line)
        
        return ''.join(lines[start_idx:end_idx])

    def _list_directory_tree(self, path: str, depth: int, show_hidden: bool) -> List[Dict]:
        """List directory with depth and metadata"""
        items = []
        path_obj = Path(path)
        
        if not path_obj.exists() or not path_obj.is_dir():
            return items
            
        def _scan_dir(dir_path: Path, current_depth: int) -> List[Dict]:
            dir_items = []
            if current_depth > depth:
                return dir_items
                
            try:
                for item in sorted(dir_path.iterdir()):
                    if not show_hidden and item.name.startswith('.'):
                        continue
                        
                    stat = item.stat()
                    item_info = {
                        "name": item.name,
                        "type": "directory" if item.is_dir() else "file",
                        "size": stat.st_size if item.is_file() else None,
                        "modified": stat.st_mtime,
                        "permissions": oct(stat.st_mode)[-3:],
                        "path": str(item)
                    }
                    
                    if item.is_dir() and current_depth < depth:
                        item_info["children"] = _scan_dir(item, current_depth + 1)
                    
                    dir_items.append(item_info)
            except PermissionError:
                pass
                
            return dir_items
        
        return _scan_dir(path_obj, 0)

    def _search_in_file(self, path: str, pattern: str, context_lines: int, case_sensitive: bool) -> List[Dict]:
        """Search pattern in file with context"""
        matches = []
        
        try:
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
        except:
            return matches
        
        search_pattern = pattern if case_sensitive else pattern.lower()
        
        for i, line in enumerate(lines):
            search_line = line if case_sensitive else line.lower()
            
            if search_pattern in search_line:
                start_ctx = max(0, i - context_lines)
                end_ctx = min(len(lines), i + context_lines + 1)
                
                context = []
                for j in range(start_ctx, end_ctx):
                    context.append({
                        "line_number": j + 1,
                        "content": lines[j].rstrip(),
                        "is_match": j == i
                    })
                
                matches.append({
                    "line_number": i + 1,
                    "content": line.rstrip(),
                    "context": context
                })
        
        return matches

    def _find_files(self, path: str, name_pattern: str, file_type: str, max_depth: int) -> List[Dict]:
        """Find files by name pattern"""
        found = []
        path_obj = Path(path)
        
        def _search_recursive(dir_path: Path, current_depth: int):
            if current_depth > max_depth:
                return
                
            try:
                for item in dir_path.iterdir():
                    if fnmatch.fnmatch(item.name, name_pattern):
                        if (file_type == "all" or 
                            (file_type == "file" and item.is_file()) or
                            (file_type == "dir" and item.is_dir())):
                            
                            stat = item.stat()
                            found.append({
                                "name": item.name,
                                "path": str(item),
                                "type": "directory" if item.is_dir() else "file",
                                "size": stat.st_size if item.is_file() else None,
                                "modified": stat.st_mtime
                            })
                    
                    if item.is_dir() and current_depth < max_depth:
                        _search_recursive(item, current_depth + 1)
            except PermissionError:
                pass
        
        if path_obj.is_dir():
            _search_recursive(path_obj, 0)
        
        return found

    def _grep_files(self, path: str, pattern: str, file_pattern: str, recursive: bool, context_lines: int) -> List[Dict]:
        """Grep pattern across multiple files"""
        matches = []
        path_obj = Path(path)
        
        def _grep_file(file_path: Path):
            try:
                file_matches = self._search_in_file(str(file_path), pattern, context_lines, False)
                if file_matches:
                    matches.append({
                        "file": str(file_path),
                        "matches": file_matches
                    })
            except:
                pass
        
        def _scan_directory(dir_path: Path):
            try:
                for item in dir_path.iterdir():
                    if item.is_file() and fnmatch.fnmatch(item.name, file_pattern):
                        _grep_file(item)
                    elif item.is_dir() and recursive:
                        _scan_directory(item)
            except PermissionError:
                pass
        
        if path_obj.is_file():
            _grep_file(path_obj)
        elif path_obj.is_dir():
            _scan_directory(path_obj)
        
        return matches

    def _generate_tree(self, path: str, max_depth: int, show_hidden: bool) -> str:
        """Generate tree-like directory structure"""
        tree_lines = []
        path_obj = Path(path)
        
        def _build_tree(dir_path: Path, prefix: str, current_depth: int):
            if current_depth > max_depth:
                return
                
            try:
                items = sorted(dir_path.iterdir(), key=lambda x: (x.is_file(), x.name.lower()))
                if not show_hidden:
                    items = [item for item in items if not item.name.startswith('.')]
                
                for i, item in enumerate(items):
                    is_last = i == len(items) - 1
                    current_prefix = "└── " if is_last else "├── "
                    tree_lines.append(f"{prefix}{current_prefix}{item.name}")
                    
                    if item.is_dir() and current_depth < max_depth:
                        next_prefix = prefix + ("    " if is_last else "│   ")
                        _build_tree(item, next_prefix, current_depth + 1)
            except PermissionError:
                tree_lines.append(f"{prefix}└── [Permission Denied]")
        
        tree_lines.append(str(path_obj))
        if path_obj.is_dir():
            _build_tree(path_obj, "", 0)
        
        return "\n".join(tree_lines)

    def _fs_write(self, command: str, path: str, **kwargs) -> Dict[str, Any]:
        """file writing with diff preview and advanced operations"""
        import difflib
        import shutil
        from datetime import datetime
        
        try:
            path_obj = Path(path)
            show_diff = kwargs.get("show_diff", True)
            create_backup = kwargs.get("create_backup", False)
            summary = kwargs.get("summary", "")
            
            # Read original content if file exists
            original_content = ""
            if path_obj.exists():
                try:
                    with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                        original_content = f.read()
                except:
                    original_content = "[Binary or unreadable file]"
            
            # Create backup if requested
            if create_backup and path_obj.exists():
                backup_path = f"{path}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                shutil.copy2(path, backup_path)
            
            new_content = original_content
            
            if command == "create":
                file_text = kwargs.get("file_text", "")
                path_obj.parent.mkdir(parents=True, exist_ok=True)
                new_content = file_text
                
                with open(path, 'w', encoding='utf-8') as f:
                    f.write(file_text)
                
                result = {
                    "success": True, 
                    "message": f"Created file: {path}",
                    "lines_added": len(file_text.splitlines()),
                    "size_bytes": len(file_text.encode('utf-8'))
                }
                
                if show_diff and file_text:
                    result["diff"] = self._generate_diff("", file_text, path, "created")
                
                return result
            
            elif command == "append":
                new_str = kwargs.get("new_str", "")
                new_content = original_content + ('\n' if original_content and not original_content.endswith('\n') else '') + new_str
                
                with open(path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                
                result = {
                    "success": True, 
                    "message": f"Appended to file: {path}",
                    "lines_added": len(new_str.splitlines())
                }
                
                if show_diff:
                    result["diff"] = self._generate_diff(original_content, new_content, path, "appended")
                
                return result
            
            elif command == "prepend":
                new_str = kwargs.get("new_str", "")
                new_content = new_str + ('\n' if new_str and not new_str.endswith('\n') else '') + original_content
                
                with open(path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                
                result = {
                    "success": True, 
                    "message": f"Prepended to file: {path}",
                    "lines_added": len(new_str.splitlines())
                }
                
                if show_diff:
                    result["diff"] = self._generate_diff(original_content, new_content, path, "prepended")
                
                return result
            
            elif command == "str_replace":
                old_str = kwargs.get("old_str", "")
                new_str = kwargs.get("new_str", "")
                
                if old_str not in original_content:
                    return {
                        "error": f"String not found in file: '{old_str[:100]}{'...' if len(old_str) > 100 else ''}'",
                        "suggestion": "Use fs_read to check file content first"
                    }
                
                # Count occurrences
                occurrences = original_content.count(old_str)
                if occurrences > 1:
                    return {
                        "error": f"Multiple occurrences found ({occurrences}). Please be more specific with old_str to ensure unique match.",
                        "occurrences": occurrences
                    }
                
                new_content = original_content.replace(old_str, new_str)
                
                with open(path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                
                result = {
                    "success": True, 
                    "message": f"Replaced text in: {path}",
                    "replacements": 1,
                    "old_length": len(old_str),
                    "new_length": len(new_str)
                }
                
                if show_diff:
                    result["diff"] = self._generate_diff(original_content, new_content, path, "modified")
                
                return result
            
            elif command == "insert":
                insert_line = kwargs.get("insert_line", 1)
                new_str = kwargs.get("new_str", "")
                
                lines = original_content.splitlines(keepends=True)
                insert_idx = min(max(0, insert_line - 1), len(lines))
                
                # Insert new content
                new_lines = new_str.splitlines(keepends=True)
                if new_lines and not new_lines[-1].endswith('\n'):
                    new_lines[-1] += '\n'
                
                lines[insert_idx:insert_idx] = new_lines
                new_content = ''.join(lines)
                
                with open(path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                
                result = {
                    "success": True, 
                    "message": f"Inserted at line {insert_line} in: {path}",
                    "lines_added": len(new_lines)
                }
                
                if show_diff:
                    result["diff"] = self._generate_diff(original_content, new_content, path, "inserted")
                
                return result
            
            elif command == "delete_lines":
                start_line = kwargs.get("start_line", 1)
                end_line = kwargs.get("end_line", start_line)
                
                lines = original_content.splitlines(keepends=True)
                start_idx = max(0, start_line - 1)
                end_idx = min(len(lines), end_line)
                
                deleted_lines = lines[start_idx:end_idx]
                del lines[start_idx:end_idx]
                new_content = ''.join(lines)
                
                with open(path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                
                result = {
                    "success": True, 
                    "message": f"Deleted lines {start_line}-{end_line} from: {path}",
                    "lines_deleted": len(deleted_lines)
                }
                
                if show_diff:
                    result["diff"] = self._generate_diff(original_content, new_content, path, "deleted")
                
                return result
            
        except Exception as e:
            return {"error": f"File operation failed: {str(e)}"}

    def _generate_diff(self, original: str, new: str, filepath: str, operation: str) -> str:
        """Generate git-style diff output"""
        if original == new:
            return "No changes"
        
        original_lines = original.splitlines(keepends=True)
        new_lines = new.splitlines(keepends=True)
        
        diff_lines = list(difflib.unified_diff(
            original_lines, 
            new_lines,
            fromfile=f"a/{filepath}",
            tofile=f"b/{filepath}",
            lineterm=""
        ))
        
        if not diff_lines:
            return "No changes"
        
        # Format like git diff
        diff_output = []
        diff_output.append(f"--- a/{filepath}")
        diff_output.append(f"+++ b/{filepath}")
        
        # Skip the first two lines (file headers) from difflib output
        for line in diff_lines[2:]:
            diff_output.append(line.rstrip())
        
        # Add summary
        added_lines = sum(1 for line in diff_lines if line.startswith('+') and not line.startswith('+++'))
        removed_lines = sum(1 for line in diff_lines if line.startswith('-') and not line.startswith('---'))
        
        summary = f"\n📝 Changes: +{added_lines} -{removed_lines} lines ({operation})"
        
        return '\n'.join(diff_output) + summary

    def _execute_bash(self, command: str, **kwargs) -> Dict[str, Any]:
        """bash execution with advanced features"""
        import shlex
        import time
        from datetime import datetime
        
        # Extract parameters
        working_dir = kwargs.get("working_dir", os.getcwd())
        timeout = kwargs.get("timeout", 30)
        env_vars = kwargs.get("env_vars", {})
        capture_output = kwargs.get("capture_output", True)
        shell_path = kwargs.get("shell", "/bin/bash")
        summary = kwargs.get("summary", "")
        
        # Prepare environment
        env = os.environ.copy()
        env.update(env_vars)
        
        # Security check for dangerous commands
        dangerous_patterns = [
            "rm -rf /", ":(){ :|:& };:", "dd if=/dev/zero", "mkfs", "fdisk",
            "shutdown", "reboot", "halt", "init 0", "init 6"
        ]
        
        if any(pattern in command.lower() for pattern in dangerous_patterns):
            return {
                "error": "Potentially dangerous command blocked for safety",
                "command": command,
                "blocked_patterns": [p for p in dangerous_patterns if p in command.lower()]
            }
        
        try:
            start_time = time.time()
            
            # Change to working directory if specified
            original_cwd = os.getcwd()
            if working_dir and working_dir != original_cwd:
                if not os.path.exists(working_dir):
                    return {"error": f"Working directory does not exist: {working_dir}"}
                os.chdir(working_dir)
            
            # Execute command
            if capture_output:
                result = subprocess.run(
                    command,
                    shell=True,
                    capture_output=True,
                    text=True,
                    timeout=timeout,
                    env=env,
                    executable=shell_path if os.path.exists(shell_path) else None
                )
                
                execution_time = time.time() - start_time
                
                # Format output
                output_info = {
                    "command": command,
                    "exit_status": result.returncode,
                    "execution_time": round(execution_time, 3),
                    "working_directory": working_dir,
                    "timestamp": datetime.now().isoformat()
                }
                
                if result.stdout:
                    output_info["stdout"] = result.stdout
                    output_info["stdout_lines"] = len(result.stdout.splitlines())
                
                if result.stderr:
                    output_info["stderr"] = result.stderr
                    output_info["stderr_lines"] = len(result.stderr.splitlines())
                
                # Add success/failure indicator
                if result.returncode == 0:
                    output_info["status"] = "success"
                    if summary:
                        output_info["summary"] = f"✅ {summary}"
                else:
                    output_info["status"] = "failed"
                    if summary:
                        output_info["summary"] = f"❌ {summary}"
                
                # Format output like Amazon Q
                formatted_output = self._format_command_output(output_info)
                output_info["formatted_output"] = formatted_output
                
                return output_info
            
            else:
                # Non-capturing execution for interactive commands
                result = subprocess.run(
                    command,
                    shell=True,
                    timeout=timeout,
                    env=env,
                    executable=shell_path if os.path.exists(shell_path) else None
                )
                
                return {
                    "command": command,
                    "exit_status": result.returncode,
                    "status": "success" if result.returncode == 0 else "failed",
                    "execution_time": round(time.time() - start_time, 3),
                    "note": "Output not captured (interactive mode)"
                }
        
        except subprocess.TimeoutExpired:
            return {
                "error": f"Command timed out after {timeout} seconds",
                "command": command,
                "timeout": timeout
            }
        except Exception as e:
            return {
                "error": f"Command execution failed: {str(e)}",
                "command": command
            }
        finally:
            # Restore original working directory
            if working_dir and working_dir != original_cwd:
                os.chdir(original_cwd)

    def _format_command_output(self, output_info: Dict[str, Any]) -> str:
        """Format command output in Amazon Q style"""
        lines = []
        
        # Header
        status_icon = "✅" if output_info["status"] == "success" else "❌"
        lines.append(f"{status_icon} Command: {output_info['command']}")
        lines.append(f"📁 Working Directory: {output_info['working_directory']}")
        lines.append(f"⏱️  Execution Time: {output_info['execution_time']}s")
        lines.append(f"🔢 Exit Code: {output_info['exit_status']}")
        
        # Stdout
        if output_info.get("stdout"):
            lines.append(f"\n📤 Output ({output_info.get('stdout_lines', 0)} lines):")
            lines.append("```")
            lines.append(output_info["stdout"].rstrip())
            lines.append("```")
        
        # Stderr
        if output_info.get("stderr"):
            lines.append(f"\n⚠️  Error Output ({output_info.get('stderr_lines', 0)} lines):")
            lines.append("```")
            lines.append(output_info["stderr"].rstrip())
            lines.append("```")
        
        # Summary
        if output_info.get("summary"):
            lines.append(f"\n{output_info['summary']}")
        
        return "\n".join(lines)

    def _get_command_suggestions(self, command: str, error: str) -> List[str]:
        """Provide helpful suggestions for failed commands"""
        suggestions = []
        
        if "command not found" in error.lower():
            cmd_name = command.split()[0]
            suggestions.append(f"Install {cmd_name}: sudo apt install {cmd_name} (Ubuntu/Debian)")
            suggestions.append(f"Check if {cmd_name} is in PATH: which {cmd_name}")
        
        elif "permission denied" in error.lower():
            suggestions.append(f"Try with sudo: sudo {command}")
            suggestions.append("Check file permissions: ls -la")
        
        elif "no such file or directory" in error.lower():
            suggestions.append("Check if the file/directory exists")
            suggestions.append("Use absolute path instead of relative path")
        
        elif "connection refused" in error.lower():
            suggestions.append("Check if the service is running")
            suggestions.append("Verify network connectivity")
        
        return suggestions

    def _memory_manager(self, action: str, **kwargs) -> Dict[str, Any]:
        """Manage conversation memory and token budget"""
        messages = kwargs.get("messages", [])
        
        if action == "status":
            token_info = self.get_token_info(messages)
            return {
                "token_usage": token_info,
                "message_count": len(messages),
                "status": "healthy" if not token_info["needs_compression"] else "needs_compression",
                "recommendation": "Continue normally" if not token_info["needs_compression"] else "Compression recommended"
            }
        
        elif action == "compress":
            if len(messages) <= 2:
                return {"error": "Not enough messages to compress"}
            
            compressed, was_compressed = self.check_and_compress_history(messages)
            if was_compressed:
                original_tokens = self.token_manager.count_conversation_tokens(messages)
                new_tokens = self.token_manager.count_conversation_tokens(compressed)
                return {
                    "success": True,
                    "compressed": True,
                    "original_messages": len(messages),
                    "compressed_messages": len(compressed),
                    "original_tokens": original_tokens,
                    "compressed_tokens": new_tokens,
                    "tokens_saved": original_tokens - new_tokens,
                    "compression_ratio": round((1 - new_tokens/original_tokens) * 100, 1),
                    "compressed_history": compressed
                }
            else:
                return {"success": True, "compressed": False, "reason": "No compression needed"}
        
        elif action == "optimize":
            token_info = self.get_token_info(messages)
            if token_info["needs_compression"]:
                return self._memory_manager("compress", messages=messages)
            else:
                return {
                    "success": True,
                    "action_taken": "none",
                    "reason": "Memory usage within acceptable limits",
                    "token_info": token_info
                }
        
        return {"error": "Invalid action"}

    def _code_interpreter(self, code: str, **kwargs) -> Dict[str, Any]:
        
        if action == "critique":
            return self._critique_action(context, focus_areas)
        elif action == "validate":
            return self._validate_decision(context, focus_areas)
        elif action == "reflect":
            return self._self_reflect(context)
        elif action == "improve":
            return self._suggest_improvements(context, focus_areas)
        else:
            return {"error": "Invalid introspection action"}

    def _critique_action(self, context: Dict, focus_areas: List[str]) -> Dict[str, Any]:
        """Provide constructive criticism of an action"""
        tool_used = context.get("tool_used", "")
        command = context.get("command", "")
        result = context.get("result", {})
        
        critique = {
            "overall_assessment": "analyzing...",
            "strengths": [],
            "weaknesses": [],
            "risks": [],
            "suggestions": [],
            "score": 0  # 1-10 scale
        }
        
        # Analyze based on tool used
        if tool_used == "fs_write":
            critique.update(self._critique_fs_write(context, focus_areas))
        elif tool_used == "execute_bash":
            critique.update(self._critique_bash_execution(context, focus_areas))
        elif tool_used == "fs_read":
            critique.update(self._critique_fs_read(context, focus_areas))
        else:
            critique.update(self._generic_critique(context, focus_areas))
        
        return {"critique": critique, "action_recommended": self._get_recommendation(critique)}

    def _critique_fs_write(self, context: Dict, focus_areas: List[str]) -> Dict[str, Any]:
        """Critique file writing operations"""
        command = context.get("command", "")
        result = context.get("result", {})
        
        strengths = []
        weaknesses = []
        risks = []
        suggestions = []
        score = 7  # default
        
        if "safety" in focus_areas:
            if command == "str_replace" and result.get("replacements", 0) > 1:
                risks.append("Multiple replacements could affect unintended code")
                suggestions.append("Use more specific old_str to ensure single replacement")
                score -= 1
            elif command == "create" and not result.get("show_diff"):
                weaknesses.append("No diff preview for file creation")
                suggestions.append("Enable show_diff to review changes")
            else:
                strengths.append("Safe file operation with proper validation")
        
        if "efficiency" in focus_areas:
            if result.get("size_bytes", 0) > 1024 * 1024:  # > 1MB
                weaknesses.append("Large file operation may impact performance")
                suggestions.append("Consider chunked operations for large files")
                score -= 1
            else:
                strengths.append("Efficient file size handling")
        
        if "completeness" in focus_areas:
            if not result.get("success"):
                weaknesses.append("Operation failed - incomplete action")
                suggestions.append("Check file permissions and path validity")
                score -= 2
            elif result.get("diff"):
                strengths.append("Complete with change visualization")
                score += 1
        
        return {
            "overall_assessment": f"File operation scored {score}/10",
            "strengths": strengths,
            "weaknesses": weaknesses,
            "risks": risks,
            "suggestions": suggestions,
            "score": score
        }

    def _critique_bash_execution(self, context: Dict, focus_areas: List[str]) -> Dict[str, Any]:
        """Critique bash command execution"""
        command = context.get("command", "")
        result = context.get("result", {})
        
        strengths = []
        weaknesses = []
        risks = []
        suggestions = []
        score = 7
        
        if "safety" in focus_areas:
            dangerous_commands = ["rm -rf", "sudo rm", "dd if=", "mkfs", "fdisk"]
            if any(cmd in command.lower() for cmd in dangerous_commands):
                risks.append("Potentially destructive command detected")
                suggestions.append("Use safer alternatives or add confirmation")
                score -= 3
            elif result.get("exit_status") != 0:
                weaknesses.append("Command failed - potential system impact")
                suggestions.append("Check error output and validate command syntax")
                score -= 1
            else:
                strengths.append("Safe command execution")
        
        if "efficiency" in focus_areas:
            exec_time = result.get("execution_time", 0)
            if exec_time > 10:
                weaknesses.append(f"Long execution time: {exec_time}s")
                suggestions.append("Consider background execution or optimization")
                score -= 1
            elif exec_time < 0.1:
                strengths.append("Fast execution")
                score += 1
        
        return {
            "overall_assessment": f"Command execution scored {score}/10",
            "strengths": strengths,
            "weaknesses": weaknesses,
            "risks": risks,
            "suggestions": suggestions,
            "score": score
        }

    def _critique_fs_read(self, context: Dict, focus_areas: List[str]) -> Dict[str, Any]:
        """Critique file reading operations"""
        result = context.get("result", {})
        
        strengths = []
        weaknesses = []
        suggestions = []
        score = 8
        
        if "efficiency" in focus_areas:
            results = result.get("results", [])
            for res in results:
                if res.get("error"):
                    weaknesses.append(f"Failed to read: {res.get('path')}")
                    score -= 1
                elif "tree" in res and len(res["tree"].split('\n')) > 100:
                    weaknesses.append("Large tree output may be overwhelming")
                    suggestions.append("Use max_depth parameter to limit output")
                    score -= 1
                else:
                    strengths.append("Efficient file reading strategy")
        
        return {
            "overall_assessment": f"File reading scored {score}/10",
            "strengths": strengths,
            "weaknesses": weaknesses,
            "risks": [],
            "suggestions": suggestions,
            "score": score
        }

    def _generic_critique(self, context: Dict, focus_areas: List[str]) -> Dict[str, Any]:
        """Generic critique for unknown actions"""
        return {
            "overall_assessment": "Generic action - limited analysis available",
            "strengths": ["Action completed"],
            "weaknesses": ["No specific analysis available"],
            "risks": [],
            "suggestions": ["Provide more context for detailed critique"],
            "score": 6
        }

    def _validate_decision(self, context: Dict, focus_areas: List[str]) -> Dict[str, Any]:
        """Validate a decision or approach"""
        decision_type = context.get("decision_type", "unknown")
        
        validation = {
            "is_valid": True,
            "confidence": 0.8,
            "concerns": [],
            "alternatives": [],
            "recommendation": "proceed"
        }
        
        # Add decision-specific validation logic
        if "architecture" in decision_type.lower():
            validation["alternatives"].append("Consider microservices vs monolith trade-offs")
            validation["concerns"].append("Scalability implications")
        elif "security" in decision_type.lower():
            validation["concerns"].append("Ensure compliance with security standards")
            validation["alternatives"].append("Multi-factor authentication options")
        
        return {"validation": validation}

    def _self_reflect(self, context: Dict) -> Dict[str, Any]:
        """Self-reflection on overall performance"""
        return {
            "reflection": {
                "performance_areas": ["tool_usage", "decision_making", "communication"],
                "strengths": ["Systematic approach", "Tool integration"],
                "improvement_areas": ["Context awareness", "Error handling"],
                "learning_points": ["User feedback incorporation", "Efficiency optimization"],
                "confidence_level": 0.75
            }
        }

    def _suggest_improvements(self, context: Dict, focus_areas: List[str]) -> Dict[str, Any]:
        """Suggest specific improvements"""
        improvements = []
        
        for area in focus_areas:
            if area == "efficiency":
                improvements.append("Use batch operations for multiple file operations")
                improvements.append("Implement caching for repeated operations")
            elif area == "safety":
                improvements.append("Add confirmation prompts for destructive operations")
                improvements.append("Implement rollback mechanisms")
            elif area == "completeness":
                improvements.append("Add comprehensive error handling")
                improvements.append("Provide detailed operation summaries")
        
        return {"improvements": improvements, "priority": "high"}

    def _get_recommendation(self, critique: Dict) -> str:
        """Get action recommendation based on critique"""
        score = critique.get("score", 5)
        
        if score >= 8:
            return "proceed_confidently"
        elif score >= 6:
            return "proceed_with_caution"
        elif score >= 4:
            return "revise_approach"
        else:
            return "stop_and_reconsider"

    def _debate_agent(self, decision: str, context: str, **kwargs) -> Dict[str, Any]:
        """Multi-perspective debate agent for design decisions"""
        perspectives = kwargs.get("perspectives", ["security", "performance", "maintainability"])
        debate_style = kwargs.get("debate_style", "pros_cons")
        
        if debate_style == "pros_cons":
            return self._pros_cons_analysis(decision, context, perspectives)
        elif debate_style == "alternatives":
            return self._alternatives_analysis(decision, context, perspectives)
        elif debate_style == "devil_advocate":
            return self._devil_advocate_analysis(decision, context, perspectives)
        elif debate_style == "stakeholder_views":
            return self._stakeholder_analysis(decision, context, perspectives)
        
        return {"error": "Invalid debate style"}

    def _pros_cons_analysis(self, decision: str, context: str, perspectives: List[str]) -> Dict[str, Any]:
        """Analyze pros and cons from multiple perspectives"""
        analysis = {"decision": decision, "perspectives": {}}
        
        for perspective in perspectives:
            pros, cons = self._get_perspective_pros_cons(decision, perspective, context)
            analysis["perspectives"][perspective] = {"pros": pros, "cons": cons}
        
        # Overall recommendation
        total_pros = sum(len(p["pros"]) for p in analysis["perspectives"].values())
        total_cons = sum(len(p["cons"]) for p in analysis["perspectives"].values())
        
        analysis["recommendation"] = {
            "overall_score": total_pros - total_cons,
            "confidence": min(0.9, max(0.3, abs(total_pros - total_cons) / max(total_pros + total_cons, 1))),
            "action": "approve" if total_pros > total_cons else "reconsider"
        }
        
        return analysis

    def _get_perspective_pros_cons(self, decision: str, perspective: str, context: str) -> tuple:
        """Get pros and cons for a specific perspective"""
        pros = []
        cons = []
        
        if perspective == "security":
            if "authentication" in decision.lower():
                pros.append("user verification")
                cons.append("Additional complexity for users")
            if "encryption" in decision.lower():
                pros.append("Data protection in transit/rest")
                cons.append("Performance overhead")
        
        elif perspective == "performance":
            if "cache" in decision.lower():
                pros.append("Faster response times")
                cons.append("Memory usage increase")
            if "database" in decision.lower():
                pros.append("Optimized data access")
                cons.append("Potential bottleneck")
        
        elif perspective == "maintainability":
            if "microservice" in decision.lower():
                pros.append("Independent deployment")
                cons.append("Increased operational complexity")
            if "monolith" in decision.lower():
                pros.append("Simpler deployment")
                cons.append("Harder to scale specific components")
        
        # Generic fallbacks
        if not pros:
            pros.append(f"Addresses {perspective} requirements")
        if not cons:
            cons.append(f"May impact {perspective} considerations")
        
        return pros, cons

    def _alternatives_analysis(self, decision: str, context: str, perspectives: List[str]) -> Dict[str, Any]:
        """Generate alternative approaches"""
        alternatives = []
        
        # Generate alternatives based on decision type
        if "database" in decision.lower():
            alternatives.extend(["SQL database", "NoSQL database", "In-memory cache", "File-based storage"])
        elif "architecture" in decision.lower():
            alternatives.extend(["Microservices", "Monolith", "Serverless", "Event-driven"])
        elif "deployment" in decision.lower():
            alternatives.extend(["Docker containers", "Virtual machines", "Kubernetes", "Serverless functions"])
        else:
            alternatives.extend(["Current approach", "Simplified version", "version", "Alternative technology"])
        
        # Score each alternative
        scored_alternatives = []
        for alt in alternatives[:4]:  # Limit to 4 alternatives
            score = self._score_alternative(alt, perspectives, context)
            scored_alternatives.append({"alternative": alt, "score": score, "reasoning": f"Evaluated against {', '.join(perspectives)}"})
        
        return {
            "original_decision": decision,
            "alternatives": scored_alternatives,
            "recommendation": max(scored_alternatives, key=lambda x: x["score"])
        }

    def _score_alternative(self, alternative: str, perspectives: List[str], context: str) -> float:
        """Score an alternative based on perspectives"""
        base_score = 0.5
        
        # Simple scoring logic
        for perspective in perspectives:
            if perspective == "security" and "secure" in alternative.lower():
                base_score += 0.2
            elif perspective == "performance" and any(word in alternative.lower() for word in ["fast", "cache", "optimized"]):
                base_score += 0.2
            elif perspective == "maintainability" and any(word in alternative.lower() for word in ["simple", "clean", "modular"]):
                base_score += 0.2
        
        return min(1.0, base_score)

    def _devil_advocate_analysis(self, decision: str, context: str, perspectives: List[str]) -> Dict[str, Any]:
        """Play devil's advocate against the decision"""
        challenges = []
        
        challenges.append(f"What if {decision} fails under high load?")
        challenges.append(f"How does {decision} handle edge cases?")
        challenges.append(f"What are the long-term maintenance costs of {decision}?")
        challenges.append(f"Are there simpler alternatives to {decision}?")
        
        return {
            "decision": decision,
            "challenges": challenges,
            "risk_assessment": "medium",
            "mitigation_strategies": [
                "Implement comprehensive testing",
                "Plan rollback procedures",
                "Monitor key metrics",
                "Document decision rationale"
            ]
        }

    def _stakeholder_analysis(self, decision: str, context: str, perspectives: List[str]) -> Dict[str, Any]:
        """Analyze from different stakeholder viewpoints"""
        stakeholders = {
            "developers": {"concerns": ["Complexity", "Learning curve"], "benefits": ["Better tools", "Cleaner code"]},
            "operations": {"concerns": ["Deployment complexity", "Monitoring"], "benefits": ["Automation", "Reliability"]},
            "users": {"concerns": ["Performance impact", "Downtime"], "benefits": ["Better experience", "New features"]},
            "management": {"concerns": ["Cost", "Timeline"], "benefits": ["ROI", "Competitive advantage"]}
        }
        
        return {
            "decision": decision,
            "stakeholder_views": stakeholders,
            "consensus_level": "moderate",
            "key_negotiations": ["Timeline vs Quality", "Cost vs Features"]
        }
        
    def _todo(self, task: str, action: str) -> Dict[str, Any]:
        """Break down big tasks into smaller chunks"""
        task_hash = str(hash(task))
        
        if action == "plan":
            steps = [
                {"description": "Analyze requirements"},
                {"description": "Create project structure"},
                {"description": "Implement core functionality"},
                {"description": "Add tests and documentation"}
            ]
            self._task_plans[task_hash] = {"task": task, "steps": steps, "completed": []}
            return {"task": task, "steps": steps, "total_steps": len(steps)}
        
        elif action == "execute":
            if task_hash in self._task_plans:
                plan = self._task_plans[task_hash]
                next_step = len(plan["completed"])
                if next_step < len(plan["steps"]):
                    step = plan["steps"][next_step]
                    plan["completed"].append(next_step)
                    return {"step": step, "progress": f"{len(plan['completed'])}/{len(plan['steps'])}"}
            return {"message": "Task not found or completed"}
        
        elif action == "status":
            if task_hash in self._task_plans:
                plan = self._task_plans[task_hash]
                return {"progress": f"{len(plan['completed'])}/{len(plan['steps'])}"}
            return {"error": "Task not found"}
        
        return {"error": "Invalid action"}

    def _break_down_task(self, task: str) -> List[Dict[str, Any]]:
        """Break down a complex task into smaller steps"""
        # Simple heuristic-based task breakdown
        task_lower = task.lower()
        steps = []
        
        # Common patterns for different types of tasks
        if "create" in task_lower and ("app" in task_lower or "application" in task_lower):
            steps = [
                {"type": "fs_write", "description": "Create project directory structure", "action": "setup_project"},
                {"type": "fs_write", "description": "Create main application file", "action": "create_main"},
                {"type": "fs_write", "description": "Create configuration files", "action": "create_config"},
                {"type": "fs_write", "description": "Create requirements/dependencies file", "action": "create_deps"},
                {"type": "execute_bash", "description": "Install dependencies", "action": "install_deps"},
                {"type": "fs_write", "description": "Create basic tests", "action": "create_tests"},
                {"type": "execute_bash", "description": "Run tests to verify setup", "action": "run_tests"}
            ]
        elif "deploy" in task_lower:
            steps = [
                {"type": "fs_write", "description": "Create Dockerfile", "action": "create_dockerfile"},
                {"type": "fs_write", "description": "Create docker-compose.yml", "action": "create_compose"},
                {"type": "execute_bash", "description": "Build Docker image", "action": "build_image"},
                {"type": "execute_bash", "description": "Test container locally", "action": "test_container"},
                {"type": "fs_write", "description": "Create deployment manifests", "action": "create_manifests"},
                {"type": "execute_bash", "description": "Deploy to environment", "action": "deploy"}
            ]
        elif "api" in task_lower:
            steps = [
                {"type": "fs_write", "description": "Create API structure", "action": "create_api_structure"},
                {"type": "fs_write", "description": "Define data models", "action": "create_models"},
                {"type": "fs_write", "description": "Create API endpoints", "action": "create_endpoints"},
                {"type": "fs_write", "description": "Add authentication", "action": "add_auth"},
                {"type": "fs_write", "description": "Create API documentation", "action": "create_docs"},
                {"type": "execute_bash", "description": "Test API endpoints", "action": "test_api"}
            ]
        else:
            # Generic breakdown
            steps = [
                {"type": "fs_read", "description": "Analyze current state", "action": "analyze"},
                {"type": "fs_write", "description": "Create necessary files", "action": "create_files"},
                {"type": "execute_bash", "description": "Execute required commands", "action": "execute_commands"},
                {"type": "fs_read", "description": "Verify results", "action": "verify"}
            ]
        
        return steps

    def _execute_step(self, step: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a single step of a task"""
        step_type = step.get("type")
        description = step.get("description")
        action = step.get("action")
        
        try:
            if step_type == "fs_write":
                # Create a placeholder file or directory based on the action
                if action == "setup_project":
                    self._fs_write("create", "./project/README.md", file_text="# Project\n\nGenerated by todo tool")
                    return {"success": True, "message": "Project structure created"}
                elif action == "create_main":
                    self._fs_write("create", "./project/main.py", file_text="#!/usr/bin/env python3\n# Main application file\n\nif __name__ == '__main__':\n    print('Hello World')")
                    return {"success": True, "message": "Main file created"}
                else:
                    return {"success": True, "message": f"Step '{description}' planned for execution"}
            
            elif step_type == "execute_bash":
                if action == "install_deps":
                    result = self._execute_bash("echo 'Dependencies would be installed here'")
                    return result
                elif action == "run_tests":
                    result = self._execute_bash("echo 'Tests would run here'")
                    return result
                else:
                    return {"success": True, "message": f"Command '{description}' planned for execution"}
            
            elif step_type == "fs_read":
                return {"success": True, "message": f"Analysis step '{description}' completed"}
            
            else:
                return {"success": True, "message": f"Step '{description}' completed"}
                
        except Exception as e:
            return {"error": f"Failed to execute step: {str(e)}"}

    def _code_interpreter(self, code: str, **kwargs) -> Dict[str, Any]:
        """Execute Python code with visualization and result capture"""
        language = kwargs.get("language", "python")
        save_plots = kwargs.get("save_plots", True)
        timeout = kwargs.get("timeout", 30)
        working_dir = kwargs.get("working_dir", os.getcwd())
        
        if language != "python":
            return {"error": "Only Python is currently supported"}
        
        original_cwd = os.getcwd()
        plot_files = []
        
        try:
            os.chdir(working_dir)
            stdout_capture = io.StringIO()
            stderr_capture = io.StringIO()
            
            # Prepare code with plot saving
            if save_plots:
                plot_setup = '''
                            import matplotlib
                            matplotlib.use('Agg')
                            import matplotlib.pyplot as plt
                            import uuid
                            _plot_counter = 0
                            
                            def _save_plot():
                                global _plot_counter
                                if plt.get_fignums():
                                    filename = f'plot_{_plot_counter}_{uuid.uuid4().hex[:8]}.png'
                                    plt.savefig(filename, dpi=150, bbox_inches='tight')
                                    _plot_counter += 1
                                    return filename
                                return None
                            
                            _original_show = plt.show
                            def _custom_show(*args, **kwargs):
                                filename = _save_plot()
                                if filename:
                                    print(f'Plot saved: {filename}')
                                plt.close('all')
                            plt.show = _custom_show
                            '''
                code = plot_setup + "\n" + code
            
            start_time = time.time()
            
            with contextlib.redirect_stdout(stdout_capture), contextlib.redirect_stderr(stderr_capture):
                exec_globals = {'__name__': '__main__', '__builtins__': __builtins__}
                exec(code, exec_globals)
            
            execution_time = time.time() - start_time
            stdout_content = stdout_capture.getvalue()
            stderr_content = stderr_capture.getvalue()
            
            # Find plot files
            current_plots = [f for f in os.listdir('.') if f.startswith('plot_') and f.endswith('.png')]
            plot_files.extend(current_plots)
            
            # Encode plots
            plot_data = []
            for plot_file in plot_files:
                if os.path.exists(plot_file):
                    with open(plot_file, 'rb') as f:
                        plot_data.append({
                            "filename": plot_file,
                            "data": base64.b64encode(f.read()).decode('utf-8'),
                            "size": os.path.getsize(plot_file)
                        })
            
            return {
                "success": True,
                "execution_time": round(execution_time, 3),
                "stdout": stdout_content,
                "stderr": stderr_content,
                "plots": plot_data,
                "plot_count": len(plot_files)
            }
            
        except Exception as e:
            return {"error": f"Execution failed: {str(e)}"}
        finally:
            os.chdir(original_cwd)

    def _repo_quality_analyzer(self, repo_path: str, **kwargs) -> Dict[str, Any]:
        """Comprehensive repository code quality analysis"""
        analysis_type = kwargs.get("analysis_type", "full")
        
        repo_path = Path(repo_path)
        if not repo_path.exists():
            return {"error": f"Repository path does not exist: {repo_path}"}
        
        # Discover Python files
        python_files = list(repo_path.rglob("*.py"))
        
        analysis = {
            "repository": str(repo_path),
            "files_analyzed": len(python_files),
            "timestamp": datetime.now().isoformat(),
            "recommendations": []
        }
        
        # Basic metrics
        total_lines = 0
        total_functions = 0
        security_issues = []
        
        for file_path in python_files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    lines = content.splitlines()
                    total_lines += len(lines)
                
                # Security check
                for line_num, line in enumerate(lines, 1):
                    if any(pattern in line.lower() for pattern in ['password =', 'api_key =', 'secret =']):
                        security_issues.append({
                            "file": str(file_path),
                            "line": line_num,
                            "issue": "Potential hardcoded secret"
                        })
                
                # AST analysis
                try:
                    tree = ast.parse(content)
                    for node in ast.walk(tree):
                        if isinstance(node, ast.FunctionDef):
                            total_functions += 1
                except:
                    pass
                    
            except Exception:
                continue
        
        # Generate recommendations
        if len(security_issues) > 0:
            analysis["recommendations"].append({
                "category": "security",
                "severity": "high",
                "title": f"Found {len(security_issues)} potential security issues",
                "action": "Review hardcoded secrets and use environment variables"
            })
        
        if total_lines > 10000:
            analysis["recommendations"].append({
                "category": "maintainability",
                "severity": "medium", 
                "title": "Large codebase detected",
                "action": "Consider modularization and documentation"
            })
        
        analysis["summary"] = {
            "total_lines": total_lines,
            "total_functions": total_functions,
            "security_issues": len(security_issues),
            "quality_score": max(0, 100 - len(security_issues) * 10)
        }
        
        return analysis

    def _documentation_generator(self, repo_path: str, doc_type: str, **kwargs) -> Dict[str, Any]:
        """Generate  documentation using nbdev_preview and HTML enhancement"""
        output_format = kwargs.get("output_format", "html")
        output_path = kwargs.get("output_path", "docs")
        include_examples = kwargs.get("include_examples", True)
        target_audience = kwargs.get("target_audience", "developers")
        
        repo_path = Path(repo_path)
        if not repo_path.exists():
            return {"error": f"Repository path does not exist: {repo_path}"}
        
        try:
            # Check if it's an nbdev project
            if not (repo_path / "settings.ini").exists() and not (repo_path / "nbs").exists():
                return {"error": "Not an nbdev project. settings.ini or nbs folder not found."}
            
            original_cwd = os.getcwd()
            os.chdir(repo_path)
            
            # Run nbdev_preview to generate base documentation
            result = subprocess.run(
                ["nbdev_preview", "--port", "0", "--no_browser"], 
                capture_output=True, text=True, timeout=60
            )
            
            if result.returncode != 0:
                return {"error": f"nbdev_preview failed: {result.stderr}"}
            
            # Find generated _docs directory
            docs_dir = repo_path / "_docs"
            if not docs_dir.exists():
                return {"error": "_docs directory not found after nbdev_preview"}
            
            # Analyze repository for enhancements
            repo_analysis = self._analyze_nbdev_repo(repo_path)
            
            # Enhance HTML files
            enhanced_files = self._enhance_html_files(docs_dir, repo_analysis, include_examples, target_audience)
            
            # Copy  docs to output path if different
            if output_path != "_docs":
                output_dir = Path(output_path)
                if output_dir.exists():
                    shutil.rmtree(output_dir)
                shutil.copytree(docs_dir, output_dir)
                final_docs_dir = output_dir
            else:
                final_docs_dir = docs_dir
            
            return {
                "success": True,
                "doc_type": doc_type,
                "output_directory": str(final_docs_dir),
                "enhanced_files": enhanced_files,
                "base_generator": "nbdev_preview",
                "enhancements_applied": len(enhanced_files),
                "target_audience": target_audience
            }
            
        except subprocess.TimeoutExpired:
            return {"error": "nbdev_preview timed out"}
        except Exception as e:
            return {"error": f"Documentation generation failed: {str(e)}"}
        finally:
            os.chdir(original_cwd)

    def _analyze_nbdev_repo(self, repo_path: Path) -> Dict[str, Any]:
        """Analyze nbdev repository structure"""
        analysis = {
            "name": repo_path.name,
            "notebooks": [],
            "modules": [],
            "functions": [],
            "classes": [],
            "examples": [],
            "settings": {}
        }
        
        # Read settings.ini
        settings_file = repo_path / "settings.ini"
        if settings_file.exists():
            try:
                with open(settings_file, 'r') as f:
                    for line in f:
                        if '=' in line and not line.strip().startswith('#'):
                            key, value = line.strip().split('=', 1)
                            analysis["settings"][key.strip()] = value.strip()
            except:
                pass
        
        # Analyze notebooks
        nbs_dir = repo_path / "nbs"
        if nbs_dir.exists():
            for nb_file in nbs_dir.rglob("*.ipynb"):
                try:
                    with open(nb_file, 'r', encoding='utf-8') as f:
                        nb_content = json.load(f)
                    
                    nb_info = {
                        "name": nb_file.name,
                        "path": str(nb_file.relative_to(repo_path)),
                        "cells": len(nb_content.get("cells", [])),
                        "exports": [],
                        "examples": []
                    }
                    
                    # Extract exports and examples
                    for cell in nb_content.get("cells", []):
                        if cell.get("cell_type") == "code":
                            source = "".join(cell.get("source", []))
                            if "# | export" in source:
                                nb_info["exports"].append(source)
                            if any(keyword in source.lower() for keyword in ["example", "demo", "usage"]):
                                nb_info["examples"].append(source)
                    
                    analysis["notebooks"].append(nb_info)
                except:
                    continue
        
        return analysis

    def _enhance_html_files(self, docs_dir: Path, analysis: Dict[str, Any], include_examples: bool, target_audience: str) -> List[str]:
        """Enhance generated HTML files with additional content"""
        enhanced_files = []
        
        for html_file in docs_dir.rglob("*.html"):
            try:
                with open(html_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Parse HTML
                from html.parser import HTMLParser
                
                # Add enhancements
                enhanced_content = self._add_html_enhancements(content, analysis, include_examples, target_audience, html_file.name)
                
                # Write  content
                with open(html_file, 'w', encoding='utf-8') as f:
                    f.write(enhanced_content)
                
                enhanced_files.append(str(html_file))
                
            except Exception as e:
                continue
        
        return enhanced_files

    def _add_html_enhancements(self, content: str, analysis: Dict[str, Any], include_examples: bool, target_audience: str, filename: str) -> str:
        """Add enhancements to HTML content"""
        
        # Add custom CSS for better styling
        custom_css = """
                    <style>
                    .enhancement-box {
                        background: #f8f9fa;
                        border-left: 4px solid #007bff;
                        padding: 15px;
                        margin: 20px 0;
                        border-radius: 4px;
                    }
                    .example-code {
                        background: #f4f4f4;
                        border: 1px solid #ddd;
                        border-radius: 4px;
                        padding: 10px;
                        margin: 10px 0;
                        font-family: 'Courier New', monospace;
                    }
                    .audience-note {
                        background: #e7f3ff;
                        border: 1px solid #b3d9ff;
                        padding: 10px;
                        margin: 15px 0;
                        border-radius: 4px;
                    }
                    .repo-stats {
                        display: flex;
                        gap: 20px;
                        margin: 20px 0;
                    }
                    .stat-item {
                        background: #fff;
                        border: 1px solid #ddd;
                        padding: 10px;
                        border-radius: 4px;
                        text-align: center;
                        min-width: 100px;
                    }
                    </style>
                    """
        
        # Insert CSS before closing head tag
        if "</head>" in content:
            content = content.replace("</head>", custom_css + "\n</head>")
        
        # Add repository statistics for index page
        if filename == "index.html":
            repo_stats = f"""
                        <div class="enhancement-box">
                            <h3>📊 Repository Statistics</h3>
                            <div class="repo-stats">
                                <div class="stat-item">
                                    <strong>{len(analysis['notebooks'])}</strong><br>
                                    <small>Notebooks</small>
                                </div>
                                <div class="stat-item">
                                    <strong>{sum(len(nb['exports']) for nb in analysis['notebooks'])}</strong><br>
                                    <small>Exports</small>
                                </div>
                                <div class="stat-item">
                                    <strong>{sum(nb['cells'] for nb in analysis['notebooks'])}</strong><br>
                                    <small>Total Cells</small>
                                </div>
                            </div>
                        </div>
                        """
            
            # Insert after first h1 or at beginning of body
            if "<h1" in content:
                h1_end = content.find("</h1>") + 5
                content = content[:h1_end] + repo_stats + content[h1_end:]
            elif "<body" in content:
                body_start = content.find(">", content.find("<body")) + 1
                content = content[:body_start] + repo_stats + content[body_start:]
        
        # Add audience-specific notes
        audience_note = f"""
                        <div class="audience-note">
                            <strong>👥 Target Audience:</strong> This documentation is optimized for <em>{target_audience}</em>.
                            {"Includes practical examples and usage patterns." if include_examples else "Focuses on API reference and technical details."}
                        </div>
                        """
        
        # Insert audience note after main content starts
        if '<div class="cell border-box-sizing text_cell rendered">' in content:
            first_cell = content.find('<div class="cell border-box-sizing text_cell rendered">')
            content = content[:first_cell] + audience_note + content[first_cell:]
        
        # Add quick navigation for notebooks
        if len(analysis['notebooks']) > 1:
            nav_links = ['<div class="enhancement-box">', '<h4>📚 Quick Navigation</h4>', '<ul>']
            
            for nb in analysis['notebooks']:
                nb_name = nb['name'].replace('.ipynb', '')
                nav_links.append(f'<li><a href="{nb_name}.html">{nb_name}</a> ({nb["cells"]} cells)</li>')
            
            nav_links.extend(['</ul>', '</div>'])
            navigation = '\n'.join(nav_links)
            
            # Insert navigation
            if "<body" in content:
                body_start = content.find(">", content.find("<body")) + 1
                content = content[:body_start] + navigation + content[body_start:]
        
        # Add examples section if requested
        if include_examples:
            examples_found = []
            for nb in analysis['notebooks']:
                examples_found.extend(nb['examples'])
            
            if examples_found:
                examples_section = ['<div class="enhancement-box">', '<h4>💡 Usage Examples</h4>']
                
                for i, example in enumerate(examples_found[:3]):  # Limit to 3 examples
                    clean_example = example.replace('# | export\n', '').strip()
                    if clean_example:
                        examples_section.append(f'<div class="example-code"><pre><code>{clean_example}</code></pre></div>')
                
                examples_section.append('</div>')
                examples_html = '\n'.join(examples_section)
                
                # Insert before closing body
                if "</body>" in content:
                    content = content.replace("</body>", examples_html + "\n</body>")
        
        # Add footer with generation info
        footer = f"""
                    <div style="margin-top: 40px; padding: 20px; background: #f8f9fa; border-top: 1px solid #ddd; font-size: 0.9em; color: #666;">
                        <p>📝 Documentation generated by <strong>nbdev_preview</strong> and  by <strong>Buddy AI Agent</strong></p>
                        <p>🕒 Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                        <p>🎯 Optimized for: {target_audience}</p>
                    </div>
                    """
        
        if "</body>" in content:
            content = content.replace("</body>", footer + "\n</body>")
        
        return content

