{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c01dc636-3b46-48f2-9d95-5e29198f50e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# | default_exp backend.tools.fs_read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2cb2d1-8b7a-4bc1-aae5-b369afcc7f20",
   "metadata": {},
   "source": [
    "https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/git-aware-selection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60257385-ef59-4b8e-8298-4e070c6a7599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from fuzzywuzzy import fuzz\n",
    "import fnmatch\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pydantic import BaseModel, field_validator, Field, ValidationInfo\n",
    "from enum import Enum\n",
    "import git\n",
    "import mimetypes\n",
    "from functools import lru_cache\n",
    "\n",
    "# Set up logging (configurable level)\n",
    "logging.basicConfig(level=logging.DEBUG)  # Default to DEBUG; can be overridden\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ToolCallMode(str, Enum):\n",
    "    DISCOVER = \"discover\"\n",
    "    EXTRACT = \"extract\"\n",
    "\n",
    "class FsReadOperation(BaseModel):\n",
    "    mode: ToolCallMode\n",
    "    path: str = Field(..., description=\"Specific file or directory path to operate on, e.g., '/project/src' or 'app.py'.\")\n",
    "    query: Optional[str] = Field(None, description=\"Search query for fuzzy matching (DISCOVER) or regex/substring (EXTRACT). Required for EXTRACT.\")\n",
    "    file_pattern: str = Field(\"*.py|*.go|*.rs|*.java|*.js|*.ts|*.cpp|*.c|*.h|*.cs\", description=\"Glob filter for files, e.g., '*.py' or '*.go|*.rs'.\")\n",
    "    max_depth: Optional[int] = Field(10, description=\"Max recursion depth for DISCOVER (default 10).\")\n",
    "    context_lines: Optional[int] = Field(3, description=\"Lines around each match in EXTRACT (default 3).\")\n",
    "    regex_mode: Optional[bool] = Field(True, description=\"True treats query as regex in EXTRACT; false for substring.\")\n",
    "    start_line: Optional[int] = Field(1, description=\"Fallback start line for EXTRACT (default 1).\")\n",
    "    end_line: Optional[int] = Field(-1, description=\"Fallback end line for EXTRACT (-1 = file end).\")\n",
    "    max_files: Optional[int] = Field(100, description=\"Max files to scan in DISCOVER (default 100).\")\n",
    "    limit_results: Optional[int] = Field(20, description=\"Max results from DISCOVER (default 5).\")\n",
    "    simple_mode: Optional[bool] = Field(True, description=\"Disable scoring for simple glob-based discovery in DISCOVER mode.\")\n",
    "    minimal_output: Optional[bool] = Field(True, description=\"Return minimal output (path, name, data only) for LLM consumption.\")\n",
    "\n",
    "    @field_validator(\"path\")\n",
    "    @classmethod\n",
    "    def validate_path(cls, value: str, info: ValidationInfo) -> str:\n",
    "        \"\"\"Validate and normalize the path, ensuring it exists and is within the project root if applicable.\n",
    "\n",
    "        Args:\n",
    "            value: Path to validate.\n",
    "            info: Validation context containing optional repo information.\n",
    "\n",
    "        Returns:\n",
    "            Normalized path string.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If path is invalid or outside project root.\n",
    "        \"\"\"\n",
    "        if not value:\n",
    "            value = os.getcwd()\n",
    "            logger.info(f\"Path empty; defaulting to project root: {value}\")\n",
    "        path_obj = Path(value)\n",
    "        if not path_obj.exists():\n",
    "            raise ValueError(f\"Path {value} does not exist\")\n",
    "        repo = info.context.get(\"repo\") if info.context is not None else None\n",
    "        if repo and not str(path_obj).startswith(repo.working_dir):\n",
    "            raise ValueError(f\"Path {value} is outside the project root {repo.working_dir}\")\n",
    "        return str(path_obj)\n",
    "\n",
    "    @field_validator(\"query\")\n",
    "    @classmethod\n",
    "    def validate_query(cls, value: Any, info: ValidationInfo) -> Any:\n",
    "        \"\"\"Ensure query is provided for EXTRACT mode and normalize for DISCOVER.\n",
    "\n",
    "        Args:\n",
    "            value: Query value to validate.\n",
    "            info: Validation context.\n",
    "\n",
    "        Returns:\n",
    "            Validated query or empty string for DISCOVER mode.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If query is missing in EXTRACT mode.\n",
    "        \"\"\"\n",
    "        mode = info.data.get(\"mode\")\n",
    "        if mode == ToolCallMode.EXTRACT and value is None:\n",
    "            raise ValueError(\"Query is required for extract mode to perform regex or substring matching.\")\n",
    "        if mode == ToolCallMode.DISCOVER and value is None:\n",
    "            return \"\"\n",
    "        return value\n",
    "\n",
    "    @field_validator(\"file_pattern\")\n",
    "    @classmethod\n",
    "    def validate_file_pattern(cls, value: Any, info: ValidationInfo) -> Any:\n",
    "        \"\"\"Validate file_pattern as a valid glob and set mode-specific defaults.\n",
    "\n",
    "        Args:\n",
    "            value: File pattern to validate.\n",
    "            info: Validation context.\n",
    "\n",
    "        Returns:\n",
    "            Validated file pattern.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If file_pattern is invalid.\n",
    "        \"\"\"\n",
    "        mode = info.data.get(\"mode\")\n",
    "        if value is None:\n",
    "            return \"*.py|*.go|*.rs|*.java|*.js|*.ts|*.cpp|*.c|*.h|*.cs\" if mode == ToolCallMode.EXTRACT else \"*\"\n",
    "        if not isinstance(value, str):\n",
    "            raise ValueError(\"file_pattern must be a string\")\n",
    "        try:\n",
    "            for pattern in value.split('|'):\n",
    "                fnmatch.fnmatch(\"test.txt\", pattern)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Invalid glob pattern: {value} ({str(e)})\")\n",
    "        return value\n",
    "\n",
    "class FsReadParams(BaseModel):\n",
    "    operations: List[FsReadOperation]\n",
    "\n",
    "class FsReadTool:\n",
    "    def __init__(self, log_level: str = \"INFO\"):\n",
    "        \"\"\"Initialize the FsReadTool with Git integration and exclusion patterns.\n",
    "\n",
    "        Args:\n",
    "            log_level: Logging level (e.g., 'DEBUG', 'INFO', 'WARNING'). Defaults to 'INFO'.\n",
    "        \"\"\"\n",
    "        logging.getLogger().setLevel(getattr(logging, log_level, logging.INFO))\n",
    "        self.repo = None\n",
    "        self.gitignore_patterns = []\n",
    "        self.git_status_cache = {}\n",
    "        self.common_exclusions = [\n",
    "            \".*\", \"*.pyc\", \"*.o\", \"*.obj\", \"*.class\", \"*.exe\", \"*.dll\", \"*.so\",\n",
    "            \"*.lock\", \"node_modules/*\", \"dist/*\", \"build/*\", \"__pycache__/*\",\n",
    "            \"*.bin\", \"*.zip\", \"*.tar.gz\", \"*.log\"\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            self.repo = git.Repo(os.getcwd(), search_parent_directories=True)\n",
    "            self.gitignore_patterns = self._load_gitignore()\n",
    "            self._batch_load_git_status()\n",
    "        except (git.InvalidGitRepositoryError, git.NoSuchPathError):\n",
    "            logger.debug(\"No Git repository found; proceeding without Git integration.\")\n",
    "\n",
    "    def _load_gitignore(self) -> List[str]:\n",
    "        \"\"\"Load .gitignore patterns recursively from repo root and subdirs.\n",
    "\n",
    "        Returns:\n",
    "            List of exclusion patterns including .gitignore and common exclusions.\n",
    "        \"\"\"\n",
    "        patterns = self.common_exclusions.copy()\n",
    "        try:\n",
    "            repo_root = Path(self.repo.working_dir)\n",
    "            for gitignore in repo_root.rglob(\".gitignore\"):\n",
    "                try:\n",
    "                    with open(gitignore, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        for line in f:\n",
    "                            line = line.strip()\n",
    "                            if line and not line.startswith('#'):\n",
    "                                rel_path = str(gitignore.parent.relative_to(repo_root)) + '/' + line\n",
    "                                if rel_path.startswith('./'):\n",
    "                                    rel_path = rel_path[2:]\n",
    "                                patterns.append(rel_path)\n",
    "                except (OSError, UnicodeDecodeError) as e:\n",
    "                    logger.debug(f\"Failed to read {gitignore}: {e}\")\n",
    "        except (OSError, Exception) as e:\n",
    "            logger.warning(f\"Failed to load .gitignore patterns: {e}; using common exclusions only.\")\n",
    "        return patterns\n",
    "\n",
    "    def _batch_load_git_status(self):\n",
    "        \"\"\"Batch load Git status for all files, capturing index and working tree status.\"\"\"\n",
    "        try:\n",
    "            status_output = self.repo.git.status('--porcelain', '-u', 'all')\n",
    "            for line in status_output.split('\\n'):\n",
    "                if line.strip():\n",
    "                    parts = line.split(maxsplit=1)\n",
    "                    if len(parts) >= 2:\n",
    "                        status_code = parts[0]  # e.g., \" M\", \"AM\", \"??\"\n",
    "                        rel_path = parts[1].strip()\n",
    "                        self.git_status_cache[rel_path] = {\n",
    "                            \"index\": status_code[0] if status_code[0] != \" \" else None,\n",
    "                            \"working_tree\": status_code[1] if len(status_code) > 1 and status_code[1] != \" \" else None\n",
    "                        }\n",
    "        except (OSError, git.GitCommandError) as e:\n",
    "            logger.warning(f\"Failed to batch-load Git status: {e}; falling back to per-file.\")\n",
    "\n",
    "    def refresh_git_status(self):\n",
    "        \"\"\"Refresh the Git status cache.\"\"\"\n",
    "        self.git_status_cache.clear()\n",
    "        self._batch_load_git_status()\n",
    "\n",
    "    def _is_excluded(self, rel_path: str) -> bool:\n",
    "        \"\"\"Check if a path is excluded by .gitignore or common patterns.\n",
    "\n",
    "        Args:\n",
    "            rel_path: Relative path to check.\n",
    "\n",
    "        Returns:\n",
    "            True if the path is excluded, False otherwise.\n",
    "        \"\"\"\n",
    "        return any(fnmatch.fnmatch(rel_path, p) for p in self.gitignore_patterns)\n",
    "\n",
    "    def _get_file_info(self, path: str, lightweight: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Retrieve file metadata, respecting .gitignore and detecting binary files.\n",
    "\n",
    "        Args:\n",
    "            path: File path to analyze.\n",
    "            lightweight: If True, skip detailed metadata like line counts.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with file metadata or error details.\n",
    "        \"\"\"\n",
    "        path_obj = Path(path)\n",
    "        if not path_obj.exists():\n",
    "            return {\"error\": \"File not found\"}\n",
    "\n",
    "        try:\n",
    "            stat = path_obj.stat()\n",
    "        except OSError as e:\n",
    "            return {\"error\": f\"Stat failed: {e}\"}\n",
    "\n",
    "        rel_path = path_obj.name\n",
    "        if self.repo:\n",
    "            try:\n",
    "                rel_path = str(path_obj.relative_to(self.repo.working_dir))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if self._is_excluded(rel_path):\n",
    "            return {\"error\": \"Excluded by .gitignore or common patterns\"}\n",
    "\n",
    "        file_size = stat.st_size\n",
    "        mtime = stat.st_mtime\n",
    "        recency_boost = 1.0 if time.time() - mtime < 86400 else 0.5\n",
    "\n",
    "        mime_type, _ = mimetypes.guess_type(path)\n",
    "        is_binary = mime_type is None or not mime_type.startswith('text/')\n",
    "\n",
    "        file_info = {\n",
    "            \"size\": file_size,\n",
    "            \"is_binary\": is_binary,\n",
    "            \"file_type\": path_obj.suffix.lower(),\n",
    "            \"is_large\": file_size > 1024 * 1024,\n",
    "            \"mtime\": mtime,\n",
    "            \"recency_boost\": recency_boost,\n",
    "            \"git_status\": None,\n",
    "        }\n",
    "\n",
    "        if self.repo:\n",
    "            rel_path = os.path.relpath(path, self.repo.working_dir)\n",
    "            file_info[\"git_status\"] = self.git_status_cache.get(rel_path, {\"index\": None, \"working_tree\": None})\n",
    "            if file_info[\"git_status\"].get(\"index\") in [\"M\", \"A\"] or file_info[\"git_status\"].get(\"working_tree\") in [\"M\", \"A\"]:\n",
    "                file_info[\"recency_boost\"] = 1.0\n",
    "\n",
    "        if lightweight or is_binary:\n",
    "            return file_info\n",
    "\n",
    "        try:\n",
    "            if file_size < 10 * 1024 * 1024:\n",
    "                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    file_info[\"lines\"] = sum(1 for _ in f)\n",
    "            else:\n",
    "                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    sample_lines = sum(1 for _, _ in zip(range(10000), f))\n",
    "                    file_info[\"lines\"] = int((file_size / (f.tell() or 1)) * sample_lines)\n",
    "        except (OSError, UnicodeDecodeError) as e:\n",
    "            logger.debug(f\"Failed to count lines for {path}: {e}\")\n",
    "            file_info[\"is_binary\"] = True\n",
    "\n",
    "        return file_info\n",
    "\n",
    "    @lru_cache(maxsize=100)\n",
    "    def _cache_query(self, query: str) -> str:\n",
    "        \"\"\"Cache lowercase query for efficient reuse.\n",
    "\n",
    "        Args:\n",
    "            query: Query string to cache.\n",
    "\n",
    "        Returns:\n",
    "            Lowercase query string.\n",
    "        \"\"\"\n",
    "        return query.lower() if query else \"\"\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the query cache.\"\"\"\n",
    "        self._cache_query.cache_clear()\n",
    "\n",
    "    def _score_file_content(self, file_path: str, query_lower: str) -> float:\n",
    "        \"\"\"Score file content for relevance to query.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to the file to score.\n",
    "            query_lower: Lowercase query string.\n",
    "\n",
    "        Returns:\n",
    "            Fuzzy matching score (0-100).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read(10 * 1024).lower()  # Read 10KB for better coverage\n",
    "            return fuzz.partial_ratio(query_lower, content)\n",
    "        except (OSError, UnicodeDecodeError) as e:\n",
    "            logger.debug(f\"Failed to score {file_path}: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _discover_files(self, path: str, file_pattern: str, query: str, max_depth: int, max_files: int = 100, limit_results: int = 5, timeout: float = 10.0, simple_mode: bool = False, minimal_output: bool = False) -> List[Dict]:\n",
    "        \"\"\"Discover relevant files using scandir and optional scoring.\n",
    "\n",
    "        Args:\n",
    "            path: Directory path to scan.\n",
    "            file_pattern: Glob pattern for file filtering.\n",
    "            query: Search query for fuzzy matching.\n",
    "            max_depth: Maximum recursion depth.\n",
    "            max_files: Maximum number of files to process.\n",
    "            limit_results: Maximum number of results to return.\n",
    "            timeout: Maximum time (seconds) for discovery.\n",
    "            simple_mode: If True, disable scoring and sort by path.\n",
    "            minimal_output: If True, return only path and name in results.\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries with file details or error messages.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        path_obj = Path(path)\n",
    "        if not path_obj.is_dir():\n",
    "            return [{\"error\": f\"Path {path} is not a directory for discovery\"}]\n",
    "\n",
    "        query_lower = self._cache_query(query)\n",
    "        patterns = [fnmatch.translate(p) for p in file_pattern.split('|')]\n",
    "        pattern_regexes = [re.compile(p) for p in patterns]\n",
    "        candidates = []\n",
    "\n",
    "        def _collect_with_scandir(dir_path: str, current_depth: int, file_count: List[int]) -> None:\n",
    "            if current_depth > max_depth or file_count[0] >= max_files or time.time() - start_time > timeout:\n",
    "                return\n",
    "            try:\n",
    "                with os.scandir(dir_path) as entries:\n",
    "                    for entry in entries:\n",
    "                        if time.time() - start_time > timeout:\n",
    "                            logger.warning(f\"Discovery timeout after {timeout}s\")\n",
    "                            return\n",
    "                        rel_path = entry.name\n",
    "                        if self.repo:\n",
    "                            try:\n",
    "                                rel_path = str(Path(entry.path).relative_to(self.repo.working_dir))\n",
    "                            except ValueError:\n",
    "                                pass\n",
    "                        if self._is_excluded(rel_path):\n",
    "                            continue\n",
    "                        if any(p.match(entry.name) for p in pattern_regexes) and entry.is_file() and Path(entry.path).exists():\n",
    "                            file_info = self._get_file_info(entry.path, lightweight=True)\n",
    "                            if \"error\" in file_info or file_info.get(\"is_binary\"):\n",
    "                                continue\n",
    "                            base_score = 0\n",
    "                            if query and not simple_mode:\n",
    "                                name_score = fuzz.partial_ratio(query_lower, entry.name.lower())\n",
    "                                base_score = name_score\n",
    "                            git_boost = 20 if not simple_mode and file_info[\"git_status\"] and file_info[\"git_status\"].get(\"index\") in [\"M\", \"A\"] else 10 if not simple_mode and file_info[\"git_status\"] and file_info[\"git_status\"].get(\"working_tree\") == \"?\" else 0\n",
    "                            mtime_boost = 10 if not simple_mode and file_info[\"recency_boost\"] > 0.8 else 0\n",
    "                            candidates.append({\n",
    "                                \"path\": entry.path,\n",
    "                                \"name\": entry.name,\n",
    "                                \"type\": \"file\",\n",
    "                                \"score\": base_score + git_boost + mtime_boost,\n",
    "                                \"file_info\": file_info\n",
    "                            })\n",
    "                            file_count[0] += 1\n",
    "                            if file_count[0] >= max_files:\n",
    "                                logger.info(f\"Reached max files: {max_files}\")\n",
    "                                return\n",
    "                        if entry.is_dir():\n",
    "                            _collect_with_scandir(entry.path, current_depth + 1, file_count)\n",
    "            except (OSError, PermissionError) as e:\n",
    "                logger.debug(f\"Scan error in {dir_path}: {e}\")\n",
    "\n",
    "        file_count = [0]\n",
    "        _collect_with_scandir(str(path_obj), 0, file_count)\n",
    "\n",
    "        if not candidates:\n",
    "            return [{\"error\": \"No files matched the criteria\"}]\n",
    "\n",
    "        if query and not simple_mode:\n",
    "            with ThreadPoolExecutor(max_workers=min(8, os.cpu_count() or 4)) as executor:\n",
    "                futures = {executor.submit(self._score_file_content, c[\"path\"], query_lower): c for c in candidates}\n",
    "                for future in as_completed(futures):\n",
    "                    content_score = future.result()\n",
    "                    if content_score > 0:\n",
    "                        idx = candidates.index(futures[future])\n",
    "                        candidates[idx][\"score\"] = max(candidates[idx][\"score\"], content_score)\n",
    "\n",
    "        if not simple_mode:\n",
    "            candidates.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        else:\n",
    "            candidates.sort(key=lambda x: x[\"path\"])\n",
    "\n",
    "        found = []\n",
    "        for item in candidates[:limit_results]:\n",
    "            result = {\"path\": item[\"path\"], \"name\": item[\"name\"]}\n",
    "            if not minimal_output:\n",
    "                result.update({\n",
    "                    \"type\": item[\"type\"],\n",
    "                    \"score\": item[\"score\"],\n",
    "                    \"git_status\": item[\"file_info\"].get(\"git_status\", {\"index\": None, \"working_tree\": None}),\n",
    "                    \"recency\": item[\"file_info\"][\"mtime\"]\n",
    "                })\n",
    "            found.append(result)\n",
    "\n",
    "        logger.info(f\"Discovered {len(found)} files in {time.time() - start_time:.2f}s (processed {file_count[0]})\")\n",
    "        return found\n",
    "\n",
    "    def _extract_content(self, path: str, query: str, file_pattern: str, context_lines: int, regex_mode: bool, start_line: int, end_line: int, minimal_output: bool = False) -> str:\n",
    "        \"\"\"Extract content from files or directories with context-aware snippets.\n",
    "\n",
    "        Args:\n",
    "            path: File or directory path to process.\n",
    "            query: Search query for matching.\n",
    "            file_pattern: Glob pattern for file filtering.\n",
    "            context_lines: Number of context lines around matches.\n",
    "            regex_mode: True for regex matching, False for substring.\n",
    "            start_line: Starting line for extraction.\n",
    "            end_line: Ending line for extraction (-1 for file end).\n",
    "            minimal_output: If True, omit metadata in output.\n",
    "\n",
    "        Returns:\n",
    "            JSON string with snippets or file content, or error message.\n",
    "        \"\"\"\n",
    "        path_obj = Path(path)\n",
    "        if not path_obj.exists():\n",
    "            return json.dumps({\"error\": f\"Path {path} does not exist\"})\n",
    "\n",
    "        if path_obj.is_dir():\n",
    "            snippets = []\n",
    "            discovered = self._discover_files(path, file_pattern, query, 1, max_files=10, limit_results=10, simple_mode=True, minimal_output=minimal_output)\n",
    "            for file_item in discovered:\n",
    "                if \"error\" not in file_item and isinstance(file_item.get(\"path\"), str) and Path(file_item[\"path\"]).is_file():\n",
    "                    rel_path = file_item[\"path\"]\n",
    "                    if self.repo:\n",
    "                        try:\n",
    "                            rel_path = str(Path(file_item[\"path\"]).relative_to(self.repo.working_dir))\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                    if self._is_excluded(rel_path):\n",
    "                        continue\n",
    "                    file_info = self._get_file_info(file_item[\"path\"], lightweight=False)\n",
    "                    if \"error\" in file_info or file_info.get(\"is_binary\"):\n",
    "                        continue\n",
    "                    file_snip = self._extract_from_file(file_item[\"path\"], query, context_lines, regex_mode, start_line, end_line, minimal_output)\n",
    "                    if file_snip and not file_snip.startswith(\"[Binary or invalid file\") and not file_snip.startswith(\"Error reading file\"):\n",
    "                        snippets.append({\"file\": file_item[\"path\"], \"snippet\": file_snip})\n",
    "            if not snippets:\n",
    "                return json.dumps({\"error\": f\"No valid text files matched the criteria in {path}\"})\n",
    "            content = json.dumps(snippets)\n",
    "            content_lines = content.split('\\n')\n",
    "            byte_count = 0\n",
    "            truncated_content = []\n",
    "            for line in content_lines:\n",
    "                byte_count += len(line.encode('utf-8')) + 1\n",
    "                if byte_count > 16 * 1024:\n",
    "                    truncated_content.append(\"... [truncated]\")\n",
    "                    break\n",
    "                truncated_content.append(line)\n",
    "            return '\\n'.join(truncated_content)\n",
    "        else:\n",
    "            rel_path = path\n",
    "            if self.repo:\n",
    "                try:\n",
    "                    rel_path = str(path_obj.relative_to(self.repo.working_dir))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            if self._is_excluded(rel_path):\n",
    "                return json.dumps({\"error\": f\"Path {path} is excluded by .gitignore or common patterns\"})\n",
    "            return self._extract_from_file(path, query, context_lines, regex_mode, start_line, end_line, minimal_output)\n",
    "\n",
    "    def _extract_from_file(self, file_path: str, query: str, context_lines: int, regex_mode: bool, start_line: int, end_line: int, minimal_output: bool = False) -> str:\n",
    "        \"\"\"Extract matching lines from a file with surrounding context.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to the file to process.\n",
    "            query: Search query for matching.\n",
    "            context_lines: Number of context lines around matches.\n",
    "            regex_mode: True for regex matching, False for substring.\n",
    "            start_line: Starting line for processing.\n",
    "            end_line: Ending line for processing (-1 for file end).\n",
    "            minimal_output: If True, omit metadata in output.\n",
    "\n",
    "        Returns:\n",
    "            Formatted string with matches, context, and optional metadata.\n",
    "        \"\"\"\n",
    "        if not isinstance(file_path, str) or not Path(file_path).is_file():\n",
    "            return f\"[Invalid file path: {file_path}]\"\n",
    "\n",
    "        file_info = self._get_file_info(file_path, lightweight=False)\n",
    "        if \"error\" in file_info or file_info.get(\"is_binary\"):\n",
    "            return f\"[Binary or invalid file: {file_info.get('size', 0)} bytes]\"\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                matches = []\n",
    "                pattern = None\n",
    "                if query and regex_mode:\n",
    "                    pattern = re.compile(query, re.IGNORECASE | re.MULTILINE)\n",
    "                elif query:\n",
    "                    query_lower = self._cache_query(query)\n",
    "\n",
    "                current_line = start_line\n",
    "                byte_count = 0\n",
    "                line_buffer = []\n",
    "                for line_num, line in enumerate(f, start_line):\n",
    "                    if current_line > end_line and end_line != -1:\n",
    "                        break\n",
    "                    line_stripped = line.rstrip()\n",
    "                    byte_count += len(line_stripped.encode('utf-8'))\n",
    "                    if byte_count > 16 * 1024:\n",
    "                        break\n",
    "                    line_buffer.append((current_line, line_stripped))\n",
    "                    if len(line_buffer) > 2 * context_lines + 1:\n",
    "                        line_buffer.pop(0)\n",
    "\n",
    "                    matched = False\n",
    "                    if pattern:\n",
    "                        if pattern.search(line):\n",
    "                            matched = True\n",
    "                    elif query:\n",
    "                        if query_lower in line.lower():\n",
    "                            matched = True\n",
    "\n",
    "                    if matched:\n",
    "                        context_start = max(1, current_line - context_lines)\n",
    "                        context_end = current_line + context_lines + 1\n",
    "                        context_lines_content = [f\"Line {ln}: {text}\" for ln, text in line_buffer if context_start <= ln < context_end]\n",
    "                        matches.append({\n",
    "                            \"line_number\": current_line,\n",
    "                            \"content\": line_stripped,\n",
    "                            \"context\": \"\\n\".join(context_lines_content)\n",
    "                        })\n",
    "                    current_line += 1\n",
    "\n",
    "                snippets = []\n",
    "                for match in matches:\n",
    "                    snippet = f\"Line {match['line_number']}: {match['content']}\\n{match['context']}\\n\"\n",
    "                    snippets.append(snippet)\n",
    "                content = '\\n--- Match ---\\n'.join(snippets) if matches else \"No matches found; fallback content truncated.\"\n",
    "        except (OSError, UnicodeDecodeError) as e:\n",
    "            return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "        if minimal_output:\n",
    "            return content\n",
    "\n",
    "        git_info = file_info['git_status'] or {\"index\": None, \"working_tree\": None}\n",
    "        metadata = (\n",
    "            f\"\\n--- File Info: {file_info['size']} bytes, \"\n",
    "            f\"{file_info.get('lines', 0)} lines, \"\n",
    "            f\"Git: index={git_info['index']}, working_tree={git_info['working_tree']} ---\"\n",
    "        )\n",
    "        return content + metadata\n",
    "\n",
    "    def get_tool_schema(self, verbose: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Return JSON schema for LLM integration, with optional minimal descriptions.\n",
    "\n",
    "        Args:\n",
    "            verbose: If True, include detailed descriptions; if False, return minimal schema.\n",
    "\n",
    "        Returns:\n",
    "            JSON schema dictionary.\n",
    "        \"\"\"\n",
    "        schema = {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"fs_read\",\n",
    "                \"description\": \"Discover relevant files or extract context-aware snippets. Optimized for large repos with .gitignore support.\" if verbose else \"\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"operations\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"mode\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"enum\": [mode.value for mode in ToolCallMode],\n",
    "                                        \"description\": \"Select 'discover' to list files or 'extract' to pull snippets.\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"path\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"File or directory path, e.g., '/project/src' or 'app.py'.\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"query\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"Search term for fuzzy (DISCOVER) or regex/substring (EXTRACT). Required for EXTRACT.\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"file_pattern\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"Glob filter, e.g., '*.py' or '*.go|*.rs'.\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"max_depth\": {\n",
    "                                        \"type\": \"integer\",\n",
    "                                        \"description\": \"Max dir recursion for DISCOVER (default 10).\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"context_lines\": {\n",
    "                                        \"type\": \"integer\",\n",
    "                                        \"description\": \"Lines around matches in EXTRACT (default 3).\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"regex_mode\": {\n",
    "                                        \"type\": \"boolean\",\n",
    "                                        \"description\": \"True for regex in EXTRACT; false for substring (default True).\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"start_line\": {\n",
    "                                        \"type\": \"integer\",\n",
    "                                        \"description\": \"Start line for EXTRACT (default 1).\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"end_line\": {\n",
    "                                        \"type\": \"integer\",\n",
    "                                        \"description\": \"End line for EXTRACT (-1 = file end).\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"max_files\": {\n",
    "                                        \"type\": \"integer\",\n",
    "                                        \"description\": \"Max files to scan in DISCOVER (default 100).\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"limit_results\": {\n",
    "                                        \"type\": \"integer\",\n",
    "                                        \"description\": \"Max results from DISCOVER (default 5).\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"simple_mode\": {\n",
    "                                        \"type\": \"boolean\",\n",
    "                                        \"description\": \"Disable scoring for simple glob-based discovery (default False).\" if verbose else \"\"\n",
    "                                    },\n",
    "                                    \"minimal_output\": {\n",
    "                                        \"type\": \"boolean\",\n",
    "                                        \"description\": \"Return minimal output (path, name, data only) for LLM (default False).\" if verbose else \"\"\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\"mode\", \"path\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"operations\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return schema\n",
    "\n",
    "    def execute(self, params: FsReadParams) -> Dict[str, Any]:\n",
    "        \"\"\"Execute operations with chainable JSON output.\n",
    "\n",
    "        Args:\n",
    "            params: FsReadParams object containing operations to execute.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with results, errors, and metadata.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        git_detected = self.repo is not None\n",
    "        total_processed = 0\n",
    "\n",
    "        for op in params.operations:\n",
    "            # Validate operation with repo context\n",
    "            op = op.model_copy()  # Create a copy to avoid modifying the original\n",
    "            op = op.model_validate(op.model_dump(), context={\"repo\": self.repo})\n",
    "            mode = op.mode.value\n",
    "            path = op.path\n",
    "            output_data = None\n",
    "            error = None\n",
    "\n",
    "            try:\n",
    "                if mode == \"discover\":\n",
    "                    output_data = self._discover_files(\n",
    "                        path, op.file_pattern, op.query or \"\", op.max_depth,\n",
    "                        op.max_files, op.limit_results, simple_mode=op.simple_mode,\n",
    "                        minimal_output=op.minimal_output\n",
    "                    )\n",
    "                    total_processed += len([x for x in output_data if \"error\" not in x])\n",
    "                elif mode == \"extract\":\n",
    "                    output_data = self._extract_content(\n",
    "                        path, op.query, op.file_pattern, op.context_lines,\n",
    "                        op.regex_mode, op.start_line, op.end_line, op.minimal_output\n",
    "                    )\n",
    "                    total_processed += 1\n",
    "            except (OSError, UnicodeDecodeError, ValueError) as e:\n",
    "                error = {\"type\": type(e).__name__, \"message\": str(e)}\n",
    "                logger.error(f\"Error in {mode} on {path}: {str(e)}\")\n",
    "\n",
    "            results.append({\n",
    "                \"mode\": mode,\n",
    "                \"path\": path,\n",
    "                \"data\": output_data,\n",
    "                \"error\": error\n",
    "            })\n",
    "\n",
    "        return {\n",
    "            \"success\": all(r.get(\"error\") is None for r in results),\n",
    "            \"data\": results,\n",
    "            \"error\": None if all(r.get(\"error\") is None for r in results) else {\"type\": \"BatchError\", \"message\": \"Some operations failedâ€”check individual errors.\"},\n",
    "            \"metadata\": {\n",
    "                \"git_detected\": git_detected,\n",
    "                \"processed_files\": total_processed,\n",
    "                \"description\": \"Results ready for chaining: Use the returned 'data[].path' and 'data[].snippet' fields as inputs for the next processing step.\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89717580-4156-4a28-97c8-69ca18d01e19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Discovered 20 files in 0.06s (processed 48)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"success\": true,\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"mode\": \"discover\",\n",
      "      \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab\",\n",
      "      \"data\": [\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/__init__.py\",\n",
      "          \"name\": \"__init__.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/_modidx.py\",\n",
      "          \"name\": \"_modidx.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/__init__.py\",\n",
      "          \"name\": \"__init__.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/analyzer.py\",\n",
      "          \"name\": \"analyzer.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/llm_factory.py\",\n",
      "          \"name\": \"llm_factory.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/schemas.py\",\n",
      "          \"name\": \"schemas.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/tokens.py\",\n",
      "          \"name\": \"tokens.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/tools/__init__.py\",\n",
      "          \"name\": \"__init__.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/tools/base.py\",\n",
      "          \"name\": \"base.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/tools/code_interpreter.py\",\n",
      "          \"name\": \"code_interpreter.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/tools/code_quality.py\",\n",
      "          \"name\": \"code_quality.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/tools/debate_agent.py\",\n",
      "          \"name\": \"debate_agent.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/tools/doc_generator.py\",\n",
      "          \"name\": \"doc_generator.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/tools/execute_bash.py\",\n",
      "          \"name\": \"execute_bash.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/tools/fs_read.py\",\n",
      "          \"name\": \"fs_read.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/tools/fs_read_backup.py\",\n",
      "          \"name\": \"fs_read_backup.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/tools/fs_write.py\",\n",
      "          \"name\": \"fs_write.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/tools/fs_write_backup.py\",\n",
      "          \"name\": \"fs_write_backup.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/tools/introspect.py\",\n",
      "          \"name\": \"introspect.py\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"/home/pranav-pc/projects/applied-GenAI-lab/agentic/backend/tools/manager.py\",\n",
      "          \"name\": \"manager.py\"\n",
      "        }\n",
      "      ],\n",
      "      \"error\": null\n",
      "    }\n",
      "  ],\n",
      "  \"error\": null,\n",
      "  \"metadata\": {\n",
      "    \"git_detected\": true,\n",
      "    \"processed_files\": 20,\n",
      "    \"description\": \"Results ready for chaining: Use the returned 'data[].path' and 'data[].snippet' fields as inputs for the next processing step.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "tool = FsReadTool()\n",
    "params = FsReadParams(operations=[\n",
    "    FsReadOperation(mode=\"discover\", path=\"/home/pranav-pc/projects/applied-GenAI-lab\", query=\"system prompt\", file_pattern=\"*.py\",simple_mode=True,minimal_output=True,limit_results=20),\n",
    "    # FsReadOperation(mode=ToolCallMode.EXTRACT, path=\"/home/pranav-pc/projects/applied-GenAI-lab/\", query=r\"def\\s+get_system_prompt\", regex_mode=True, context_lines=5)\n",
    "])\n",
    "result = tool.execute(params)\n",
    "print(json.dumps(result, indent=2))  # Chainable JSON for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91403b0d-354c-4adb-8929-8bef4225fc1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be03412-3c78-49dc-8ece-089ff73388ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e75bf-6c7d-4227-b8ee-822b0c376e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04aab41-c836-4243-b582-edb11c5830aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec6d15-f0f8-4b4b-925a-746aa5c4b000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664632fe-75e9-47fe-9e05-f83be3d56236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e46b2-3ab0-4bc3-b867-361db68125b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
