# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/buddy/backend/core/agent.ipynb.

# %% auto 0
__all__ = ['logger', 'Message', 'AgentConfig', 'Agent']

# %% ../../nbs/buddy/backend/core/agent.ipynb 1
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass, field
import json
from ..llms.client import LLMClient
from ..configs.loader import get_model_config
from ..tools.manager import ToolManager
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# %% ../../nbs/buddy/backend/core/agent.ipynb 2
@dataclass
class Message:
    role: str
    content: str
    tool_calls: Optional[List[Dict]] = None
    tool_call_id: Optional[str] = None

@dataclass
class AgentConfig:
    name: str
    instructions: str = ""
    model: Optional[str] = None
    tools: List[str] = field(default_factory=list)
    temperature: float = 0.7
    max_tokens: Optional[int] = None

class Agent:
    """Core Agent class with tool execution and conversation management"""

    def __init__(self, config: AgentConfig, llm_client: Optional[LLMClient] = None):
        self.config = config
        self.system_prompt = config.instructions
        self.llm_client = llm_client or self._create_default_llm_client()
        logger.info(f"Initialized LLM client with model: {self.llm_client.model}")
        self.conversation_history: List[Message] = [Message(role="system", content=self.system_prompt)]
        self.tools_registry: Dict[str, Callable] = {}
        self.guardrails: List[Callable] = []
        self.tool_manager = ToolManager()

    def _create_default_llm_client(self) -> LLMClient:
        """Create default LLM client from config."""
        model_config = get_model_config()
        model_name = self.config.model or model_config.get('name')
        if not model_name:
            raise ValueError("No model specified in config")
        return LLMClient(
            model=model_name,
            base_url=model_config.get('url'),
            api_key=model_config.get('api_key')
        )

    def add_tool(self, name: str, tool_func: Callable) -> None:
        """Register a tool with the agent."""
        if not callable(tool_func):
            raise ValueError(f"Tool function '{name}' must be callable")
        self.tools_registry[name] = tool_func

    def add_guardrail(self, guardrail_func: Callable) -> None:
        """Add a guardrail function."""
        self.guardrails.append(guardrail_func)

    def run(self, message: str, **kwargs) -> Dict[str, Any]:
        """Execute agent with message and return response."""
        # Apply guardrails
        for guardrail in self.guardrails:
            result = guardrail(message)
            if not isinstance(result, bool) or not result:
                return {"content": "Request blocked by guardrails", "blocked": True}

        # Add user message
        self.conversation_history.append(Message(role="user", content=message))

        # Initialize result
        final_result = {"content": "", "tool_calls": [], "blocked": False}
        max_iterations = kwargs.get('max_iterations', 10)  # Fallback safety

        while True:
            # Get available tools
            available_tools = self._get_available_tools()

            # Create completion
            messages = self._format_messages_for_llm()
            stream = kwargs.get('stream', True)
            try:
                response = self.llm_client.create_completion(
                    messages=messages,
                    tools=available_tools,
                    **kwargs
                )
            except Exception as e:
                logger.error(f"LLM completion failed: {str(e)}")
                return {"content": f"Error: {str(e)}", "blocked": True}

            # Process response
            if stream:
                if not hasattr(response, '__iter__'):
                    raise ValueError("Streaming response expected but non-iterable response received")
                try:
                    result = self.llm_client.handle_streaming_response(response)
                except Exception as e:
                    logger.error(f"Error processing streaming response: {str(e)}")
                    return {"content": f"Streaming error: {str(e)}", "blocked": True}
            else:
                try:
                    result = self.llm_client.process_response(response)
                except Exception as e:
                    logger.error(f"Error processing response: {str(e)}")
                    return {"content": f"Response error: {str(e)}", "blocked": True}

            # Add assistant response to history
            assistant_message = Message(
                role="assistant",
                content=result.get("content", ""),
                tool_calls=result.get("tool_calls")
            )
            self.conversation_history.append(assistant_message)
            final_result["content"] = result.get("content", "")
            final_result["tool_calls"].extend(result.get("tool_calls", []))

            # Check stop reason
            finish_reason = result.get("finish_reason", "unknown")
            logger.debug(f"LLM response finish reason: {finish_reason}")

            # Handle tool calls if present
            if result.get("tool_calls") and finish_reason == "tool_calls":
                executed_calls = self._execute_tool_calls(result["tool_calls"])
                final_result["tool_calls"] = executed_calls
                continue  # Continue loop to process tool results
            
            elif finish_reason in ["stop", "length", "content_filter"]:
                break  # Stop if LLM indicates completion
            else:
                logger.warning(f"Unknown finish reason: {finish_reason}")
                break  # Break on unknown finish reason

        # Limit conversation history
        if len(self.conversation_history) > 50:
            self.conversation_history = [self.conversation_history[0]] + self.conversation_history[-49:]

        return final_result

    def _execute_tool_calls(self, tool_calls: List[Dict]) -> List[Dict]:
        """Execute tool calls and append results to conversation history."""
        from agentic.tools.display import ToolExecutionDisplay
        display = ToolExecutionDisplay()
        executed_calls = []

        for tool_call in tool_calls:
            function_name = tool_call["function"]["name"]
            tool_call_id = tool_call.get("id")
            try:
                arguments = json.loads(tool_call["function"]["arguments"])
            except json.JSONDecodeError as e:
                logger.error(f"Invalid arguments for {function_name}: {str(e)}")
                display.show_tool_error(f"Error in {function_name}", str(e))
                tool_call["error"] = str(e)
                executed_calls.append(tool_call)
                continue

            display.show_tool_start(function_name, trusted=True, args=arguments)

            try:
                if function_name in self.tools_registry:
                    result = self.tools_registry[function_name](**arguments)
                else:
                    result = self.tool_manager.execute_tool(function_name, arguments)
                tool_call["result"] = result
                executed_calls.append(tool_call)

                # Append tool result to conversation history
                self.conversation_history.append(Message(
                    role="tool",
                    content=str(result),
                    tool_call_id=tool_call_id
                ))
            except Exception as e:
                logger.error(f"Error executing {function_name}: {str(e)}")
                display.show_tool_error(f"Error in {function_name}", str(e))
                tool_call["error"] = str(e)
                executed_calls.append(tool_call)

        return executed_calls

    def _get_available_tools(self) -> List[Dict]:
        """Get OpenAI-formatted tools for the configured tool names."""
        return self.tool_manager.get_tools(self.config.tools)

    def _format_messages_for_llm(self) -> List[Dict]:
        """Convert Message objects to a format suitable for the LLM client."""
        messages = []
        for msg in self.conversation_history:
            message_dict = {"role": msg.role, "content": msg.content}
            if msg.tool_calls:
                message_dict["tool_calls"] = msg.tool_calls
            if msg.tool_call_id:
                message_dict["tool_call_id"] = msg.tool_call_id
            messages.append(message_dict)
        return messages

    def clear_history(self) -> None:
        """Clear conversation history except system message."""
        self.conversation_history = [Message(role="system", content=self.system_prompt)]
